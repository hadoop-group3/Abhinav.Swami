[INFO ]20161111@18:44:40,109:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@18:44:40,890:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@18:44:41,505:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@18:44:41,510:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@18:44:41,511:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@18:44:42,056:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 54747.
[INFO ]20161111@18:44:42,720:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@18:44:42,858:Remoting - Starting remoting
[INFO ]20161111@18:44:43,393:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:55758]
[INFO ]20161111@18:44:43,397:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:55758]
[INFO ]20161111@18:44:43,428:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55758.
[INFO ]20161111@18:44:43,531:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@18:44:43,609:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@18:44:43,680:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-86586f8d-f962-4447-a716-0b0803d993c0
[INFO ]20161111@18:44:43,763:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@18:44:43,982:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@18:44:44,895:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@18:44:44,904:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@18:44:45,151:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@18:44:45,201:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59605.
[INFO ]20161111@18:44:45,205:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59605
[INFO ]20161111@18:44:45,207:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@18:44:45,223:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:59605 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 59605)
[INFO ]20161111@18:44:45,225:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@18:44:46,564:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@18:44:47,109:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@18:44:47,115:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59605 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@18:44:47,130:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:108
[INFO ]20161111@18:44:47,845:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@18:44:48,004:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:124
[INFO ]20161111@18:44:48,048:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:124) with 1 output partitions
[INFO ]20161111@18:44:48,050:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:124)
[INFO ]20161111@18:44:48,055:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@18:44:48,063:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@18:44:48,090:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:108), which has no missing parents
[INFO ]20161111@18:44:48,141:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@18:44:48,164:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@18:44:48,167:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59605 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@18:44:48,168:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@18:44:48,173:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:108)
[INFO ]20161111@18:44:48,175:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@18:44:48,255:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@18:44:48,276:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@18:44:48,324:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@18:44:48,328:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@18:44:48,347:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@18:44:48,347:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@18:44:48,348:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@18:44:48,348:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@18:44:48,348:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@18:44:48,561:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@18:44:48,563:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:59605 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@18:44:48,658:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@18:44:48,716:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:124) finished in 0.517 s
[INFO ]20161111@18:44:48,715:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 478 ms on localhost (1/1)
[INFO ]20161111@18:44:48,729:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@18:44:48,745:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:124, took 0.735033 s
[INFO ]20161111@18:44:48,831:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@18:44:48,872:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@18:44:48,887:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@18:44:48,888:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@18:44:48,892:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@18:44:48,897:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@18:44:48,925:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@18:44:48,931:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@18:44:48,949:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@18:44:48,959:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@18:44:48,960:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-8c841381-30f6-486c-96b2-7d8a2556ae64
[INFO ]20161111@18:54:53,989:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@18:54:55,102:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@18:54:55,904:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@18:54:55,906:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@18:54:55,910:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@18:54:56,810:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46683.
[INFO ]20161111@18:54:58,137:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@18:54:58,321:Remoting - Starting remoting
[INFO ]20161111@18:54:58,941:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:55921]
[INFO ]20161111@18:54:58,952:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:55921]
[INFO ]20161111@18:54:58,982:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55921.
[INFO ]20161111@18:54:59,089:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@18:54:59,210:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@18:54:59,275:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-571f72d8-43d1-403c-9fab-529a1e451918
[INFO ]20161111@18:54:59,356:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@18:54:59,619:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@18:55:00,616:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@18:55:00,623:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@18:55:01,013:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@18:55:01,146:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37454.
[INFO ]20161111@18:55:01,148:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 37454
[INFO ]20161111@18:55:01,155:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@18:55:01,165:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:37454 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 37454)
[INFO ]20161111@18:55:01,168:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@18:55:04,059:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@18:55:04,941:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@18:55:04,947:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:37454 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@18:55:04,970:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@18:55:06,052:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@18:55:06,294:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@18:55:06,378:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@18:55:06,383:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@18:55:06,389:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@18:55:06,399:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@18:55:06,443:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@18:55:06,525:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@18:55:06,580:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@18:55:06,590:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:37454 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@18:55:06,594:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@18:55:06,602:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@18:55:06,608:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@18:55:06,774:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@18:55:06,816:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@18:55:06,916:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@18:55:06,924:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@18:55:06,962:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@18:55:06,962:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@18:55:06,963:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@18:55:06,963:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@18:55:06,963:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@18:55:07,289:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@18:55:07,291:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:37454 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@18:55:07,470:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@18:55:07,603:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.944 s
[INFO ]20161111@18:55:07,603:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 893 ms on localhost (1/1)
[INFO ]20161111@18:55:07,621:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@18:55:07,649:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.354202 s
[INFO ]20161111@18:55:07,823:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@18:55:07,920:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@18:55:07,942:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@18:55:07,954:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@18:55:07,964:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@18:55:07,976:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@18:55:07,987:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@18:55:08,002:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@18:55:08,031:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@18:55:08,053:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@18:55:08,054:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-b911b0b6-9716-448a-ab5e-3bbcc94943ad
[INFO ]20161111@18:56:08,320:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@18:56:09,296:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@18:56:09,955:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@18:56:09,956:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@18:56:09,959:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@18:56:10,741:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45309.
[INFO ]20161111@18:56:11,720:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@18:56:11,862:Remoting - Starting remoting
[INFO ]20161111@18:56:12,360:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:37704]
[INFO ]20161111@18:56:12,379:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:37704]
[INFO ]20161111@18:56:12,410:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 37704.
[INFO ]20161111@18:56:12,489:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@18:56:12,577:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@18:56:12,627:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-dedbc1a3-7af8-44c7-8357-ed1f80311471
[INFO ]20161111@18:56:12,689:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@18:56:12,895:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@18:56:13,813:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@18:56:13,815:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@18:56:14,140:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@18:56:14,216:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38313.
[INFO ]20161111@18:56:14,221:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 38313
[INFO ]20161111@18:56:14,223:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@18:56:14,257:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:38313 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 38313)
[INFO ]20161111@18:56:14,275:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@18:56:16,332:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@18:56:17,039:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@18:56:17,047:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:38313 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@18:56:17,061:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@18:56:18,014:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@18:56:18,224:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@18:56:18,293:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@18:56:18,295:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@18:56:18,298:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@18:56:18,310:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@18:56:18,346:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@18:56:18,410:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@18:56:18,439:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@18:56:18,441:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:38313 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@18:56:18,443:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@18:56:18,469:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@18:56:18,479:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@18:56:18,622:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@18:56:18,656:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@18:56:18,754:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@18:56:18,763:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@18:56:18,798:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@18:56:18,798:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@18:56:18,798:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@18:56:18,799:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@18:56:18,799:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@18:56:19,114:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@18:56:19,115:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:38313 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@18:56:19,253:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@18:56:19,330:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.805 s
[INFO ]20161111@18:56:19,336:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 773 ms on localhost (1/1)
[INFO ]20161111@18:56:19,343:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@18:56:19,360:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.135304 s
[INFO ]20161111@18:56:19,474:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@18:56:19,501:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@18:56:19,517:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@18:56:19,518:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@18:56:19,555:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@18:56:19,567:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@18:56:19,575:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@18:56:19,584:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@18:56:19,594:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@18:56:19,607:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@18:56:19,608:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e526d535-de96-42a1-b9d8-0eaf6c49ac36
[INFO ]20161111@18:57:22,473:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@18:57:23,813:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@18:57:24,715:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@18:57:24,717:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@18:57:24,719:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@18:57:25,663:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35657.
[INFO ]20161111@18:57:26,970:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@18:57:27,181:Remoting - Starting remoting
[INFO ]20161111@18:57:27,864:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:47647]
[INFO ]20161111@18:57:27,871:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:47647]
[INFO ]20161111@18:57:27,904:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 47647.
[INFO ]20161111@18:57:28,030:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@18:57:28,109:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@18:57:28,142:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-0d0960be-9850-4c5e-be57-202e282cb22c
[INFO ]20161111@18:57:28,196:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@18:57:28,491:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@18:57:29,566:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@18:57:29,575:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@18:57:30,003:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@18:57:30,101:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43910.
[INFO ]20161111@18:57:30,103:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 43910
[INFO ]20161111@18:57:30,108:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@18:57:30,120:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:43910 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 43910)
[INFO ]20161111@18:57:30,130:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@18:57:33,157:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@18:57:34,103:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@18:57:34,112:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:43910 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@18:57:34,129:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@18:57:35,503:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@18:57:35,823:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@18:57:35,893:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@18:57:35,900:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@18:57:35,907:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@18:57:35,929:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@18:57:35,990:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@18:57:36,053:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@18:57:36,095:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@18:57:36,103:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:43910 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@18:57:36,103:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@18:57:36,112:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@18:57:36,122:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@18:57:36,356:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@18:57:36,407:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@18:57:36,557:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@18:57:36,580:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@18:57:36,658:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@18:57:36,659:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@18:57:36,659:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@18:57:36,660:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@18:57:36,660:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@18:57:36,987:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@18:57:36,991:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:43910 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@18:57:37,247:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@18:57:37,370:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 1.165 s
[INFO ]20161111@18:57:37,369:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1099 ms on localhost (1/1)
[INFO ]20161111@18:57:37,401:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@18:57:37,412:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.588961 s
[INFO ]20161111@18:57:37,711:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@18:57:37,804:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@18:57:37,843:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@18:57:37,844:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@18:57:37,863:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@18:57:37,909:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@18:57:37,916:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@18:57:37,933:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@18:57:37,954:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@18:57:37,957:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@18:57:37,958:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-02f4471f-9d28-48a5-8386-a64216c87f3b
[INFO ]20161111@19:00:28,412:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:00:29,140:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:00:29,653:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:00:29,654:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:00:29,655:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:00:30,240:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49861.
[INFO ]20161111@19:00:30,999:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:00:31,134:Remoting - Starting remoting
[INFO ]20161111@19:00:31,587:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:59898]
[INFO ]20161111@19:00:31,595:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:59898]
[INFO ]20161111@19:00:31,636:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 59898.
[INFO ]20161111@19:00:31,731:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:00:31,801:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:00:31,819:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-adf10769-8c78-43c1-be07-09b0a2a93633
[INFO ]20161111@19:00:31,866:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:00:32,031:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:00:32,893:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:00:32,896:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:00:33,189:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:00:33,255:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49087.
[INFO ]20161111@19:00:33,256:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 49087
[INFO ]20161111@19:00:33,258:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:00:33,265:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:49087 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 49087)
[INFO ]20161111@19:00:33,268:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:00:35,303:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:00:35,945:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:00:35,952:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:49087 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:00:35,972:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:00:36,593:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:00:36,752:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:00:36,812:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:00:36,814:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:00:36,817:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:00:36,828:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:00:36,878:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:00:36,939:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:00:36,974:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:00:36,975:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:49087 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:00:36,977:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:00:36,983:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:00:36,985:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:00:37,092:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:00:37,122:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:00:37,184:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:00:37,188:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:00:37,216:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:00:37,218:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:00:37,218:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:00:37,218:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:00:37,218:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:00:37,424:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:00:37,425:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:49087 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:00:37,543:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:00:37,651:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.636 s
[INFO ]20161111@19:00:37,652:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 590 ms on localhost (1/1)
[INFO ]20161111@19:00:37,658:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:00:37,680:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 0.922650 s
[INFO ]20161111@19:00:37,776:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:161
[INFO ]20161111@19:00:37,784:org.apache.spark.scheduler.DAGScheduler - Got job 1 (first at HW2_Part1.java:161) with 1 output partitions
[INFO ]20161111@19:00:37,785:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (first at HW2_Part1.java:161)
[INFO ]20161111@19:00:37,785:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:00:37,792:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:00:37,796:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144), which has no missing parents
[INFO ]20161111@19:00:37,816:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 343.8 KB)
[INFO ]20161111@19:00:37,872:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161111@19:00:37,875:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:49087 (size: 2.2 KB, free: 1027.1 MB)
[INFO ]20161111@19:00:37,877:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:00:37,881:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144)
[INFO ]20161111@19:00:37,885:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@19:00:37,899:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:00:37,901:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@19:00:37,912:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@19:01:16,350:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2065 bytes result sent to driver
[INFO ]20161111@19:01:16,384:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (first at HW2_Part1.java:161) finished in 38.479 s
[INFO ]20161111@19:01:16,385:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: first at HW2_Part1.java:161, took 38.603488 s
[INFO ]20161111@19:01:16,394:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 38487 ms on localhost (1/1)
[INFO ]20161111@19:01:16,401:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:01:16,475:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@19:01:16,541:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@19:01:16,574:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@19:01:16,577:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@19:01:16,583:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@19:01:16,594:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@19:01:16,621:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@19:01:16,630:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@19:01:16,642:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@19:01:16,658:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@19:01:16,659:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-72a029fe-4977-47b6-8f00-380909790757
[INFO ]20161111@19:01:36,868:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:01:37,517:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:01:38,022:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:01:38,023:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:01:38,025:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:01:38,623:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 59919.
[INFO ]20161111@19:01:39,361:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:01:39,459:Remoting - Starting remoting
[INFO ]20161111@19:01:39,991:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:60163]
[INFO ]20161111@19:01:39,997:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:60163]
[INFO ]20161111@19:01:40,040:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 60163.
[INFO ]20161111@19:01:40,130:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:01:40,175:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:01:40,191:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-0db85a8d-17a6-45f1-9601-f934fa1f4626
[INFO ]20161111@19:01:40,228:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:01:40,440:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:01:41,085:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:01:41,093:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:01:41,415:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:01:41,467:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56582.
[INFO ]20161111@19:01:41,468:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56582
[INFO ]20161111@19:01:41,470:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:01:41,482:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:56582 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 56582)
[INFO ]20161111@19:01:41,485:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:01:43,377:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:01:43,985:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:01:43,994:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56582 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:01:44,010:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:01:44,743:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:01:44,912:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:01:44,960:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:01:44,964:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:01:44,966:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:01:44,975:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:01:45,009:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:01:45,067:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:01:45,117:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:01:45,127:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56582 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:01:45,129:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:01:45,141:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:01:45,145:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:01:45,286:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:01:45,313:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:01:45,386:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:01:45,400:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:01:45,448:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:01:45,448:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:01:45,449:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:01:45,449:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:01:45,449:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:01:45,687:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:01:45,690:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:56582 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:01:45,804:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:01:45,899:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.712 s
[INFO ]20161111@19:01:45,899:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 657 ms on localhost (1/1)
[INFO ]20161111@19:01:45,909:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:01:45,939:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.023664 s
[INFO ]20161111@19:01:46,042:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:161
[INFO ]20161111@19:01:46,051:org.apache.spark.scheduler.DAGScheduler - Got job 1 (first at HW2_Part1.java:161) with 1 output partitions
[INFO ]20161111@19:01:46,051:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (first at HW2_Part1.java:161)
[INFO ]20161111@19:01:46,051:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:01:46,055:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:01:46,056:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144), which has no missing parents
[INFO ]20161111@19:01:46,078:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 343.8 KB)
[INFO ]20161111@19:01:46,112:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161111@19:01:46,121:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:56582 (size: 2.2 KB, free: 1027.1 MB)
[INFO ]20161111@19:01:46,122:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:01:46,125:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144)
[INFO ]20161111@19:01:46,127:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@19:01:46,146:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:01:46,147:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@19:01:46,153:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@19:02:05,329:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2065 bytes result sent to driver
[INFO ]20161111@19:02:05,373:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (first at HW2_Part1.java:161) finished in 19.220 s
[INFO ]20161111@19:02:05,375:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: first at HW2_Part1.java:161, took 19.328206 s
[INFO ]20161111@19:02:05,387:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 19231 ms on localhost (1/1)
[INFO ]20161111@19:02:05,388:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:02:05,460:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@19:02:05,527:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@19:02:05,560:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@19:02:05,569:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@19:02:05,572:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@19:02:05,581:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@19:02:05,593:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@19:02:05,608:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@19:02:05,627:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@19:02:05,676:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@19:02:05,677:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3ed24ade-5def-424e-af65-b4e420b30188
[INFO ]20161111@19:05:15,608:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:05:16,160:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:05:16,596:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:05:16,597:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:05:16,598:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:05:17,112:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 60426.
[INFO ]20161111@19:05:17,710:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:05:17,815:Remoting - Starting remoting
[INFO ]20161111@19:05:18,176:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:45937]
[INFO ]20161111@19:05:18,180:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:45937]
[INFO ]20161111@19:05:18,207:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 45937.
[INFO ]20161111@19:05:18,252:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:05:18,296:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:05:18,330:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-dd7679b3-838d-4f65-af5c-e255050c5397
[INFO ]20161111@19:05:18,373:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:05:18,513:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:05:19,144:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:05:19,147:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:05:19,373:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:05:19,438:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56762.
[INFO ]20161111@19:05:19,439:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56762
[INFO ]20161111@19:05:19,441:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:05:19,449:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:56762 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 56762)
[INFO ]20161111@19:05:19,454:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:05:21,161:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:05:21,604:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:05:21,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56762 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:05:21,622:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:05:22,392:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:05:22,613:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:05:22,681:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:05:22,683:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:05:22,684:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:05:22,695:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:05:22,720:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:05:22,777:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:05:22,822:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:05:22,825:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56762 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:05:22,828:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:05:22,844:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:05:22,851:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:05:22,956:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:05:22,977:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:05:23,037:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:05:23,040:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:05:23,061:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:05:23,062:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:05:23,062:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:05:23,062:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:05:23,064:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:05:23,264:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:05:23,265:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:56762 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:05:23,365:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:05:23,448:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.565 s
[INFO ]20161111@19:05:23,450:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 531 ms on localhost (1/1)
[INFO ]20161111@19:05:23,457:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:05:23,477:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 0.853846 s
[INFO ]20161111@19:05:23,565:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:161
[INFO ]20161111@19:05:23,569:org.apache.spark.scheduler.DAGScheduler - Got job 1 (first at HW2_Part1.java:161) with 1 output partitions
[INFO ]20161111@19:05:23,570:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (first at HW2_Part1.java:161)
[INFO ]20161111@19:05:23,571:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:05:23,574:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:05:23,577:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144), which has no missing parents
[INFO ]20161111@19:05:23,593:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 343.8 KB)
[INFO ]20161111@19:05:23,639:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161111@19:05:23,663:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161111@19:05:23,686:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:56762 (size: 2.2 KB, free: 1027.1 MB)
[INFO ]20161111@19:05:23,693:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:05:23,694:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144)
[INFO ]20161111@19:05:23,694:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@19:05:23,707:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:56762 in memory (size: 1859.0 B, free: 1027.1 MB)
[INFO ]20161111@19:05:23,718:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:05:23,723:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@19:05:23,740:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@19:05:23,779:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2065 bytes result sent to driver
[INFO ]20161111@19:05:23,806:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (first at HW2_Part1.java:161) finished in 0.096 s
[INFO ]20161111@19:05:23,808:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: first at HW2_Part1.java:161, took 0.241570 s
[INFO ]20161111@19:05:23,814:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 95 ms on localhost (1/1)
[INFO ]20161111@19:05:23,817:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:05:23,890:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@19:05:23,911:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@19:05:23,922:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@19:05:23,927:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@19:05:23,930:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@19:05:23,933:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@19:05:23,954:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@19:05:23,972:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@19:05:23,965:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@19:05:23,980:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@19:05:23,998:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-38be43ef-c1df-474b-84d3-dfb6afe1d7bd
[INFO ]20161111@19:05:41,135:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:05:41,751:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:05:42,249:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:05:42,250:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:05:42,253:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:05:42,835:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 58090.
[INFO ]20161111@19:05:43,598:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:05:43,703:Remoting - Starting remoting
[INFO ]20161111@19:05:44,136:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:47811]
[INFO ]20161111@19:05:44,146:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:47811]
[INFO ]20161111@19:05:44,169:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 47811.
[INFO ]20161111@19:05:44,240:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:05:44,297:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:05:44,317:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b3f84f2f-b771-403a-a383-737cc4899612
[INFO ]20161111@19:05:44,335:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:05:44,492:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:05:45,124:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:05:45,130:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:05:45,432:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:05:45,498:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38292.
[INFO ]20161111@19:05:45,499:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 38292
[INFO ]20161111@19:05:45,501:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:05:45,507:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:38292 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 38292)
[INFO ]20161111@19:05:45,510:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:05:47,315:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:05:47,890:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:05:47,897:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:38292 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:05:47,917:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:05:48,623:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:05:48,801:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:05:48,848:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:05:48,852:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:05:48,854:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:05:48,865:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:05:48,902:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:05:48,971:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:05:49,012:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:05:49,014:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:38292 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:05:49,016:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:05:49,031:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:05:49,043:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:05:49,182:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:05:49,210:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:05:49,297:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:05:49,313:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:05:49,345:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:05:49,346:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:05:49,346:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:05:49,347:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:05:49,350:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:05:49,574:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:05:49,577:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:38292 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:05:49,724:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:05:49,797:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.707 s
[INFO ]20161111@19:05:49,793:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 659 ms on localhost (1/1)
[INFO ]20161111@19:05:49,800:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:05:49,822:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.018537 s
[INFO ]20161111@19:05:49,927:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:161
[INFO ]20161111@19:05:49,932:org.apache.spark.scheduler.DAGScheduler - Got job 1 (first at HW2_Part1.java:161) with 1 output partitions
[INFO ]20161111@19:05:49,932:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (first at HW2_Part1.java:161)
[INFO ]20161111@19:05:49,932:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:05:49,935:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:05:49,941:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144), which has no missing parents
[INFO ]20161111@19:05:49,950:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 343.8 KB)
[INFO ]20161111@19:05:49,994:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161111@19:05:49,995:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:38292 (size: 2.2 KB, free: 1027.1 MB)
[INFO ]20161111@19:05:49,996:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:05:50,002:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144)
[INFO ]20161111@19:05:50,002:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@19:05:50,023:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:05:50,025:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@19:05:50,036:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@19:18:24,613:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:18:25,329:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:18:25,831:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:18:25,832:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:18:25,834:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:18:26,466:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 47011.
[INFO ]20161111@19:18:27,223:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:18:27,334:Remoting - Starting remoting
[INFO ]20161111@19:18:27,887:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:40712]
[INFO ]20161111@19:18:27,891:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:40712]
[INFO ]20161111@19:18:27,935:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 40712.
[INFO ]20161111@19:18:28,019:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:18:28,069:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:18:28,091:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-71d6574c-9699-4053-a5ab-1e2ca22b8bf5
[INFO ]20161111@19:18:28,128:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:18:28,341:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161111@19:18:29,090:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161111@19:18:29,245:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161111@19:18:29,256:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4041
[INFO ]20161111@19:18:29,679:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:18:29,761:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60047.
[INFO ]20161111@19:18:29,762:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 60047
[INFO ]20161111@19:18:29,764:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:18:29,769:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:60047 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 60047)
[INFO ]20161111@19:18:29,777:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:18:31,774:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:18:32,401:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:18:32,407:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:60047 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:18:32,423:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:18:33,270:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:18:33,436:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:18:33,485:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:18:33,491:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:18:33,494:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:18:33,506:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:18:33,535:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:18:33,613:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:18:33,671:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 158.3 KB)
[INFO ]20161111@19:18:33,676:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:60047 (size: 1862.0 B, free: 1027.3 MB)
[INFO ]20161111@19:18:33,677:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:18:33,717:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:18:33,731:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:18:33,928:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:18:33,973:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:18:34,097:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:18:34,110:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:18:34,176:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:18:34,178:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:18:34,179:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:18:34,180:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:18:34,181:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:18:34,465:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:18:34,466:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:60047 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:18:34,578:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:18:34,674:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.896 s
[INFO ]20161111@19:18:34,675:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 835 ms on localhost (1/1)
[INFO ]20161111@19:18:34,679:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:18:34,702:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.264143 s
[INFO ]20161111@19:18:39,327:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2065 bytes result sent to driver
[INFO ]20161111@19:18:39,374:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (first at HW2_Part1.java:161) finished in 769.344 s
[INFO ]20161111@19:18:39,377:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: first at HW2_Part1.java:161, took 769.447845 s
[INFO ]20161111@19:18:39,383:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 769353 ms on localhost (1/1)
[INFO ]20161111@19:18:39,384:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:18:39,461:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@19:18:39,530:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@19:18:39,570:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@19:18:39,572:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@19:18:39,579:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@19:18:39,588:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@19:18:39,610:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@19:18:39,621:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@19:18:39,633:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@19:18:39,638:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@19:18:39,654:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-8f649a12-5a90-4469-9c58-80f30f8784af
[INFO ]20161111@19:19:04,904:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:19:05,538:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:19:06,112:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:19:06,113:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:19:06,117:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:19:06,671:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43053.
[INFO ]20161111@19:19:07,389:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:19:07,512:Remoting - Starting remoting
[INFO ]20161111@19:19:07,921:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:39505]
[INFO ]20161111@19:19:07,931:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:39505]
[INFO ]20161111@19:19:07,960:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 39505.
[INFO ]20161111@19:19:08,020:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:19:08,100:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:19:08,142:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e4433e19-d310-4e69-826b-63a6574c9bcb
[INFO ]20161111@19:19:08,164:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:19:08,329:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:19:08,923:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:19:08,929:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:19:09,209:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:19:09,262:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36468.
[INFO ]20161111@19:19:09,264:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 36468
[INFO ]20161111@19:19:09,266:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:19:09,273:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:36468 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 36468)
[INFO ]20161111@19:19:09,276:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:19:11,128:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:19:11,795:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:19:11,803:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:36468 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:19:11,818:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:19:12,579:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:19:12,738:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:19:12,791:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:19:12,792:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:19:12,795:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:19:12,807:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:19:12,838:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:19:12,905:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:19:12,947:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:19:12,948:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:36468 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:19:12,952:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:19:12,960:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:19:12,968:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:19:13,102:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:19:13,135:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:19:13,209:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:19:13,215:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:19:13,263:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:19:13,264:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:19:13,264:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:19:13,264:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:19:13,265:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:19:13,471:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:19:13,473:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:36468 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:19:13,590:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:19:13,665:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 607 ms on localhost (1/1)
[INFO ]20161111@19:19:13,672:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.658 s
[INFO ]20161111@19:19:13,672:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:19:13,697:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 0.953368 s
[INFO ]20161111@19:19:22,097:org.apache.spark.SparkContext - Starting job: collect at HW2_Part1.java:161
[INFO ]20161111@19:19:22,104:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part1.java:161) with 1 output partitions
[INFO ]20161111@19:19:22,104:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (collect at HW2_Part1.java:161)
[INFO ]20161111@19:19:22,104:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:19:22,107:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:19:22,112:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144), which has no missing parents
[INFO ]20161111@19:19:22,123:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.6 KB, free 343.8 KB)
[INFO ]20161111@19:19:22,144:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 345.9 KB)
[INFO ]20161111@19:19:22,145:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:36468 (size: 2.1 KB, free: 1027.1 MB)
[INFO ]20161111@19:19:22,149:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:19:22,151:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at HW2_Part1.java:144)
[INFO ]20161111@19:19:22,152:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@19:19:22,169:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:19:22,170:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@19:19:22,177:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@19:19:22,537:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 11812 bytes result sent to driver
[INFO ]20161111@19:19:22,594:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (collect at HW2_Part1.java:161) finished in 0.418 s
[INFO ]20161111@19:19:22,595:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part1.java:161, took 0.493607 s
[INFO ]20161111@19:19:22,603:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 429 ms on localhost (1/1)
[INFO ]20161111@19:19:22,603:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:33:54,944:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:33:55,602:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:33:56,107:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:33:56,109:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:33:56,110:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:33:56,782:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40530.
[INFO ]20161111@19:33:57,622:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:33:57,769:Remoting - Starting remoting
[INFO ]20161111@19:33:58,222:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:42542]
[INFO ]20161111@19:33:58,232:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:42542]
[INFO ]20161111@19:33:58,262:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 42542.
[INFO ]20161111@19:33:58,335:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:33:58,396:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:33:58,415:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e86dc38b-36b7-4e9d-b790-90a116bffc0f
[INFO ]20161111@19:33:58,449:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:33:58,629:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:33:59,377:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:33:59,384:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:33:59,633:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:33:59,680:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 48833.
[INFO ]20161111@19:33:59,681:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 48833
[INFO ]20161111@19:33:59,683:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:33:59,687:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:48833 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 48833)
[INFO ]20161111@19:33:59,690:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:34:01,833:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:34:02,486:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:34:02,491:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:48833 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:34:02,515:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:34:03,559:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:34:03,728:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:34:03,791:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:34:03,792:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:34:03,797:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:34:03,805:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:34:03,835:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:34:03,904:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:34:03,948:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:34:03,950:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:48833 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:34:03,953:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:34:03,965:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:34:03,972:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:34:04,179:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:34:04,241:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:34:04,415:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:34:04,429:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:34:04,498:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:34:04,502:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:34:04,502:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:34:04,503:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:34:04,503:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:34:05,031:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:34:05,037:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:48833 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:34:05,316:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:34:05,412:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1353 ms on localhost (1/1)
[INFO ]20161111@19:34:05,422:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 1.412 s
[INFO ]20161111@19:34:05,430:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:34:05,459:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.726743 s
[INFO ]20161111@19:47:07,485:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:47:08,169:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:47:08,684:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:47:08,686:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:47:08,688:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:47:09,302:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41035.
[INFO ]20161111@19:47:10,057:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:47:10,177:Remoting - Starting remoting
[INFO ]20161111@19:47:10,598:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:53229]
[INFO ]20161111@19:47:10,607:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:53229]
[INFO ]20161111@19:47:10,632:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 53229.
[INFO ]20161111@19:47:10,705:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:47:10,793:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:47:10,821:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d1560348-8667-40b2-abb2-b45ae9979f7e
[INFO ]20161111@19:47:10,898:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:47:11,102:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:47:11,734:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:47:11,742:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:47:12,070:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:47:12,138:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50954.
[INFO ]20161111@19:47:12,139:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50954
[INFO ]20161111@19:47:12,143:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:47:12,150:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:50954 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 50954)
[INFO ]20161111@19:47:12,156:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:47:14,079:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:47:14,664:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:47:14,670:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50954 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:47:14,690:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:47:15,377:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:47:15,548:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:47:15,594:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:47:15,597:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:47:15,599:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:47:15,610:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:47:15,648:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:47:15,705:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:47:15,750:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:47:15,752:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50954 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:47:15,763:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:47:15,770:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:47:15,780:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:47:15,898:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:47:15,926:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:47:15,996:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:47:16,001:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:47:16,022:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:47:16,022:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:47:16,023:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:47:16,023:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:47:16,024:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:47:16,218:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:47:16,220:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:50954 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:47:16,372:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:47:16,448:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.635 s
[INFO ]20161111@19:47:16,452:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 595 ms on localhost (1/1)
[INFO ]20161111@19:47:16,465:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:47:16,490:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 0.937144 s
[INFO ]20161111@19:47:16,642:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@19:47:16,703:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@19:47:16,725:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@19:47:16,728:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@19:47:16,736:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@19:47:16,744:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@19:47:16,766:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@19:47:16,776:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@19:47:16,789:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@19:47:16,817:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@19:47:16,821:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a66bc343-9dad-44db-970e-553c7f96f643
[INFO ]20161111@19:48:22,463:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:48:23,090:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:48:23,602:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:48:23,603:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:48:23,605:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:48:24,403:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 58699.
[INFO ]20161111@19:48:25,121:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:48:25,252:Remoting - Starting remoting
[INFO ]20161111@19:48:25,674:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:59888]
[INFO ]20161111@19:48:25,675:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:59888]
[INFO ]20161111@19:48:25,705:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 59888.
[INFO ]20161111@19:48:25,773:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:48:25,823:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:48:25,844:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3bfe3f8e-42a7-47d3-a694-4bfa38dd13eb
[INFO ]20161111@19:48:25,889:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:48:26,062:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:48:26,719:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:48:26,730:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:48:27,039:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:48:27,118:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56058.
[INFO ]20161111@19:48:27,120:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56058
[INFO ]20161111@19:48:27,123:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:48:27,130:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:56058 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 56058)
[INFO ]20161111@19:48:27,135:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:48:29,004:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:48:29,633:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:48:29,638:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56058 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:48:29,653:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:48:30,428:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:48:30,608:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:48:30,670:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:48:30,672:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:48:30,674:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:48:30,682:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:48:30,700:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:48:30,776:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:48:30,816:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:48:30,818:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56058 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:48:30,823:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:48:30,828:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:48:30,832:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:48:30,951:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:48:30,976:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:48:31,063:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:48:31,077:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:48:31,114:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:48:31,115:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:48:31,115:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:48:31,116:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:48:31,116:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:48:31,358:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:48:31,361:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:56058 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:48:31,502:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:48:31,603:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.747 s
[INFO ]20161111@19:48:31,604:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 704 ms on localhost (1/1)
[INFO ]20161111@19:48:31,622:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:48:31,642:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.030089 s
[INFO ]20161111@19:48:31,807:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@19:48:31,861:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@19:48:31,887:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@19:48:31,895:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@19:48:31,905:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@19:48:31,918:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@19:48:31,932:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@19:48:31,956:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@19:48:31,968:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@19:48:31,970:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@19:48:31,984:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-151397b2-6b51-496c-a1fd-2cc755aa3380
[INFO ]20161111@19:49:29,371:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:49:30,022:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:49:30,633:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:49:30,635:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:49:30,636:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:49:31,294:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 55210.
[INFO ]20161111@19:49:32,095:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:49:32,220:Remoting - Starting remoting
[INFO ]20161111@19:49:32,644:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:53649]
[INFO ]20161111@19:49:32,658:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:53649]
[INFO ]20161111@19:49:32,685:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 53649.
[INFO ]20161111@19:49:32,768:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:49:32,820:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:49:32,837:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-501b4576-82ab-4d08-993a-0fb6354fcfa8
[INFO ]20161111@19:49:32,865:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:49:33,021:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:49:33,666:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:49:33,670:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:49:33,954:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:49:34,025:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56041.
[INFO ]20161111@19:49:34,026:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56041
[INFO ]20161111@19:49:34,029:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:49:34,037:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:56041 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 56041)
[INFO ]20161111@19:49:34,043:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:49:36,010:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:49:36,654:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:49:36,660:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56041 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:49:36,685:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:49:37,486:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:49:37,694:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:49:37,754:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:49:37,755:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:49:37,760:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:49:37,772:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:49:37,815:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:49:37,891:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:49:37,945:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1858.0 B, free 158.3 KB)
[INFO ]20161111@19:49:37,946:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56041 (size: 1858.0 B, free: 1027.3 MB)
[INFO ]20161111@19:49:37,948:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:49:37,959:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:49:37,974:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:49:38,112:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:49:38,142:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:49:38,202:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:49:38,208:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:49:38,253:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:49:38,254:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:49:38,255:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:49:38,255:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:49:38,257:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:49:38,528:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:49:38,530:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:56041 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:49:38,654:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:49:38,760:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.743 s
[INFO ]20161111@19:49:38,753:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 684 ms on localhost (1/1)
[INFO ]20161111@19:49:38,763:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:49:38,781:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.080670 s
[INFO ]20161111@19:49:38,938:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@19:49:38,997:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@19:49:39,021:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@19:49:39,024:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@19:49:39,035:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@19:49:39,048:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@19:49:39,054:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@19:49:39,065:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@19:49:39,074:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@19:49:39,117:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@19:49:39,118:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-1f01a995-30d0-4650-98cb-5dbb3d29778a
[INFO ]20161111@19:50:07,745:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:50:08,394:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:50:08,884:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:50:08,885:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:50:08,886:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:50:09,490:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 48179.
[INFO ]20161111@19:50:10,230:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:50:10,359:Remoting - Starting remoting
[INFO ]20161111@19:50:10,785:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:55876]
[INFO ]20161111@19:50:10,785:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:55876]
[INFO ]20161111@19:50:10,814:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55876.
[INFO ]20161111@19:50:10,890:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:50:10,943:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:50:10,986:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-65752791-22b1-49ec-9586-a08d5e77501f
[INFO ]20161111@19:50:11,008:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:50:11,177:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:50:11,986:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:50:11,989:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:50:12,337:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:50:12,412:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42455.
[INFO ]20161111@19:50:12,414:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 42455
[INFO ]20161111@19:50:12,416:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:50:12,424:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:42455 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 42455)
[INFO ]20161111@19:50:12,427:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:50:14,482:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:50:15,092:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:50:15,097:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:42455 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:50:15,118:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:50:15,914:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:50:16,116:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:50:16,173:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:50:16,174:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:50:16,178:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:50:16,190:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:50:16,224:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:50:16,299:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:50:16,340:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:50:16,348:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:42455 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:50:16,352:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:50:16,363:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:50:16,369:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:50:16,529:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:50:16,557:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:50:16,640:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:50:16,651:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:50:16,693:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:50:16,694:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:50:16,695:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:50:16,695:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:50:16,698:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:50:16,904:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:50:16,906:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:42455 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:50:17,046:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:50:17,158:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 675 ms on localhost (1/1)
[INFO ]20161111@19:50:17,166:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.749 s
[INFO ]20161111@19:50:17,169:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:50:17,196:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.076212 s
[INFO ]20161111@19:50:17,330:org.apache.spark.SparkContext - Starting job: collect at HW2_Part1.java:193
[INFO ]20161111@19:50:17,336:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part1.java:193) with 1 output partitions
[INFO ]20161111@19:50:17,337:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (collect at HW2_Part1.java:193)
[INFO ]20161111@19:50:17,337:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:50:17,341:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:50:17,345:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:174), which has no missing parents
[INFO ]20161111@19:50:17,356:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 344.0 KB)
[INFO ]20161111@19:50:17,394:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 346.3 KB)
[INFO ]20161111@19:50:17,401:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:42455 (size: 2.2 KB, free: 1027.1 MB)
[INFO ]20161111@19:50:17,413:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:50:17,414:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:174)
[INFO ]20161111@19:50:17,415:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@19:50:17,437:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:50:17,438:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@19:50:17,451:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[ERROR]20161111@19:50:56,500:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NumberFormatException: For input string: "Dividend Yield"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at java.lang.Double.valueOf(Double.java:502)
	at homework2.HW2_Part1$2.call(HW2_Part1.java:180)
	at homework2.HW2_Part1$2.call(HW2_Part1.java:1)
	at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)
	at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1869)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1869)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161111@19:50:58,970:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.NumberFormatException: For input string: "Dividend Yield"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at java.lang.Double.valueOf(Double.java:502)
	at homework2.HW2_Part1$2.call(HW2_Part1.java:180)
	at homework2.HW2_Part1$2.call(HW2_Part1.java:1)
	at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)
	at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:927)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1869)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1869)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161111@19:50:58,986:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
[INFO ]20161111@19:50:59,006:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:50:59,009:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 1
[INFO ]20161111@19:50:59,026:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (collect at HW2_Part1.java:193) failed in 41.570 s
[INFO ]20161111@19:50:59,028:org.apache.spark.scheduler.DAGScheduler - Job 1 failed: collect at HW2_Part1.java:193, took 41.694608 s
[INFO ]20161111@19:59:20,839:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@19:59:21,507:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@19:59:22,037:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@19:59:22,038:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@19:59:22,040:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@19:59:22,632:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 59939.
[INFO ]20161111@19:59:23,421:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@19:59:23,549:Remoting - Starting remoting
[INFO ]20161111@19:59:23,971:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:32806]
[INFO ]20161111@19:59:23,983:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:32806]
[INFO ]20161111@19:59:24,014:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 32806.
[INFO ]20161111@19:59:24,089:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@19:59:24,149:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@19:59:24,170:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-478d889f-037d-44a5-bac7-2cfb34bb068f
[INFO ]20161111@19:59:24,194:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@19:59:24,401:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@19:59:25,068:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@19:59:25,075:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@19:59:25,421:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@19:59:25,518:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34844.
[INFO ]20161111@19:59:25,523:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 34844
[INFO ]20161111@19:59:25,530:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@19:59:25,544:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:34844 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 34844)
[INFO ]20161111@19:59:25,550:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@19:59:27,454:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@19:59:28,062:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@19:59:28,069:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:34844 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@19:59:28,090:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@19:59:28,793:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@19:59:28,970:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@19:59:29,032:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@19:59:29,036:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@19:59:29,038:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:59:29,043:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:59:29,069:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@19:59:29,133:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@19:59:29,177:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@19:59:29,178:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:34844 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@19:59:29,182:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:59:29,193:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@19:59:29,197:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@19:59:29,316:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:59:29,346:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@19:59:29,433:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@19:59:29,443:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@19:59:29,481:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@19:59:29,482:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@19:59:29,483:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@19:59:29,483:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@19:59:29,483:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@19:59:29,717:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@19:59:29,720:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:34844 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@19:59:29,877:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@19:59:29,964:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.734 s
[INFO ]20161111@19:59:29,957:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 697 ms on localhost (1/1)
[INFO ]20161111@19:59:29,967:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@19:59:29,991:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 1.020553 s
[INFO ]20161111@19:59:30,135:org.apache.spark.SparkContext - Starting job: collect at HW2_Part1.java:193
[INFO ]20161111@19:59:30,139:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part1.java:193) with 1 output partitions
[INFO ]20161111@19:59:30,141:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (collect at HW2_Part1.java:193)
[INFO ]20161111@19:59:30,141:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@19:59:30,145:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@19:59:30,154:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:174), which has no missing parents
[INFO ]20161111@19:59:30,173:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 344.1 KB)
[INFO ]20161111@19:59:30,215:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 346.3 KB)
[INFO ]20161111@19:59:30,221:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:34844 (size: 2.3 KB, free: 1027.1 MB)
[INFO ]20161111@19:59:30,223:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@19:59:30,225:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:174)
[INFO ]20161111@19:59:30,226:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@19:59:30,244:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@19:59:30,246:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@19:59:30,258:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@20:04:00,114:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@20:04:00,781:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@20:04:01,202:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@20:04:01,203:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@20:04:01,205:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@20:04:01,733:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 58962.
[INFO ]20161111@20:04:02,344:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@20:04:02,423:Remoting - Starting remoting
[INFO ]20161111@20:04:02,783:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:33223]
[INFO ]20161111@20:04:02,784:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:33223]
[INFO ]20161111@20:04:02,812:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 33223.
[INFO ]20161111@20:04:02,854:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@20:04:02,898:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@20:04:02,944:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a06c0329-e0f4-405e-bcd5-179edbe2cd5b
[INFO ]20161111@20:04:02,981:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@20:04:03,188:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@20:04:03,977:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@20:04:03,985:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@20:04:04,219:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@20:04:04,268:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49026.
[INFO ]20161111@20:04:04,269:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 49026
[INFO ]20161111@20:04:04,271:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@20:04:04,278:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:49026 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 49026)
[INFO ]20161111@20:04:04,281:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@20:04:05,578:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@20:04:06,021:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@20:04:06,027:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:49026 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@20:04:06,042:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:111
[INFO ]20161111@20:04:06,620:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@20:04:06,774:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:127
[INFO ]20161111@20:04:06,823:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:127) with 1 output partitions
[INFO ]20161111@20:04:06,824:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:127)
[INFO ]20161111@20:04:06,829:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@20:04:06,836:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@20:04:06,859:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111), which has no missing parents
[INFO ]20161111@20:04:06,909:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@20:04:06,936:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@20:04:06,941:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:49026 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@20:04:06,942:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@20:04:06,947:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:111)
[INFO ]20161111@20:04:06,952:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@20:04:07,045:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@20:04:07,070:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@20:04:07,128:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@20:04:07,131:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@20:04:07,153:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@20:04:07,153:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@20:04:07,154:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@20:04:07,154:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@20:04:07,154:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@20:04:07,352:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@20:04:07,354:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:49026 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@20:04:07,461:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@20:04:07,517:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:127) finished in 0.546 s
[INFO ]20161111@20:04:07,518:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 513 ms on localhost (1/1)
[INFO ]20161111@20:04:07,529:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@20:04:07,543:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:127, took 0.763150 s
[INFO ]20161111@20:04:07,604:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:193
[INFO ]20161111@20:04:07,609:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:193) with 1 output partitions
[INFO ]20161111@20:04:07,610:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:193)
[INFO ]20161111@20:04:07,610:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@20:04:07,613:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@20:04:07,617:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:174), which has no missing parents
[INFO ]20161111@20:04:07,640:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 343.9 KB)
[INFO ]20161111@20:04:07,672:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161111@20:04:07,689:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 346.1 KB)
[INFO ]20161111@20:04:07,701:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:49026 (size: 2.2 KB, free: 1027.1 MB)
[INFO ]20161111@20:04:07,703:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@20:04:07,703:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:174)
[INFO ]20161111@20:04:07,703:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@20:04:07,738:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@20:04:07,739:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@20:04:07,750:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:49026 in memory (size: 1859.0 B, free: 1027.1 MB)
[INFO ]20161111@20:04:07,769:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@20:04:07,929:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2003 bytes result sent to driver
[INFO ]20161111@20:04:07,945:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:193) finished in 0.223 s
[INFO ]20161111@20:04:07,947:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:193, took 0.338861 s
[INFO ]20161111@20:04:07,955:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 220 ms on localhost (1/1)
[INFO ]20161111@20:04:07,956:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@20:04:08,027:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@20:04:08,042:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@20:04:08,052:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@20:04:08,054:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@20:04:08,056:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@20:04:08,059:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@20:04:08,070:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@20:04:08,082:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@20:04:08,077:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@20:04:08,101:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@20:04:08,102:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-e084a2b7-1d49-46a6-b73d-d8aff795d219
[INFO ]20161111@21:36:26,333:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:36:26,880:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:36:27,292:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:36:27,295:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:36:27,296:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:36:27,754:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45480.
[INFO ]20161111@21:36:28,334:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:36:28,414:Remoting - Starting remoting
[INFO ]20161111@21:36:28,724:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:57900]
[INFO ]20161111@21:36:28,728:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:57900]
[INFO ]20161111@21:36:28,760:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 57900.
[INFO ]20161111@21:36:28,803:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:36:28,844:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:36:28,895:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-66d8098d-45ec-44ea-9ee2-2171f10d0944
[INFO ]20161111@21:36:28,933:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:36:29,069:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:36:29,656:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:36:29,661:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:36:29,864:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:36:29,924:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38179.
[INFO ]20161111@21:36:29,925:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 38179
[INFO ]20161111@21:36:29,927:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:36:29,931:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:38179 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 38179)
[INFO ]20161111@21:36:29,934:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:36:31,271:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:36:31,691:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:36:31,696:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:38179 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:36:31,707:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:36:32,261:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:36:32,410:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:131
[INFO ]20161111@21:36:32,457:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:131) with 1 output partitions
[INFO ]20161111@21:36:32,460:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:131)
[INFO ]20161111@21:36:32,462:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:36:32,470:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:36:32,494:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115), which has no missing parents
[INFO ]20161111@21:36:32,531:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@21:36:32,565:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@21:36:32,572:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:38179 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@21:36:32,572:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:36:32,581:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115)
[INFO ]20161111@21:36:32,583:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:36:32,675:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:36:32,695:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:36:32,740:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:36:32,743:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:36:32,758:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:36:32,759:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:36:32,759:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:36:32,759:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:36:32,759:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:36:32,961:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@21:36:32,962:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:38179 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:36:33,051:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@21:36:33,114:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 462 ms on localhost (1/1)
[INFO ]20161111@21:36:33,121:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:131) finished in 0.503 s
[INFO ]20161111@21:36:33,124:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:36:33,141:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:131, took 0.725085 s
[INFO ]20161111@21:36:33,176:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161111@21:36:33,228:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:38179 in memory (size: 1859.0 B, free: 1027.1 MB)
[INFO ]20161111@21:36:33,300:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:36:33,313:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:36:33,322:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:36:33,326:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:36:33,329:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:36:33,333:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:36:33,350:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:36:33,352:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:36:33,361:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:36:33,365:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:36:33,377:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-0c784f9c-9533-4510-ac78-012df5d9d786
[INFO ]20161111@21:37:40,743:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:37:41,376:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:37:41,837:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:37:41,838:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:37:41,839:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:37:42,409:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46112.
[INFO ]20161111@21:37:43,125:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:37:43,232:Remoting - Starting remoting
[INFO ]20161111@21:37:43,639:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:58338]
[INFO ]20161111@21:37:43,647:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:58338]
[INFO ]20161111@21:37:43,675:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 58338.
[INFO ]20161111@21:37:43,752:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:37:43,793:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:37:43,811:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-18c1befd-a735-4d44-bb72-f601dabd3302
[INFO ]20161111@21:37:43,831:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:37:43,991:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:37:44,603:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:37:44,611:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:37:44,901:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:37:44,973:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 48500.
[INFO ]20161111@21:37:44,975:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 48500
[INFO ]20161111@21:37:44,976:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:37:44,982:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:48500 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 48500)
[INFO ]20161111@21:37:44,986:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:37:46,900:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:37:47,497:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:37:47,504:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:48500 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:37:47,525:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:37:48,214:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:37:48,379:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:131
[INFO ]20161111@21:37:48,433:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:131) with 1 output partitions
[INFO ]20161111@21:37:48,435:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:131)
[INFO ]20161111@21:37:48,436:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:37:48,442:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:37:48,467:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115), which has no missing parents
[INFO ]20161111@21:37:48,502:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@21:37:48,554:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@21:37:48,558:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:48500 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@21:37:48,561:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:37:48,566:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115)
[INFO ]20161111@21:37:48,570:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:37:48,678:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:37:48,703:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:37:48,776:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:37:48,784:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:37:48,823:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:37:48,826:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:37:48,826:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:37:48,826:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:37:48,827:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:37:49,054:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@21:37:49,055:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:48500 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:37:49,178:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@21:37:49,283:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:131) finished in 0.681 s
[INFO ]20161111@21:37:49,279:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 650 ms on localhost (1/1)
[INFO ]20161111@21:37:49,289:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:37:49,308:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:131, took 0.907561 s
[INFO ]20161111@21:37:49,456:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:37:49,513:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:37:49,532:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:37:49,536:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:37:49,546:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:37:49,559:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:37:49,577:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:37:49,587:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:37:49,599:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:37:49,615:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:37:49,631:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-917060f9-eb94-4bfc-afab-82233d30a03a
[INFO ]20161111@21:38:08,742:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:38:09,376:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:38:09,980:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:38:09,981:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:38:09,985:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:38:10,631:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57622.
[INFO ]20161111@21:38:11,322:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:38:11,417:Remoting - Starting remoting
[INFO ]20161111@21:38:11,821:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:55319]
[INFO ]20161111@21:38:11,826:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:55319]
[INFO ]20161111@21:38:11,855:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55319.
[INFO ]20161111@21:38:11,926:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:38:11,974:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:38:11,992:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-05330128-1045-4fb0-92e7-b199dc46340b
[INFO ]20161111@21:38:12,024:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:38:12,173:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:38:12,790:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:38:12,793:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:38:13,061:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:38:13,117:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60980.
[INFO ]20161111@21:38:13,118:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 60980
[INFO ]20161111@21:38:13,122:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:38:13,127:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:60980 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 60980)
[INFO ]20161111@21:38:13,131:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:38:14,972:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:38:15,723:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:38:15,731:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:60980 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:38:15,757:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:38:16,526:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:38:16,727:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:131
[INFO ]20161111@21:38:16,786:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:131) with 1 output partitions
[INFO ]20161111@21:38:16,789:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:131)
[INFO ]20161111@21:38:16,791:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:38:16,800:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:38:16,840:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115), which has no missing parents
[INFO ]20161111@21:38:16,881:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@21:38:16,936:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@21:38:16,937:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:60980 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@21:38:16,945:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:38:16,965:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115)
[INFO ]20161111@21:38:16,974:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:38:17,133:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:38:17,170:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:38:17,288:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:38:17,291:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:38:17,322:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:38:17,323:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:38:17,323:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:38:17,323:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:38:17,324:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:38:17,662:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@21:38:17,663:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:60980 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:38:17,857:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@21:38:17,934:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:131) finished in 0.920 s
[INFO ]20161111@21:38:17,930:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 862 ms on localhost (1/1)
[INFO ]20161111@21:38:17,938:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:38:17,957:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:131, took 1.226728 s
[INFO ]20161111@21:38:18,127:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:38:18,182:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:38:18,207:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:38:18,212:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:38:18,222:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:38:18,246:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:38:18,247:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:38:18,281:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:38:18,283:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:38:18,291:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-722f8b54-626f-4355-9191-697257eaf82b
[INFO ]20161111@21:38:18,293:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:38:41,234:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:38:41,877:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:38:42,396:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:38:42,397:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:38:42,398:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:38:42,967:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 54658.
[INFO ]20161111@21:38:43,624:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:38:43,741:Remoting - Starting remoting
[INFO ]20161111@21:38:44,149:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:41649]
[INFO ]20161111@21:38:44,159:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:41649]
[INFO ]20161111@21:38:44,186:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 41649.
[INFO ]20161111@21:38:44,249:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:38:44,304:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:38:44,326:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-5b88c0d0-2697-428e-b0df-7344ee26ea2e
[INFO ]20161111@21:38:44,373:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:38:44,537:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:38:45,140:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:38:45,143:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:38:45,417:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:38:45,456:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59566.
[INFO ]20161111@21:38:45,457:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59566
[INFO ]20161111@21:38:45,459:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:38:45,463:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:59566 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 59566)
[INFO ]20161111@21:38:45,466:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:38:47,379:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:38:47,986:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:38:47,991:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59566 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:38:48,012:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:38:48,807:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:38:49,005:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:131
[INFO ]20161111@21:38:49,055:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:131) with 1 output partitions
[INFO ]20161111@21:38:49,056:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:131)
[INFO ]20161111@21:38:49,060:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:38:49,071:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:38:49,101:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115), which has no missing parents
[INFO ]20161111@21:38:49,144:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@21:38:49,200:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@21:38:49,204:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59566 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@21:38:49,205:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:38:49,224:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115)
[INFO ]20161111@21:38:49,228:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:38:49,388:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:38:49,418:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:38:49,500:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:38:49,512:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:38:49,552:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:38:49,553:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:38:49,556:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:38:49,556:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:38:49,558:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:38:49,778:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@21:38:49,780:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:59566 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:38:49,913:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@21:38:50,020:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:131) finished in 0.746 s
[INFO ]20161111@21:38:50,025:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 682 ms on localhost (1/1)
[INFO ]20161111@21:38:50,034:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:38:50,058:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:131, took 1.048425 s
[INFO ]20161111@21:38:50,253:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:38:50,323:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:38:50,343:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:38:50,348:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:38:50,360:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:38:50,376:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:38:50,393:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:38:50,400:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:38:50,393:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:38:50,437:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:38:50,451:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c6904e92-4547-4339-8f37-115ab8cbf4a2
[INFO ]20161111@21:39:11,403:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:39:11,937:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:39:12,356:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:39:12,357:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:39:12,358:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:39:12,838:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41787.
[INFO ]20161111@21:39:13,393:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:39:13,473:Remoting - Starting remoting
[INFO ]20161111@21:39:13,799:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:42025]
[INFO ]20161111@21:39:13,807:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:42025]
[INFO ]20161111@21:39:13,824:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 42025.
[INFO ]20161111@21:39:13,878:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:39:13,918:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:39:13,962:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-fa3d41cb-307a-4837-9d0f-f92e4ca87d6b
[INFO ]20161111@21:39:13,995:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:39:14,132:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:39:14,726:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:39:14,728:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:39:14,958:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:39:15,010:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49423.
[INFO ]20161111@21:39:15,012:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 49423
[INFO ]20161111@21:39:15,013:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:39:15,020:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:49423 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 49423)
[INFO ]20161111@21:39:15,026:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:39:16,285:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:39:16,697:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:39:16,705:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:49423 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:39:16,713:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:39:17,303:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:39:17,437:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:131
[INFO ]20161111@21:39:17,481:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:131) with 1 output partitions
[INFO ]20161111@21:39:17,482:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:131)
[INFO ]20161111@21:39:17,485:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:39:17,493:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:39:17,525:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115), which has no missing parents
[INFO ]20161111@21:39:17,559:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@21:39:17,587:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@21:39:17,590:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:49423 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@21:39:17,596:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:39:17,604:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115)
[INFO ]20161111@21:39:17,608:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:39:17,715:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:39:17,736:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:39:17,784:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:39:17,787:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:39:17,815:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:39:17,816:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:39:17,816:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:39:17,816:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:39:17,818:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:39:18,006:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@21:39:18,008:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:49423 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:39:18,098:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@21:39:18,152:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:131) finished in 0.524 s
[INFO ]20161111@21:39:18,155:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 497 ms on localhost (1/1)
[INFO ]20161111@21:39:18,165:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:39:18,179:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:131, took 0.738167 s
[INFO ]20161111@21:39:18,222:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161111@21:39:18,260:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:49423 in memory (size: 1859.0 B, free: 1027.1 MB)
[INFO ]20161111@21:39:18,282:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:203
[INFO ]20161111@21:39:18,286:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:203) with 1 output partitions
[INFO ]20161111@21:39:18,287:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:203)
[INFO ]20161111@21:39:18,288:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:39:18,292:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:39:18,295:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:185), which has no missing parents
[INFO ]20161111@21:39:18,324:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 339.5 KB)
[INFO ]20161111@21:39:18,349:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 342.0 KB)
[INFO ]20161111@21:39:18,352:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:49423 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161111@21:39:18,354:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:39:18,357:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:185)
[INFO ]20161111@21:39:18,362:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@21:39:18,376:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:39:18,376:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@21:39:18,389:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@21:39:18,685:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2063 bytes result sent to driver
[INFO ]20161111@21:39:18,703:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:203) finished in 0.320 s
[INFO ]20161111@21:39:18,705:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:203, took 0.422270 s
[INFO ]20161111@21:39:18,709:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 328 ms on localhost (1/1)
[INFO ]20161111@21:39:18,709:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:39:18,785:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:39:18,799:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:39:18,809:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:39:18,810:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:39:18,813:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:39:18,815:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:39:18,827:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:39:18,831:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:39:18,833:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:39:18,855:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:39:18,856:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-8a45347f-9959-45f0-ad2f-2e80d0ae1cfa
[INFO ]20161111@21:40:07,640:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:40:08,174:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:40:08,578:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:40:08,579:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:40:08,580:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:40:09,099:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43966.
[INFO ]20161111@21:40:09,695:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:40:09,789:Remoting - Starting remoting
[INFO ]20161111@21:40:10,103:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:52682]
[INFO ]20161111@21:40:10,107:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:52682]
[INFO ]20161111@21:40:10,131:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 52682.
[INFO ]20161111@21:40:10,175:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:40:10,223:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:40:10,255:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a2a6557a-8837-44db-a863-c5d23706f9ce
[INFO ]20161111@21:40:10,292:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:40:10,425:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:40:11,017:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:40:11,024:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:40:11,236:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:40:11,283:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50992.
[INFO ]20161111@21:40:11,284:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50992
[INFO ]20161111@21:40:11,286:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:40:11,290:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:50992 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 50992)
[INFO ]20161111@21:40:11,292:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:40:12,586:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:40:13,013:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:40:13,022:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50992 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:40:13,032:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:40:13,608:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:40:13,744:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:131
[INFO ]20161111@21:40:13,780:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:131) with 1 output partitions
[INFO ]20161111@21:40:13,781:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:131)
[INFO ]20161111@21:40:13,784:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:40:13,791:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:40:13,821:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115), which has no missing parents
[INFO ]20161111@21:40:13,858:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@21:40:13,903:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@21:40:13,904:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50992 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@21:40:13,906:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:40:13,914:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115)
[INFO ]20161111@21:40:13,924:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:40:14,025:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:40:14,047:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:40:14,110:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:40:14,113:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:40:14,131:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:40:14,131:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:40:14,131:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:40:14,132:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:40:14,134:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:40:14,326:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@21:40:14,327:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:50992 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:40:14,415:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@21:40:14,476:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:131) finished in 0.522 s
[INFO ]20161111@21:40:14,475:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 491 ms on localhost (1/1)
[INFO ]20161111@21:40:14,486:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:40:14,500:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:131, took 0.751092 s
[INFO ]20161111@21:40:14,527:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161111@21:40:14,580:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:50992 in memory (size: 1859.0 B, free: 1027.1 MB)
[INFO ]20161111@21:40:14,607:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:203
[INFO ]20161111@21:40:14,612:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:203) with 1 output partitions
[INFO ]20161111@21:40:14,612:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:203)
[INFO ]20161111@21:40:14,612:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:40:14,620:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:40:14,623:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:185), which has no missing parents
[INFO ]20161111@21:40:14,656:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 339.5 KB)
[INFO ]20161111@21:40:14,679:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 342.0 KB)
[INFO ]20161111@21:40:14,683:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:50992 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161111@21:40:14,684:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:40:14,687:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:185)
[INFO ]20161111@21:40:14,688:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@21:40:14,697:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:40:14,698:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@21:40:14,715:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@21:40:14,890:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2063 bytes result sent to driver
[INFO ]20161111@21:40:14,916:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:203) finished in 0.211 s
[INFO ]20161111@21:40:14,918:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:203, took 0.309540 s
[INFO ]20161111@21:40:14,921:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 218 ms on localhost (1/1)
[INFO ]20161111@21:40:14,922:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:40:15,000:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:40:15,012:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:40:15,022:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:40:15,024:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:40:15,026:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:40:15,028:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:40:15,041:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:40:15,044:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:40:15,055:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:40:15,054:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:40:15,065:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-acf7b50d-cd46-457b-b4e3-3ab4f571e20f
[INFO ]20161111@21:44:11,485:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:44:12,042:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:44:12,490:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:44:12,490:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:44:12,491:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:44:13,018:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34896.
[INFO ]20161111@21:44:13,594:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:44:13,678:Remoting - Starting remoting
[INFO ]20161111@21:44:13,996:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:55708]
[INFO ]20161111@21:44:13,998:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:55708]
[INFO ]20161111@21:44:14,030:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55708.
[INFO ]20161111@21:44:14,077:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:44:14,116:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:44:14,158:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-5dae0eb2-04ff-441d-8adb-26f0243bd332
[INFO ]20161111@21:44:14,198:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:44:14,339:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:44:14,941:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:44:14,945:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:44:15,179:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:44:15,233:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51713.
[INFO ]20161111@21:44:15,234:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 51713
[INFO ]20161111@21:44:15,236:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:44:15,240:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:51713 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 51713)
[INFO ]20161111@21:44:15,245:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:44:16,589:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:44:17,003:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:44:17,009:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:51713 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:44:17,024:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:44:17,564:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:44:17,702:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:131
[INFO ]20161111@21:44:17,747:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:131) with 1 output partitions
[INFO ]20161111@21:44:17,749:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:131)
[INFO ]20161111@21:44:17,752:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:44:17,762:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:44:17,780:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115), which has no missing parents
[INFO ]20161111@21:44:17,831:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161111@21:44:17,870:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161111@21:44:17,875:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:51713 (size: 1859.0 B, free: 1027.3 MB)
[INFO ]20161111@21:44:17,876:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:44:17,882:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:115)
[INFO ]20161111@21:44:17,885:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:44:17,999:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:44:18,017:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:44:18,077:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:44:18,080:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:44:18,112:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:44:18,113:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:44:18,114:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:44:18,115:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:44:18,115:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:44:18,305:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161111@21:44:18,311:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:51713 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:44:18,411:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161111@21:44:18,465:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:131) finished in 0.544 s
[INFO ]20161111@21:44:18,469:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 510 ms on localhost (1/1)
[INFO ]20161111@21:44:18,478:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:44:18,492:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:131, took 0.787207 s
[INFO ]20161111@21:44:18,534:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161111@21:44:18,567:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:51713 in memory (size: 1859.0 B, free: 1027.1 MB)
[INFO ]20161111@21:44:18,582:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:204
[INFO ]20161111@21:44:18,590:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:204) with 1 output partitions
[INFO ]20161111@21:44:18,590:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:204)
[INFO ]20161111@21:44:18,591:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:44:18,595:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:44:18,597:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161111@21:44:18,618:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 339.5 KB)
[INFO ]20161111@21:44:18,647:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 342.0 KB)
[INFO ]20161111@21:44:18,651:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:51713 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161111@21:44:18,654:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:44:18,654:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161111@21:44:18,655:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161111@21:44:18,669:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:44:18,670:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161111@21:44:18,684:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161111@21:44:18,920:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2063 bytes result sent to driver
[INFO ]20161111@21:44:18,942:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:204) finished in 0.266 s
[INFO ]20161111@21:44:18,943:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:204, took 0.361199 s
[INFO ]20161111@21:44:18,946:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 273 ms on localhost (1/1)
[INFO ]20161111@21:44:18,949:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:44:19,021:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:44:19,037:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:44:19,046:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:44:19,048:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:44:19,050:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:44:19,053:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:44:19,066:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:44:19,072:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:44:19,079:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:44:19,090:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:44:19,097:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-0cba9b1d-3449-4461-b76d-b8ad5d4b56dc
[INFO ]20161111@21:44:53,808:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161111@21:44:54,359:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161111@21:44:54,779:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161111@21:44:54,780:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161111@21:44:54,781:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161111@21:44:55,262:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41151.
[INFO ]20161111@21:44:55,871:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161111@21:44:55,956:Remoting - Starting remoting
[INFO ]20161111@21:44:56,281:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:56937]
[INFO ]20161111@21:44:56,282:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:56937]
[INFO ]20161111@21:44:56,307:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 56937.
[INFO ]20161111@21:44:56,352:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161111@21:44:56,391:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161111@21:44:56,433:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a6a51aaa-3a48-49ad-9658-f2b16e1f5df5
[INFO ]20161111@21:44:56,468:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161111@21:44:56,604:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161111@21:44:57,252:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161111@21:44:57,255:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161111@21:44:57,519:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161111@21:44:57,573:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 48583.
[INFO ]20161111@21:44:57,575:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 48583
[INFO ]20161111@21:44:57,576:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161111@21:44:57,580:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:48583 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 48583)
[INFO ]20161111@21:44:57,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161111@21:44:59,021:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161111@21:44:59,422:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161111@21:44:59,430:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:48583 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161111@21:44:59,444:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161111@21:45:00,573:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161111@21:45:00,632:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:204
[INFO ]20161111@21:45:00,657:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:204) with 1 output partitions
[INFO ]20161111@21:45:00,658:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:204)
[INFO ]20161111@21:45:00,660:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161111@21:45:00,664:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161111@21:45:00,695:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161111@21:45:00,832:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161111@21:45:00,858:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161111@21:45:00,860:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:48583 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161111@21:45:00,861:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161111@21:45:00,869:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161111@21:45:00,872:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161111@21:45:00,967:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161111@21:45:00,982:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161111@21:45:01,024:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161111@21:45:01,027:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161111@21:45:01,064:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161111@21:45:01,065:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161111@21:45:01,066:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161111@21:45:01,066:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161111@21:45:01,066:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161111@21:45:01,259:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161111@21:45:01,263:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:48583 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161111@21:45:01,561:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2643 bytes result sent to driver
[INFO ]20161111@21:45:01,611:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:204) finished in 0.709 s
[INFO ]20161111@21:45:01,619:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 683 ms on localhost (1/1)
[INFO ]20161111@21:45:01,621:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161111@21:45:01,636:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:204, took 1.001066 s
[INFO ]20161111@21:45:01,724:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161111@21:45:01,744:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161111@21:45:01,760:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161111@21:45:01,760:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161111@21:45:01,764:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161111@21:45:01,767:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161111@21:45:01,780:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161111@21:45:01,788:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161111@21:45:01,784:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161111@21:45:01,804:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161111@21:45:01,812:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d1e3b3a3-0d8b-4b20-b55c-a17589081100
[INFO ]20161112@15:50:34,680:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@15:50:35,476:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@15:50:36,074:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@15:50:36,075:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@15:50:36,077:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@15:50:36,728:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 55732.
[INFO ]20161112@15:50:37,539:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@15:50:37,671:Remoting - Starting remoting
[INFO ]20161112@15:50:38,131:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:50360]
[INFO ]20161112@15:50:38,142:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:50360]
[INFO ]20161112@15:50:38,176:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 50360.
[INFO ]20161112@15:50:38,248:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@15:50:38,295:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@15:50:38,354:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-989cbbea-125b-44cf-ba9b-b0d91de8e9b8
[INFO ]20161112@15:50:38,381:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@15:50:38,659:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@15:50:39,375:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@15:50:39,384:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@15:50:39,687:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@15:50:39,757:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40258.
[INFO ]20161112@15:50:39,758:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 40258
[INFO ]20161112@15:50:39,761:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@15:50:39,771:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:40258 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 40258)
[INFO ]20161112@15:50:39,776:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@15:50:41,921:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@15:50:42,806:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@15:50:42,816:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40258 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@15:50:42,834:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@15:50:43,779:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@15:50:43,835:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:202
[INFO ]20161112@15:50:43,891:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:202) with 1 output partitions
[INFO ]20161112@15:50:43,900:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:202)
[INFO ]20161112@15:50:43,907:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@15:50:43,920:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@15:50:43,952:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@15:50:44,125:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@15:50:44,171:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@15:50:44,174:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40258 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@15:50:44,175:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@15:50:44,195:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@15:50:44,204:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@15:50:44,344:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@15:50:44,369:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@15:50:44,469:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@15:50:44,476:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@15:50:44,530:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@15:50:44,531:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@15:50:44,531:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@15:50:44,531:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@15:50:44,531:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@15:50:44,802:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@15:50:44,803:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:40258 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@15:50:45,314:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2643 bytes result sent to driver
[INFO ]20161112@15:50:45,407:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1114 ms on localhost (1/1)
[INFO ]20161112@15:50:45,420:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:202) finished in 1.165 s
[INFO ]20161112@15:50:45,425:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@15:50:45,448:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:202, took 1.609614 s
[INFO ]20161112@15:50:45,484:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:205
[INFO ]20161112@15:50:45,488:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:205) with 1 output partitions
[INFO ]20161112@15:50:45,489:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:205)
[INFO ]20161112@15:50:45,490:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@15:50:45,496:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@15:50:45,504:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@15:50:45,525:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@15:50:45,560:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@15:50:45,566:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40258 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@15:50:45,568:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@15:50:45,571:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@15:50:45,572:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@15:50:45,586:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@15:50:45,590:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@15:50:45,620:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@15:50:45,840:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2063 bytes result sent to driver
[INFO ]20161112@15:50:45,863:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:205) finished in 0.276 s
[INFO ]20161112@15:50:45,865:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:205, took 0.379311 s
[INFO ]20161112@15:50:45,868:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 279 ms on localhost (1/1)
[INFO ]20161112@15:50:45,869:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@15:50:45,939:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161112@15:50:45,995:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161112@15:50:46,019:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161112@15:50:46,022:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161112@15:50:46,034:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161112@15:50:46,048:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161112@15:50:46,054:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161112@15:50:46,062:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161112@15:50:46,064:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161112@15:50:46,075:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161112@15:50:46,078:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d942a02a-dbef-41ac-8500-052006d35f0a
[INFO ]20161112@15:51:18,748:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@15:51:19,606:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@15:51:20,329:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@15:51:20,330:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@15:51:20,332:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@15:51:21,237:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56964.
[INFO ]20161112@15:51:22,248:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@15:51:22,416:Remoting - Starting remoting
[INFO ]20161112@15:51:23,011:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:32845]
[INFO ]20161112@15:51:23,021:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:32845]
[INFO ]20161112@15:51:23,057:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 32845.
[INFO ]20161112@15:51:23,163:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@15:51:23,224:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@15:51:23,248:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-99b1e296-2279-48dd-937b-2ff35869e45a
[INFO ]20161112@15:51:23,306:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@15:51:23,538:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@15:51:24,362:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@15:51:24,376:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@15:51:24,745:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@15:51:24,796:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34976.
[INFO ]20161112@15:51:24,798:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 34976
[INFO ]20161112@15:51:24,801:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@15:51:24,810:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:34976 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 34976)
[INFO ]20161112@15:51:24,814:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@15:51:27,580:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@15:51:28,433:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@15:51:28,443:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:34976 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@15:51:28,466:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@15:51:29,577:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@15:51:29,653:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:202
[INFO ]20161112@15:51:29,723:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:202) with 1 output partitions
[INFO ]20161112@15:51:29,728:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:202)
[INFO ]20161112@15:51:29,733:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@15:51:29,747:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@15:51:29,812:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@15:51:30,011:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@15:51:30,060:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@15:51:30,063:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:34976 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@15:51:30,065:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@15:51:30,089:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@15:51:30,097:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@15:51:30,273:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@15:51:30,304:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@15:51:30,428:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@15:51:30,438:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@15:51:30,494:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@15:51:30,496:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@15:51:30,496:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@15:51:30,496:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@15:51:30,498:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@15:51:30,842:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@15:51:30,843:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:34976 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@15:59:20,449:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@15:59:21,289:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@15:59:21,945:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@15:59:21,947:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@15:59:21,948:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@15:59:22,669:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38346.
[INFO ]20161112@15:59:23,485:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@15:59:23,627:Remoting - Starting remoting
[INFO ]20161112@15:59:24,090:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:60870]
[INFO ]20161112@15:59:24,101:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:60870]
[INFO ]20161112@15:59:24,126:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 60870.
[INFO ]20161112@15:59:24,188:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@15:59:24,238:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@15:59:24,283:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f4ec5df7-35af-4748-a18c-dc9e40d26f87
[INFO ]20161112@15:59:24,310:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@15:59:24,527:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@15:59:25,351:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@15:59:25,359:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@15:59:25,679:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@15:59:25,749:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39975.
[INFO ]20161112@15:59:25,755:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 39975
[INFO ]20161112@15:59:25,756:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@15:59:25,765:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:39975 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 39975)
[INFO ]20161112@15:59:25,774:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@15:59:27,847:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@15:59:28,617:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@15:59:28,624:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:39975 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@15:59:28,646:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@15:59:29,628:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@15:59:29,678:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:204
[INFO ]20161112@15:59:29,719:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:204) with 1 output partitions
[INFO ]20161112@15:59:29,726:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:204)
[INFO ]20161112@15:59:29,728:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@15:59:29,743:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@15:59:29,778:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@15:59:29,954:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@15:59:29,993:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@15:59:29,994:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:39975 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@15:59:29,995:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@15:59:30,007:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@15:59:30,010:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@15:59:30,175:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@15:59:30,199:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@15:59:30,299:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@15:59:30,311:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@15:59:30,364:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@15:59:30,365:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@15:59:30,365:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@15:59:30,365:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@15:59:30,366:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@15:59:30,635:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@15:59:30,639:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:39975 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@15:59:31,172:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@15:59:31,241:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1118 ms on localhost (1/1)
[INFO ]20161112@15:59:31,247:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:204) finished in 1.194 s
[INFO ]20161112@15:59:31,257:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@15:59:31,274:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:204, took 1.595899 s
[INFO ]20161112@15:59:31,311:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:207
[INFO ]20161112@15:59:31,315:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:207) with 1 output partitions
[INFO ]20161112@15:59:31,316:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:207)
[INFO ]20161112@15:59:31,317:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@15:59:31,323:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@15:59:31,331:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@15:59:31,347:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@15:59:31,382:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@15:59:31,386:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:39975 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@15:59:31,388:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@15:59:31,394:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@15:59:31,395:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@15:59:31,413:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@15:59:31,414:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@15:59:31,439:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@15:59:31,622:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2058 bytes result sent to driver
[INFO ]20161112@15:59:31,650:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:207) finished in 0.229 s
[INFO ]20161112@15:59:31,651:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:207, took 0.337925 s
[INFO ]20161112@15:59:31,658:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 238 ms on localhost (1/1)
[INFO ]20161112@15:59:31,658:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@15:59:31,733:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161112@15:59:31,769:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161112@15:59:31,788:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161112@15:59:31,791:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161112@15:59:31,803:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161112@15:59:31,834:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161112@15:59:31,834:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161112@15:59:31,837:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161112@15:59:31,839:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161112@15:59:31,861:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161112@15:59:31,870:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-40170ebc-06ce-4bb5-9a16-2330192519ab
[INFO ]20161112@16:04:28,886:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:04:29,809:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:04:30,483:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:04:30,485:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:04:30,487:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:04:31,324:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 47699.
[INFO ]20161112@16:04:32,303:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:04:32,477:Remoting - Starting remoting
[INFO ]20161112@16:04:33,027:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:48898]
[INFO ]20161112@16:04:33,046:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:48898]
[INFO ]20161112@16:04:33,089:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 48898.
[INFO ]20161112@16:04:33,193:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:04:33,263:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:04:33,297:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e9de1679-9628-4c2e-873b-70f9d625ab57
[INFO ]20161112@16:04:33,355:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:04:33,580:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@16:04:34,405:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@16:04:34,415:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@16:04:34,780:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:04:34,869:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38985.
[INFO ]20161112@16:04:34,871:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 38985
[INFO ]20161112@16:04:34,875:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:04:34,884:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:38985 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 38985)
[INFO ]20161112@16:04:34,891:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:04:37,469:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:04:38,311:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:04:38,320:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:38985 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:04:38,340:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:04:39,663:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:04:39,765:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:211
[INFO ]20161112@16:04:39,875:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:211) with 1 output partitions
[INFO ]20161112@16:04:39,882:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:211)
[INFO ]20161112@16:04:39,888:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:04:39,911:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:04:39,965:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:04:40,281:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:04:40,341:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:04:40,349:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:38985 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:04:40,351:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:04:40,376:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:04:40,402:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:04:40,645:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:04:40,686:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:04:40,783:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:04:40,789:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:04:40,862:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:04:40,864:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:04:40,864:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:04:40,864:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:04:40,867:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:04:41,176:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:04:41,181:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:38985 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:06:59,411:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:07:00,425:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:07:01,144:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:07:01,147:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:07:01,148:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:07:01,927:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46409.
[INFO ]20161112@16:07:02,924:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:07:03,095:Remoting - Starting remoting
[INFO ]20161112@16:07:03,703:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:43470]
[INFO ]20161112@16:07:03,722:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:43470]
[INFO ]20161112@16:07:03,763:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 43470.
[INFO ]20161112@16:07:03,845:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:07:03,957:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:07:04,012:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ac7a457a-0cbe-4dd9-a10d-4089f43116c3
[INFO ]20161112@16:07:04,047:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:07:04,258:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@16:07:05,094:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@16:07:05,109:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@16:07:05,452:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:07:05,502:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37917.
[INFO ]20161112@16:07:05,503:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 37917
[INFO ]20161112@16:07:05,510:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:07:05,515:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:37917 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 37917)
[INFO ]20161112@16:07:05,519:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:07:08,223:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:07:09,114:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:07:09,128:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:37917 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:07:09,171:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:07:10,432:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:07:10,506:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:211
[INFO ]20161112@16:07:10,584:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:211) with 1 output partitions
[INFO ]20161112@16:07:10,594:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:211)
[INFO ]20161112@16:07:10,597:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:07:10,611:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:07:10,641:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:07:10,870:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:07:10,916:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:07:10,920:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:37917 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:07:10,921:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:07:10,937:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:07:10,949:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:07:11,093:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:07:11,125:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:07:11,217:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:07:11,227:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:07:11,264:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:07:11,265:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:07:11,266:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:07:11,266:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:07:11,268:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:07:11,620:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:07:11,622:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:37917 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:07:18,279:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@16:07:18,392:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 7359 ms on localhost (1/1)
[INFO ]20161112@16:07:18,407:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:211) finished in 7.436 s
[INFO ]20161112@16:07:18,412:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:07:18,440:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:211, took 7.929561 s
[INFO ]20161112@16:07:18,481:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:214
[INFO ]20161112@16:07:18,485:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:214) with 1 output partitions
[INFO ]20161112@16:07:18,487:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:214)
[INFO ]20161112@16:07:18,487:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:07:18,494:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:07:18,504:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:07:18,521:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@16:07:18,579:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@16:07:18,581:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:37917 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:07:18,583:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:07:18,590:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:07:18,591:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@16:07:18,608:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:07:18,610:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@16:07:18,636:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@16:08:02,692:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:08:03,581:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:08:04,277:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:08:04,279:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:08:04,281:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:08:05,080:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56234.
[INFO ]20161112@16:08:06,099:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:08:06,276:Remoting - Starting remoting
[INFO ]20161112@16:08:06,865:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:53314]
[INFO ]20161112@16:08:06,883:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:53314]
[INFO ]20161112@16:08:06,924:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 53314.
[INFO ]20161112@16:08:07,011:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:08:07,078:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:08:07,138:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-81e9c9ae-079d-4d01-b5cf-8e9d825a8731
[INFO ]20161112@16:08:07,167:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:08:07,373:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161112@16:08:08,398:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161112@16:08:08,486:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161112@16:08:08,494:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4041
[INFO ]20161112@16:08:08,862:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:08:08,970:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45636.
[INFO ]20161112@16:08:08,972:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 45636
[INFO ]20161112@16:08:08,974:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:08:08,979:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:45636 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 45636)
[INFO ]20161112@16:08:08,984:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:08:11,692:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:08:12,579:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:08:12,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:45636 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:08:12,608:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:08:13,690:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:08:13,757:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:211
[INFO ]20161112@16:08:13,820:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:211) with 1 output partitions
[INFO ]20161112@16:08:13,828:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:211)
[INFO ]20161112@16:08:13,832:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:08:13,849:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:08:13,891:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:08:14,100:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:08:14,160:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:08:14,167:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:45636 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:08:14,168:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:08:14,182:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:08:14,193:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:08:14,360:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:08:14,393:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:08:14,460:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:08:14,466:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:08:14,528:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:08:14,529:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:08:14,529:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:08:14,529:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:08:14,530:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:08:14,870:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:08:14,874:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:45636 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:08:22,687:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@16:08:22,826:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 8518 ms on localhost (1/1)
[INFO ]20161112@16:08:22,842:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:08:22,847:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:211) finished in 8.609 s
[INFO ]20161112@16:08:22,876:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:211, took 9.115470 s
[INFO ]20161112@16:08:22,918:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:214
[INFO ]20161112@16:08:22,923:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:214) with 1 output partitions
[INFO ]20161112@16:08:22,925:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:214)
[INFO ]20161112@16:08:22,925:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:08:22,937:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:08:22,940:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:08:22,965:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@16:08:23,016:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@16:08:23,020:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:45636 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:08:23,023:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:08:23,029:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:08:23,030:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@16:08:23,049:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:08:23,051:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@16:08:23,077:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@16:08:38,673:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2058 bytes result sent to driver
[INFO ]20161112@16:08:38,726:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:214) finished in 15.669 s
[INFO ]20161112@16:08:38,728:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:214, took 15.807897 s
[INFO ]20161112@16:08:38,734:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 15680 ms on localhost (1/1)
[INFO ]20161112@16:08:38,735:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:08:38,808:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4041
[INFO ]20161112@16:08:38,849:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161112@16:08:38,874:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161112@16:08:38,881:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161112@16:08:38,884:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161112@16:08:38,898:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161112@16:08:38,926:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161112@16:08:38,935:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161112@16:08:38,956:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161112@16:08:38,973:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-01a84e34-1d0b-4f11-81c8-71d69f69ef3f
[INFO ]20161112@16:12:11,452:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:12:12,296:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:12:12,967:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:12:12,970:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:12:12,971:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:12:13,935:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33684.
[INFO ]20161112@16:12:14,995:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:12:15,198:Remoting - Starting remoting
[INFO ]20161112@16:12:15,765:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:56324]
[INFO ]20161112@16:12:15,775:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:56324]
[INFO ]20161112@16:12:15,814:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 56324.
[INFO ]20161112@16:12:15,919:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:12:15,988:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:12:16,021:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3c6f6162-595d-4d93-a74a-29ad26e95aaa
[INFO ]20161112@16:12:16,049:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:12:16,258:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161112@16:12:17,139:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161112@16:12:17,198:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161112@16:12:17,203:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4041
[INFO ]20161112@16:12:17,509:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:12:17,576:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58795.
[INFO ]20161112@16:12:17,578:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58795
[INFO ]20161112@16:12:17,580:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:12:17,586:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:58795 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 58795)
[INFO ]20161112@16:12:17,590:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:12:20,229:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:12:21,070:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:12:21,079:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58795 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:12:21,102:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:12:22,173:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:12:22,235:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:216
[INFO ]20161112@16:12:22,297:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:216) with 1 output partitions
[INFO ]20161112@16:12:22,302:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:216)
[INFO ]20161112@16:12:22,306:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:12:22,320:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:12:22,378:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:12:22,603:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:12:22,650:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:12:22,657:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58795 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:12:22,659:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:12:22,678:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:12:22,691:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:12:22,877:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:12:22,932:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:12:23,075:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:12:23,084:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:12:23,139:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:12:23,141:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:12:23,142:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:12:23,144:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:12:23,149:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:12:23,490:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:12:23,491:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:58795 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:12:39,688:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2058 bytes result sent to driver
[INFO ]20161112@16:12:39,728:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:214) finished in 321.113 s
[INFO ]20161112@16:12:39,730:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:214, took 321.248467 s
[INFO ]20161112@16:12:39,739:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 321118 ms on localhost (1/1)
[INFO ]20161112@16:12:39,740:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:12:39,822:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161112@16:12:39,878:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161112@16:12:39,907:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161112@16:12:39,913:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161112@16:12:39,920:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161112@16:12:39,938:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161112@16:12:39,962:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161112@16:12:39,969:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161112@16:12:40,004:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161112@16:12:40,006:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161112@16:12:40,018:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-6c826246-8571-4e11-8b18-35fa506201ee
[INFO ]20161112@16:13:10,375:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:13:11,456:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:13:13,251:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:13:13,265:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:13:13,273:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:13:14,622:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36055.
[INFO ]20161112@16:13:15,683:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:13:15,868:Remoting - Starting remoting
[INFO ]20161112@16:13:16,611:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:48291]
[INFO ]20161112@16:13:16,629:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:48291]
[INFO ]20161112@16:13:16,676:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 48291.
[INFO ]20161112@16:13:16,830:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:13:16,947:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:13:17,004:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-4521e4f6-13b5-457e-a77f-77a4edc07c8d
[INFO ]20161112@16:13:17,056:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:13:17,281:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@16:13:18,111:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@16:13:18,119:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@16:13:18,499:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:13:18,598:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54004.
[INFO ]20161112@16:13:18,601:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 54004
[INFO ]20161112@16:13:18,605:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:13:18,610:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:54004 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 54004)
[INFO ]20161112@16:13:18,615:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:13:21,419:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:13:22,329:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:13:22,336:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:54004 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:13:22,367:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:13:23,630:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:13:23,692:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:217
[INFO ]20161112@16:13:23,781:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:217) with 1 output partitions
[INFO ]20161112@16:13:23,787:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:217)
[INFO ]20161112@16:13:23,793:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:13:23,813:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:13:23,863:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:13:24,089:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:13:24,139:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:13:24,146:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:54004 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:13:24,148:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:13:24,156:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:13:24,168:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:13:24,317:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:13:24,346:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:13:24,454:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:13:24,461:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:13:24,519:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:13:24,520:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:13:24,521:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:13:24,522:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:13:24,522:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:13:24,851:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:13:24,854:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:54004 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:13:33,122:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@16:13:33,253:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 70425 ms on localhost (1/1)
[INFO ]20161112@16:13:33,259:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:13:33,272:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:216) finished in 70.515 s
[INFO ]20161112@16:13:33,314:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:216, took 71.074990 s
[INFO ]20161112@16:13:33,368:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:220
[INFO ]20161112@16:13:33,385:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:220) with 1 output partitions
[INFO ]20161112@16:13:33,386:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:220)
[INFO ]20161112@16:13:33,386:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:13:33,396:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:13:33,397:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:13:33,414:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@16:13:33,475:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@16:13:33,483:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:58795 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:13:33,486:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:13:33,487:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:13:33,493:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@16:13:33,532:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:13:33,534:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@16:13:33,566:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@16:15:01,439:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:15:02,215:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:15:02,816:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:15:02,817:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:15:02,819:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:15:03,520:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 52956.
[INFO ]20161112@16:15:04,362:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:15:04,496:Remoting - Starting remoting
[INFO ]20161112@16:15:04,975:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:58667]
[INFO ]20161112@16:15:04,986:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:58667]
[INFO ]20161112@16:15:05,022:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 58667.
[INFO ]20161112@16:15:05,095:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:15:05,155:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:15:05,222:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d128fb3f-b44f-4db8-9279-6f96ac394847
[INFO ]20161112@16:15:05,273:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:15:05,476:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161112@16:15:06,452:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161112@16:15:06,550:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO ]20161112@16:15:06,608:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
[INFO ]20161112@16:15:06,624:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4042
[INFO ]20161112@16:15:06,946:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:15:07,013:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35547.
[INFO ]20161112@16:15:07,014:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 35547
[INFO ]20161112@16:15:07,016:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:15:07,021:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:35547 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 35547)
[INFO ]20161112@16:15:07,027:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:15:09,027:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:15:09,558:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:15:09,566:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:35547 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:15:09,589:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:15:10,528:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:15:10,583:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:217
[INFO ]20161112@16:15:10,634:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:217) with 1 output partitions
[INFO ]20161112@16:15:10,636:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:217)
[INFO ]20161112@16:15:10,639:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:15:10,651:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:15:10,680:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:15:10,860:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:15:10,898:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:15:10,901:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:35547 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:15:10,902:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:15:10,922:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:15:10,927:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:15:11,063:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:15:11,091:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:15:11,148:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:15:11,152:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:15:11,193:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:15:11,195:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:15:11,195:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:15:11,197:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:15:11,197:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:15:11,452:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:15:11,460:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:35547 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:15:11,910:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@16:15:11,982:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 975 ms on localhost (1/1)
[INFO ]20161112@16:15:11,993:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:217) finished in 1.022 s
[INFO ]20161112@16:15:11,987:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:15:12,028:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:217, took 1.443042 s
[INFO ]20161112@16:15:12,062:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:221
[INFO ]20161112@16:15:12,068:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:221) with 1 output partitions
[INFO ]20161112@16:15:12,070:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:221)
[INFO ]20161112@16:15:12,070:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:15:12,074:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:15:12,084:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186), which has no missing parents
[INFO ]20161112@16:15:12,119:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@16:15:12,178:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@16:15:12,187:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:35547 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:15:12,198:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:15:12,199:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:186)
[INFO ]20161112@16:15:12,206:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@16:15:12,243:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:15:12,250:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@16:15:12,295:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@16:15:12,549:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2058 bytes result sent to driver
[INFO ]20161112@16:15:12,577:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:221) finished in 0.334 s
[INFO ]20161112@16:15:12,579:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:221, took 0.513164 s
[INFO ]20161112@16:15:12,583:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 335 ms on localhost (1/1)
[INFO ]20161112@16:15:12,583:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:15:12,653:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4042
[INFO ]20161112@16:15:12,675:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161112@16:15:12,686:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161112@16:15:12,687:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161112@16:15:12,694:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161112@16:15:12,713:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161112@16:15:12,721:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161112@16:15:12,736:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161112@16:15:12,747:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161112@16:15:12,750:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-51d03d1f-14a0-440e-8936-c68b34bf0d8d
[INFO ]20161112@16:18:41,985:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:18:42,960:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:18:43,646:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:18:43,648:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:18:43,650:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:18:44,440:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57320.
[INFO ]20161112@16:18:45,419:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:18:45,595:Remoting - Starting remoting
[INFO ]20161112@16:18:46,202:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:37985]
[INFO ]20161112@16:18:46,231:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:37985]
[INFO ]20161112@16:18:46,285:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 37985.
[INFO ]20161112@16:18:46,395:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:18:46,467:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:18:46,501:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d45fa01b-4748-4525-b4ab-25f9cc200b5b
[INFO ]20161112@16:18:46,559:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:18:46,792:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@16:18:47,614:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@16:18:47,622:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@16:18:47,928:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:18:47,976:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40781.
[INFO ]20161112@16:18:47,978:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 40781
[INFO ]20161112@16:18:47,980:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:18:47,986:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:40781 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 40781)
[INFO ]20161112@16:18:47,990:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:18:50,683:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:18:51,592:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:18:51,602:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40781 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:18:51,628:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:18:52,745:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:18:52,830:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:223
[INFO ]20161112@16:18:52,908:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:223) with 1 output partitions
[INFO ]20161112@16:18:52,912:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:223)
[INFO ]20161112@16:18:52,917:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:18:52,939:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:18:53,012:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192), which has no missing parents
[INFO ]20161112@16:18:53,249:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:18:53,296:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:18:53,301:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40781 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:18:53,302:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:18:53,320:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192)
[INFO ]20161112@16:18:53,332:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:18:53,496:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:18:53,530:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:18:53,659:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:18:53,682:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:18:53,789:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:18:53,789:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:18:53,790:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:18:53,790:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:18:53,790:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:18:54,145:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:18:54,146:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:40781 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:19:22,490:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@16:19:22,617:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 29166 ms on localhost (1/1)
[INFO ]20161112@16:19:22,629:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:223) finished in 29.252 s
[INFO ]20161112@16:19:22,634:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:19:22,678:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:223, took 29.841411 s
[INFO ]20161112@16:19:22,735:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:227
[INFO ]20161112@16:19:22,745:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:227) with 1 output partitions
[INFO ]20161112@16:19:22,746:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:227)
[INFO ]20161112@16:19:22,746:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:19:22,754:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:19:22,762:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192), which has no missing parents
[INFO ]20161112@16:19:22,779:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@16:19:22,842:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@16:19:22,846:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40781 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:19:22,848:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:19:22,848:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192)
[INFO ]20161112@16:19:22,857:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@16:19:22,886:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:19:22,895:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@16:19:22,923:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[WARN ]20161112@16:40:43,218:org.apache.spark.HeartbeatReceiver - Removing executor driver with no recent heartbeats: 1263774 ms exceeds timeout 120000 ms
[ERROR]20161112@16:40:43,220:org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor driver on localhost: Executor heartbeat timed out after 1263774 ms
[WARN ]20161112@16:40:43,231:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 1.0 (TID 1, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 1263774 ms
[ERROR]20161112@16:40:43,243:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
[INFO ]20161112@16:40:43,253:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:40:43,276:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 1
[WARN ]20161112@16:40:43,288:org.apache.spark.SparkContext - Killing executors is only supported in coarse-grained mode
[INFO ]20161112@16:40:43,292:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:227) failed in 1280.384 s
[INFO ]20161112@16:40:43,308:org.apache.spark.scheduler.DAGScheduler - Job 1 failed: count at HW2_Part1.java:227, took 25.109001 s
[INFO ]20161112@16:40:43,347:org.apache.spark.scheduler.DAGScheduler - Executor lost: driver (epoch 0)
[INFO ]20161112@16:40:43,354:org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor driver from BlockManagerMaster.
[INFO ]20161112@16:40:43,372:org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(driver, localhost, 40781)
[INFO ]20161112@16:40:43,376:org.apache.spark.storage.BlockManagerMaster - Removed driver successfully in removeExecutor
[INFO ]20161112@16:40:43,397:org.apache.spark.scheduler.DAGScheduler - Host added was in lost list earlier: localhost
[INFO ]20161112@16:40:44,899:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161112@16:40:44,901:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161112@16:40:44,915:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:40:44,916:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:40781 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 40781)
[INFO ]20161112@16:40:44,917:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:40:44,919:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161112@16:40:44,940:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40781 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:40:44,948:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40781 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:40:44,951:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40781 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:40:44,955:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:40781 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:40:54,902:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161112@16:40:54,902:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161112@16:40:54,903:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:40:54,904:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:40:54,905:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161112@16:40:54,909:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40781 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:40:54,911:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40781 (size: 14.9 KB, free: 1027.1 MB)
[INFO ]20161112@16:40:54,913:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40781 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:40:54,915:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:40781 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:41:31,846:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:41:32,842:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:41:33,786:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:41:33,791:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:41:33,794:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:41:34,861:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44456.
[INFO ]20161112@16:41:35,868:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:41:36,035:Remoting - Starting remoting
[INFO ]20161112@16:41:36,683:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:41536]
[INFO ]20161112@16:41:36,704:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:41536]
[INFO ]20161112@16:41:36,746:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 41536.
[INFO ]20161112@16:41:36,844:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:41:36,942:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:41:36,990:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-43df9953-f7d9-4161-98a7-f66dfb5430b0
[INFO ]20161112@16:41:37,028:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:41:37,242:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@16:41:38,099:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@16:41:38,108:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@16:41:38,532:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:41:38,609:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36852.
[INFO ]20161112@16:41:38,613:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 36852
[INFO ]20161112@16:41:38,619:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:41:38,630:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:36852 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 36852)
[INFO ]20161112@16:41:38,639:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:41:41,367:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:41:42,254:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:41:42,263:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:36852 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:41:42,289:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:41:43,448:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:41:43,515:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:223
[INFO ]20161112@16:41:43,584:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:223) with 1 output partitions
[INFO ]20161112@16:41:43,585:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:223)
[INFO ]20161112@16:41:43,589:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:41:43,607:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:41:43,662:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192), which has no missing parents
[INFO ]20161112@16:41:43,878:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:41:43,933:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:41:43,937:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:36852 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:41:43,938:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:41:43,953:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192)
[INFO ]20161112@16:41:43,966:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:41:44,131:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:41:44,167:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:41:44,280:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:41:44,296:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:41:44,352:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:41:44,352:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:41:44,352:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:41:44,353:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:41:44,355:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:41:44,661:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:41:44,663:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:36852 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:48:21,131:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@16:48:22,113:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@16:48:22,730:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@16:48:22,731:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@16:48:22,733:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@16:48:23,436:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 51027.
[INFO ]20161112@16:48:24,264:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@16:48:24,398:Remoting - Starting remoting
[INFO ]20161112@16:48:24,865:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:52856]
[INFO ]20161112@16:48:24,882:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:52856]
[INFO ]20161112@16:48:24,915:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 52856.
[INFO ]20161112@16:48:24,991:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@16:48:25,048:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@16:48:25,106:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-c76e47f5-0991-4919-9c96-fa598aa70c55
[INFO ]20161112@16:48:25,162:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@16:48:25,348:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161112@16:48:26,297:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161112@16:48:26,357:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161112@16:48:26,366:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4041
[INFO ]20161112@16:48:26,680:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@16:48:26,749:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46951.
[INFO ]20161112@16:48:26,750:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 46951
[INFO ]20161112@16:48:26,752:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@16:48:26,761:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:46951 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 46951)
[INFO ]20161112@16:48:26,767:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@16:48:28,646:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@16:48:29,193:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@16:48:29,199:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:46951 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@16:48:29,221:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@16:48:30,100:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@16:48:30,151:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:223
[INFO ]20161112@16:48:30,186:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:223) with 1 output partitions
[INFO ]20161112@16:48:30,188:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:223)
[INFO ]20161112@16:48:30,189:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:48:30,210:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:48:30,241:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192), which has no missing parents
[INFO ]20161112@16:48:30,415:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@16:48:30,451:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@16:48:30,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:46951 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@16:48:30,456:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:48:30,470:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192)
[INFO ]20161112@16:48:30,477:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@16:48:30,616:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:48:30,633:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@16:48:30,688:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@16:48:30,693:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@16:48:30,745:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@16:48:30,745:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@16:48:30,746:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@16:48:30,748:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@16:48:30,748:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@16:48:31,018:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@16:48:31,020:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:46951 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@16:48:31,603:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@16:48:31,671:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1102 ms on localhost (1/1)
[INFO ]20161112@16:48:31,682:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:223) finished in 1.150 s
[INFO ]20161112@16:48:31,681:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:48:31,717:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:223, took 1.564422 s
[INFO ]20161112@16:48:31,747:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:227
[INFO ]20161112@16:48:31,755:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:227) with 1 output partitions
[INFO ]20161112@16:48:31,758:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:227)
[INFO ]20161112@16:48:31,760:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@16:48:31,769:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@16:48:31,779:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192), which has no missing parents
[INFO ]20161112@16:48:31,794:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@16:48:31,828:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@16:48:31,841:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:46951 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:48:31,844:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:48:31,848:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192)
[INFO ]20161112@16:48:31,848:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@16:48:31,864:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@16:48:31,865:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@16:48:31,891:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@16:48:32,062:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2058 bytes result sent to driver
[INFO ]20161112@16:48:32,087:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:227) finished in 0.216 s
[INFO ]20161112@16:48:32,089:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:227, took 0.337875 s
[INFO ]20161112@16:48:32,091:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 223 ms on localhost (1/1)
[INFO ]20161112@16:48:32,093:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:48:32,417:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161112@16:48:32,597:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:262
[INFO ]20161112@16:48:32,608:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:249)
[INFO ]20161112@16:48:32,610:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part1.java:262) with 1 output partitions
[INFO ]20161112@16:48:32,610:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (saveAsTextFile at HW2_Part1.java:262)
[INFO ]20161112@16:48:32,611:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161112@16:48:32,612:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161112@16:48:32,621:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:249), which has no missing parents
[INFO ]20161112@16:48:32,668:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.8 KB, free 354.5 KB)
[INFO ]20161112@16:48:32,695:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161112@16:48:32,719:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.2 KB, free 357.8 KB)
[INFO ]20161112@16:48:32,725:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:46951 (size: 3.2 KB, free: 1027.1 MB)
[INFO ]20161112@16:48:32,734:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:48:32,736:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:249)
[INFO ]20161112@16:48:32,747:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161112@16:48:32,767:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161112@16:48:32,769:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161112@16:48:32,775:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:46951 in memory (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@16:48:32,840:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@16:48:33,150:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2237 bytes result sent to driver
[INFO ]20161112@16:48:33,182:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part1.java:249) finished in 0.431 s
[INFO ]20161112@16:48:33,184:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161112@16:48:33,185:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161112@16:48:33,186:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 427 ms on localhost (1/1)
[INFO ]20161112@16:48:33,187:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161112@16:48:33,188:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:48:33,189:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161112@16:48:33,194:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:262), which has no missing parents
[INFO ]20161112@16:48:33,322:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 71.4 KB, free 422.4 KB)
[INFO ]20161112@16:48:33,354:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KB, free 447.3 KB)
[INFO ]20161112@16:48:33,358:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:46951 (size: 25.0 KB, free: 1027.1 MB)
[INFO ]20161112@16:48:33,368:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@16:48:33,370:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:262)
[INFO ]20161112@16:48:33,380:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161112@16:48:33,396:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161112@16:48:33,397:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161112@16:48:33,555:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161112@16:48:33,562:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161112@16:48:33,879:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161112@16:48:33,991:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611121648_0003_m_000000_3' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2part1_1478998107316/_temporary/0/task_201611121648_0003_m_000000
[INFO ]20161112@16:48:33,998:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611121648_0003_m_000000_3: Committed
[INFO ]20161112@16:48:34,022:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2038 bytes result sent to driver
[INFO ]20161112@16:48:34,043:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (saveAsTextFile at HW2_Part1.java:262) finished in 0.639 s
[INFO ]20161112@16:48:34,046:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part1.java:262, took 1.448104 s
[INFO ]20161112@16:48:34,065:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 645 ms on localhost (1/1)
[INFO ]20161112@16:48:34,065:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161112@16:48:34,073:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:46951 in memory (size: 3.2 KB, free: 1027.1 MB)
[INFO ]20161112@16:48:34,173:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4041
[INFO ]20161112@16:48:34,208:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161112@16:48:34,225:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161112@16:48:34,232:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161112@16:48:34,238:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161112@16:48:34,242:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161112@16:48:34,258:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161112@16:48:34,268:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161112@16:48:34,271:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161112@16:48:34,282:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-43eb689f-a989-4fc4-b25c-aba368a68369
[INFO ]20161112@16:48:34,284:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161112@17:34:48,918:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161112@17:34:49,879:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161112@17:34:50,564:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161112@17:34:50,566:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161112@17:34:50,567:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161112@17:34:51,283:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56285.
[INFO ]20161112@17:34:52,107:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161112@17:34:52,272:Remoting - Starting remoting
[INFO ]20161112@17:34:52,821:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:39066]
[INFO ]20161112@17:34:52,831:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:39066]
[INFO ]20161112@17:34:52,865:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 39066.
[INFO ]20161112@17:34:52,931:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161112@17:34:52,988:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161112@17:34:53,033:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-453ad554-7cc1-409b-97e0-83e190363fa0
[INFO ]20161112@17:34:53,095:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161112@17:34:53,282:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161112@17:34:54,105:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161112@17:34:54,112:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161112@17:34:54,429:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161112@17:34:54,491:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60284.
[INFO ]20161112@17:34:54,492:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 60284
[INFO ]20161112@17:34:54,495:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161112@17:34:54,502:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:60284 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 60284)
[INFO ]20161112@17:34:54,507:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161112@17:34:56,380:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161112@17:34:56,981:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161112@17:34:56,993:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:60284 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161112@17:34:57,009:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161112@17:34:57,876:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161112@17:34:57,926:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:223
[INFO ]20161112@17:34:57,988:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:223) with 1 output partitions
[INFO ]20161112@17:34:57,991:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:223)
[INFO ]20161112@17:34:57,993:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@17:34:58,003:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@17:34:58,046:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192), which has no missing parents
[INFO ]20161112@17:34:58,225:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.7 KB)
[INFO ]20161112@17:34:58,261:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161112@17:34:58,270:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:60284 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161112@17:34:58,271:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@17:34:58,285:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192)
[INFO ]20161112@17:34:58,289:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161112@17:34:58,423:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@17:34:58,447:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161112@17:34:58,519:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161112@17:34:58,523:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161112@17:34:58,565:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161112@17:34:58,566:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161112@17:34:58,566:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161112@17:34:58,567:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161112@17:34:58,567:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161112@17:34:58,826:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 342.0 KB)
[INFO ]20161112@17:34:58,827:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:60284 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161112@17:34:59,055:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2638 bytes result sent to driver
[INFO ]20161112@17:34:59,119:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:223) finished in 0.792 s
[INFO ]20161112@17:34:59,121:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 746 ms on localhost (1/1)
[INFO ]20161112@17:34:59,129:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161112@17:34:59,148:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:223, took 1.214279 s
[INFO ]20161112@17:34:59,199:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:227
[INFO ]20161112@17:34:59,200:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:227) with 1 output partitions
[INFO ]20161112@17:34:59,201:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:227)
[INFO ]20161112@17:34:59,201:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161112@17:34:59,205:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161112@17:34:59,222:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192), which has no missing parents
[INFO ]20161112@17:34:59,243:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161112@17:34:59,284:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161112@17:34:59,291:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:60284 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@17:34:59,292:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@17:34:59,294:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:192)
[INFO ]20161112@17:34:59,296:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161112@17:34:59,312:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161112@17:34:59,313:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161112@17:34:59,338:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@17:34:59,413:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2058 bytes result sent to driver
[INFO ]20161112@17:34:59,448:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:227) finished in 0.134 s
[INFO ]20161112@17:34:59,449:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:227, took 0.249976 s
[INFO ]20161112@17:34:59,455:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 135 ms on localhost (1/1)
[INFO ]20161112@17:34:59,455:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161112@17:34:59,706:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161112@17:34:59,886:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:262
[INFO ]20161112@17:34:59,895:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:249)
[INFO ]20161112@17:34:59,902:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part1.java:262) with 1 output partitions
[INFO ]20161112@17:34:59,902:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (saveAsTextFile at HW2_Part1.java:262)
[INFO ]20161112@17:34:59,902:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161112@17:34:59,903:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161112@17:34:59,911:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:249), which has no missing parents
[INFO ]20161112@17:34:59,956:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.8 KB, free 354.5 KB)
[INFO ]20161112@17:34:59,982:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.2 KB, free 357.8 KB)
[INFO ]20161112@17:34:59,985:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:60284 (size: 3.2 KB, free: 1027.1 MB)
[INFO ]20161112@17:34:59,986:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@17:34:59,993:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:249)
[INFO ]20161112@17:34:59,998:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161112@17:35:00,008:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161112@17:35:00,014:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161112@17:35:00,103:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161112@17:35:00,204:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161112@17:35:00,267:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:60284 in memory (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161112@17:35:00,354:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2237 bytes result sent to driver
[INFO ]20161112@17:35:00,393:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part1.java:249) finished in 0.383 s
[INFO ]20161112@17:35:00,396:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161112@17:35:00,398:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161112@17:35:00,400:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161112@17:35:00,399:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 386 ms on localhost (1/1)
[INFO ]20161112@17:35:00,402:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161112@17:35:00,405:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161112@17:35:00,410:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:262), which has no missing parents
[INFO ]20161112@17:35:00,475:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 71.4 KB, free 422.4 KB)
[INFO ]20161112@17:35:00,503:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KB, free 447.3 KB)
[INFO ]20161112@17:35:00,505:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:60284 (size: 25.0 KB, free: 1027.1 MB)
[INFO ]20161112@17:35:00,509:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161112@17:35:00,511:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:262)
[INFO ]20161112@17:35:00,512:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161112@17:35:00,524:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161112@17:35:00,525:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161112@17:35:00,685:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161112@17:35:00,694:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
[INFO ]20161112@17:35:01,052:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161112@17:35:01,168:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611121734_0003_m_000000_3' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2part1_1479000895052/_temporary/0/task_201611121734_0003_m_000000
[INFO ]20161112@17:35:01,170:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611121734_0003_m_000000_3: Committed
[INFO ]20161112@17:35:01,193:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2038 bytes result sent to driver
[INFO ]20161112@17:35:01,217:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (saveAsTextFile at HW2_Part1.java:262) finished in 0.687 s
[INFO ]20161112@17:35:01,219:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part1.java:262, took 1.331652 s
[INFO ]20161112@17:35:01,224:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 688 ms on localhost (1/1)
[INFO ]20161112@17:35:01,225:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161112@17:35:01,335:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161112@17:35:01,365:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161112@17:35:01,383:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161112@17:35:01,385:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161112@17:35:01,393:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161112@17:35:01,406:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161112@17:35:01,421:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161112@17:35:01,428:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161112@17:35:01,435:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161112@17:35:01,459:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161112@17:35:01,475:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-4f56b879-ff35-4101-8a39-d4eef00fc2c7
[INFO ]20161116@19:48:17,860:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@19:48:19,655:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@19:48:20,566:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@19:48:20,568:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@19:48:20,571:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@19:48:21,278:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35366.
[INFO ]20161116@19:48:22,100:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@19:48:22,262:Remoting - Starting remoting
[INFO ]20161116@19:48:22,768:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:58894]
[INFO ]20161116@19:48:22,781:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:58894]
[INFO ]20161116@19:48:22,817:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 58894.
[INFO ]20161116@19:48:22,883:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@19:48:22,937:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@19:48:23,011:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3048506e-3b64-4596-b5f1-2dcbb7b849fe
[INFO ]20161116@19:48:23,083:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@19:48:23,371:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@19:48:24,194:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@19:48:24,202:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@19:48:24,503:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@19:48:24,564:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38498.
[INFO ]20161116@19:48:24,565:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 38498
[INFO ]20161116@19:48:24,570:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@19:48:24,576:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:38498 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 38498)
[INFO ]20161116@19:48:24,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@19:48:26,593:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@19:48:27,159:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@19:48:27,171:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:38498 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@19:48:27,191:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161116@19:48:28,010:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@19:48:28,059:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:219
[INFO ]20161116@19:48:28,108:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:219) with 1 output partitions
[INFO ]20161116@19:48:28,109:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:219)
[INFO ]20161116@19:48:28,111:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@19:48:28,118:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@19:48:28,164:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:188), which has no missing parents
[INFO ]20161116@19:48:28,342:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.6 KB)
[INFO ]20161116@19:48:28,381:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161116@19:48:28,384:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:38498 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161116@19:48:28,386:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@19:48:28,398:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:188)
[INFO ]20161116@19:48:28,406:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@19:48:28,520:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@19:48:28,547:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@19:48:28,617:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161116@19:48:28,623:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@19:48:28,662:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@19:48:28,663:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@19:48:28,663:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@19:48:28,663:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@19:48:28,666:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@19:48:28,937:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 341.9 KB)
[INFO ]20161116@19:48:28,938:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:38498 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161116@19:48:29,169:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2623 bytes result sent to driver
[INFO ]20161116@19:48:29,247:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 767 ms on localhost (1/1)
[INFO ]20161116@19:48:29,256:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:219) finished in 0.812 s
[INFO ]20161116@19:48:29,262:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@19:48:29,289:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:219, took 1.217087 s
[INFO ]20161116@19:48:29,321:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:223
[INFO ]20161116@19:48:29,332:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:223) with 1 output partitions
[INFO ]20161116@19:48:29,334:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:223)
[INFO ]20161116@19:48:29,334:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@19:48:29,343:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@19:48:29,348:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:188), which has no missing parents
[INFO ]20161116@19:48:29,358:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 346.2 KB)
[INFO ]20161116@19:48:29,415:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 348.7 KB)
[INFO ]20161116@19:48:29,433:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:38498 (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161116@19:48:29,435:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@19:48:29,435:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at HW2_Part1.java:188)
[INFO ]20161116@19:48:29,449:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@19:48:29,483:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@19:48:29,484:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@19:48:29,521:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161116@19:48:29,605:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2043 bytes result sent to driver
[INFO ]20161116@19:48:29,635:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:223) finished in 0.150 s
[INFO ]20161116@19:48:29,636:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:223, took 0.312294 s
[INFO ]20161116@19:48:29,641:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 149 ms on localhost (1/1)
[INFO ]20161116@19:48:29,641:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@19:48:29,947:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@19:48:30,174:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:258
[INFO ]20161116@19:48:30,190:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:245)
[INFO ]20161116@19:48:30,195:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part1.java:258) with 1 output partitions
[INFO ]20161116@19:48:30,196:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (saveAsTextFile at HW2_Part1.java:258)
[INFO ]20161116@19:48:30,197:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161116@19:48:30,198:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161116@19:48:30,210:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:245), which has no missing parents
[INFO ]20161116@19:48:30,288:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.8 KB, free 354.5 KB)
[INFO ]20161116@19:48:30,328:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.2 KB, free 357.7 KB)
[INFO ]20161116@19:48:30,334:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:38498 (size: 3.2 KB, free: 1027.1 MB)
[INFO ]20161116@19:48:30,341:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@19:48:30,361:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:245)
[INFO ]20161116@19:48:30,363:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@19:48:30,378:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@19:48:30,379:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@19:48:30,493:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161116@19:48:30,739:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@19:48:30,827:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:38498 in memory (size: 2.5 KB, free: 1027.1 MB)
[INFO ]20161116@19:48:30,941:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2222 bytes result sent to driver
[INFO ]20161116@19:48:30,988:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part1.java:245) finished in 0.611 s
[INFO ]20161116@19:48:30,989:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@19:48:30,990:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@19:48:30,991:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161116@19:48:30,991:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@19:48:30,992:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 619 ms on localhost (1/1)
[INFO ]20161116@19:48:30,999:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@19:48:31,007:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:258), which has no missing parents
[INFO ]20161116@19:48:31,156:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 71.4 KB, free 422.3 KB)
[INFO ]20161116@19:48:31,179:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KB, free 447.2 KB)
[INFO ]20161116@19:48:31,185:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:38498 (size: 25.0 KB, free: 1027.1 MB)
[INFO ]20161116@19:48:31,194:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@19:48:31,194:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:258)
[INFO ]20161116@19:48:31,196:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@19:48:31,199:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161116@19:48:31,201:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@19:48:31,381:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@19:48:31,387:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 18 ms
[INFO ]20161116@19:48:31,963:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@19:48:32,109:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611161948_0003_m_000000_3' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2part1_1479354505096/_temporary/0/task_201611161948_0003_m_000000
[INFO ]20161116@19:48:32,115:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611161948_0003_m_000000_3: Committed
[INFO ]20161116@19:48:32,144:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2038 bytes result sent to driver
[INFO ]20161116@19:48:32,180:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (saveAsTextFile at HW2_Part1.java:258) finished in 0.975 s
[INFO ]20161116@19:48:32,189:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part1.java:258, took 2.011405 s
[INFO ]20161116@19:48:32,196:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 976 ms on localhost (1/1)
[INFO ]20161116@19:48:32,197:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@19:48:32,314:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@19:48:32,371:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@19:48:32,393:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@19:48:32,397:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@19:48:32,405:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@19:48:32,412:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@19:48:32,432:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@19:48:32,444:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@19:48:32,460:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@19:48:32,466:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-9de24857-695c-4ac1-8afe-49df4267ac94
[INFO ]20161116@19:48:32,468:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@19:50:30,062:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@19:50:31,325:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@19:50:32,001:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@19:50:32,003:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@19:50:32,004:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@19:50:32,766:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53035.
[INFO ]20161116@19:50:33,629:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@19:50:33,777:Remoting - Starting remoting
[INFO ]20161116@19:50:34,246:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:46388]
[INFO ]20161116@19:50:34,255:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:46388]
[INFO ]20161116@19:50:34,292:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 46388.
[INFO ]20161116@19:50:34,361:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@19:50:34,416:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@19:50:34,494:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-93c2e4fb-01de-470e-88a1-7cc18c3520ec
[INFO ]20161116@19:50:34,541:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@19:50:34,746:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@19:50:35,709:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@19:50:35,715:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@19:50:36,067:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@19:50:36,138:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52481.
[INFO ]20161116@19:50:36,145:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 52481
[INFO ]20161116@19:50:36,150:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@19:50:36,162:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:52481 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 52481)
[INFO ]20161116@19:50:36,175:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@19:50:38,549:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@19:50:39,155:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@19:50:39,163:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:52481 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@19:50:39,191:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:115
[INFO ]20161116@19:50:40,126:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@19:50:40,189:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:219
[INFO ]20161116@19:50:40,223:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part1.java:219) with 1 output partitions
[INFO ]20161116@19:50:40,225:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (count at HW2_Part1.java:219)
[INFO ]20161116@19:50:40,227:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@19:50:40,234:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@19:50:40,272:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:188), which has no missing parents
[INFO ]20161116@19:50:40,474:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 157.6 KB)
[INFO ]20161116@19:50:40,510:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 160.1 KB)
[INFO ]20161116@19:50:40,513:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:52481 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161116@19:50:40,517:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@19:50:40,528:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at filter at HW2_Part1.java:188)
[INFO ]20161116@19:50:40,530:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@19:50:40,675:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@19:50:40,711:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@19:50:40,774:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161116@19:50:40,786:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@19:50:40,814:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@19:50:40,815:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@19:50:40,815:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@19:50:40,815:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@19:50:40,816:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@19:50:41,128:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 341.9 KB)
[INFO ]20161116@19:50:41,134:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:52481 (size: 181.8 KB, free: 1027.1 MB)
[INFO ]20161116@19:50:41,385:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2623 bytes result sent to driver
[INFO ]20161116@19:50:41,465:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 838 ms on localhost (1/1)
[INFO ]20161116@19:50:41,474:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (count at HW2_Part1.java:219) finished in 0.896 s
[INFO ]20161116@19:50:41,480:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@19:50:41,511:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part1.java:219, took 1.319192 s
[INFO ]20161116@19:50:41,877:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@19:50:42,082:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:256
[INFO ]20161116@19:50:42,101:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:243)
[INFO ]20161116@19:50:42,105:org.apache.spark.scheduler.DAGScheduler - Got job 1 (saveAsTextFile at HW2_Part1.java:256) with 1 output partitions
[INFO ]20161116@19:50:42,105:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (saveAsTextFile at HW2_Part1.java:256)
[INFO ]20161116@19:50:42,106:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
[INFO ]20161116@19:50:42,109:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
[INFO ]20161116@19:50:42,118:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:243), which has no missing parents
[INFO ]20161116@19:50:42,167:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 5.8 KB, free 347.7 KB)
[INFO ]20161116@19:50:42,192:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.2 KB, free 350.9 KB)
[INFO ]20161116@19:50:42,196:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:52481 (size: 3.2 KB, free: 1027.1 MB)
[INFO ]20161116@19:50:42,197:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@19:50:42,203:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:243)
[INFO ]20161116@19:50:42,203:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@19:50:42,215:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@19:50:42,217:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@19:50:42,272:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161116@19:50:42,489:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2222 bytes result sent to driver
[INFO ]20161116@19:50:42,524:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (sortBy at HW2_Part1.java:243) finished in 0.313 s
[INFO ]20161116@19:50:42,526:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@19:50:42,529:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@19:50:42,532:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@19:50:42,529:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 314 ms on localhost (1/1)
[INFO ]20161116@19:50:42,534:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@19:50:42,535:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@19:50:42,551:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:256), which has no missing parents
[INFO ]20161116@19:50:42,708:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 71.4 KB, free 422.3 KB)
[INFO ]20161116@19:50:42,753:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.0 KB, free 447.2 KB)
[INFO ]20161116@19:50:42,756:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:52481 (size: 25.0 KB, free: 1027.1 MB)
[INFO ]20161116@19:50:42,761:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@19:50:42,772:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:256)
[INFO ]20161116@19:50:42,773:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@19:50:42,784:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161116@19:50:42,785:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@19:50:43,009:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@19:50:43,015:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 15 ms
[INFO ]20161116@19:50:43,510:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@19:50:43,696:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611161950_0002_m_000000_2' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2part1_1479354636833/_temporary/0/task_201611161950_0002_m_000000
[INFO ]20161116@19:50:43,705:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611161950_0002_m_000000_2: Committed
[INFO ]20161116@19:50:43,734:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2038 bytes result sent to driver
[INFO ]20161116@19:50:43,775:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (saveAsTextFile at HW2_Part1.java:256) finished in 0.976 s
[INFO ]20161116@19:50:43,777:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: saveAsTextFile at HW2_Part1.java:256, took 1.692482 s
[INFO ]20161116@19:50:43,781:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 988 ms on localhost (1/1)
[INFO ]20161116@19:50:43,781:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@19:50:43,902:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@19:50:43,948:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@19:50:43,989:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@19:50:43,993:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@19:50:44,010:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@19:50:44,036:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@19:50:44,041:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@19:50:44,050:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@19:50:44,079:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@19:50:44,080:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@19:50:44,093:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-c5acbb38-0dc1-47f6-9eb6-2894987dc91e
[INFO ]20161116@20:31:17,255:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@20:31:17,983:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@20:31:18,544:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@20:31:18,545:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@20:31:18,548:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@20:31:19,272:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43545.
[INFO ]20161116@20:31:20,180:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@20:31:20,311:Remoting - Starting remoting
[INFO ]20161116@20:31:21,326:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:60409]
[INFO ]20161116@20:31:21,349:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:60409]
[INFO ]20161116@20:31:21,408:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 60409.
[INFO ]20161116@20:31:21,538:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@20:31:21,594:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@20:31:21,668:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-104c0272-a6b8-4216-aa2e-014bf931e34f
[INFO ]20161116@20:31:21,721:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@20:31:21,983:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@20:31:22,730:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@20:31:22,740:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@20:31:23,029:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@20:31:23,099:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60664.
[INFO ]20161116@20:31:23,099:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 60664
[INFO ]20161116@20:31:23,102:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@20:31:23,110:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:60664 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 60664)
[INFO ]20161116@20:31:23,117:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@20:31:25,152:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@20:31:25,905:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@20:31:25,915:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:60664 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@20:31:25,932:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@20:31:26,157:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@20:31:26,226:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@20:31:26,231:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:60664 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@20:31:26,235:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@20:31:26,910:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@20:31:27,089:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:78
[INFO ]20161116@20:31:27,129:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:78) with 1 output partitions
[INFO ]20161116@20:31:27,132:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:78)
[INFO ]20161116@20:31:27,135:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@20:31:27,138:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@20:31:27,191:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@20:31:27,260:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@20:31:27,298:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@20:31:27,303:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:60664 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@20:31:27,307:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:31:27,319:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@20:31:27,322:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@20:31:27,467:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@20:31:27,494:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@20:31:27,585:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@20:31:27,629:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@20:31:27,630:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@20:31:27,630:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@20:31:27,631:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@20:31:27,632:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@20:31:27,810:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@20:31:27,891:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 471 ms on localhost (1/1)
[INFO ]20161116@20:31:27,897:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:78) finished in 0.519 s
[INFO ]20161116@20:31:27,900:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@20:31:27,927:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:78, took 0.837505 s
[INFO ]20161116@20:31:28,032:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@20:31:28,119:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:60664 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@20:31:28,167:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@20:31:28,209:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:79
[INFO ]20161116@20:31:28,214:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:79) with 1 output partitions
[INFO ]20161116@20:31:28,215:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:79)
[INFO ]20161116@20:31:28,215:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@20:31:28,215:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@20:31:28,217:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64), which has no missing parents
[INFO ]20161116@20:31:28,232:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@20:31:28,255:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2000.0 B, free 312.1 KB)
[INFO ]20161116@20:31:28,260:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:60664 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@20:31:28,267:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:31:28,268:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64)
[INFO ]20161116@20:31:28,273:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@20:31:28,276:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@20:31:28,276:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@20:31:28,286:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@20:31:28,323:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3290 bytes result sent to driver
[INFO ]20161116@20:31:28,375:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:79) finished in 0.093 s
[INFO ]20161116@20:31:28,377:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:79, took 0.166987 s
[INFO ]20161116@20:31:28,381:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 96 ms on localhost (1/1)
[INFO ]20161116@20:31:28,381:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@20:31:28,392:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@20:31:28,483:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@20:31:28,539:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@20:31:28,567:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@20:31:28,571:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@20:31:28,579:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@20:31:28,599:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@20:31:28,615:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@20:31:28,616:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@20:31:28,622:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-33a586a4-4af9-4987-af83-57e74182e981
[INFO ]20161116@20:33:31,700:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@20:33:32,515:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@20:33:33,174:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@20:33:33,176:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@20:33:33,179:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@20:33:33,959:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38711.
[INFO ]20161116@20:33:34,861:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@20:33:34,991:Remoting - Starting remoting
[INFO ]20161116@20:33:35,697:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:57920]
[INFO ]20161116@20:33:35,715:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:57920]
[INFO ]20161116@20:33:35,767:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 57920.
[INFO ]20161116@20:33:35,906:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@20:33:36,021:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@20:33:36,142:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-0cc50100-00c0-4463-a11e-33d9c4b74b2e
[INFO ]20161116@20:33:36,252:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@20:33:36,548:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@20:33:37,437:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@20:33:37,443:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@20:33:37,736:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@20:33:37,796:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35845.
[INFO ]20161116@20:33:37,797:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 35845
[INFO ]20161116@20:33:37,800:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@20:33:37,807:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:35845 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 35845)
[INFO ]20161116@20:33:37,814:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@20:33:39,641:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@20:33:40,179:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@20:33:40,188:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:35845 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@20:33:40,208:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@20:33:40,371:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@20:33:40,429:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@20:33:40,433:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:35845 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@20:33:40,435:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@20:33:41,139:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@20:33:41,274:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@20:33:41,532:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:80
[INFO ]20161116@20:33:41,573:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:80) with 1 output partitions
[INFO ]20161116@20:33:41,574:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:80)
[INFO ]20161116@20:33:41,577:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@20:33:41,587:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@20:33:41,627:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@20:33:41,711:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@20:33:41,761:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@20:33:41,767:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:35845 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@20:33:41,774:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:33:41,780:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@20:33:41,786:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@20:33:41,920:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@20:33:41,944:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@20:33:42,020:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@20:33:42,081:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@20:33:42,083:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@20:33:42,084:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@20:33:42,085:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@20:33:42,085:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@20:33:42,255:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@20:33:42,321:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 452 ms on localhost (1/1)
[INFO ]20161116@20:33:42,330:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:80) finished in 0.498 s
[INFO ]20161116@20:33:42,333:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@20:33:42,359:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:80, took 0.825206 s
[INFO ]20161116@20:33:42,423:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:81
[INFO ]20161116@20:33:42,430:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:81) with 1 output partitions
[INFO ]20161116@20:33:42,430:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:81)
[INFO ]20161116@20:33:42,430:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@20:33:42,431:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@20:33:42,431:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64), which has no missing parents
[INFO ]20161116@20:33:42,444:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@20:33:42,479:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@20:33:42,490:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:35845 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@20:33:42,491:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:33:42,492:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64)
[INFO ]20161116@20:33:42,498:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@20:33:42,506:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@20:33:42,507:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@20:33:42,519:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@20:33:42,566:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3290 bytes result sent to driver
[INFO ]20161116@20:33:42,588:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:81) finished in 0.079 s
[INFO ]20161116@20:33:42,590:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:81, took 0.162254 s
[INFO ]20161116@20:33:42,593:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 83 ms on localhost (1/1)
[INFO ]20161116@20:33:42,598:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@20:33:42,705:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:82
[INFO ]20161116@20:33:42,712:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:50)
[INFO ]20161116@20:33:42,715:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:64)
[INFO ]20161116@20:33:42,716:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:82) with 1 output partitions
[INFO ]20161116@20:33:42,720:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (take at HW2_Part2.java:82)
[INFO ]20161116@20:33:42,721:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2, ShuffleMapStage 3)
[INFO ]20161116@20:33:42,722:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2, ShuffleMapStage 3)
[INFO ]20161116@20:33:42,729:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@20:33:42,742:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@20:33:42,763:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.6 KB)
[INFO ]20161116@20:33:42,765:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:35845 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@20:33:42,768:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:33:42,774:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@20:33:42,774:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@20:33:42,781:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@20:33:42,785:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64), which has no missing parents
[INFO ]20161116@20:33:42,786:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@20:33:42,793:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@20:33:42,806:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@20:33:42,832:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@20:33:42,842:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:35845 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@20:33:42,849:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:33:42,850:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64)
[INFO ]20161116@20:33:42,850:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@20:33:43,093:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161116@20:33:43,096:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@20:33:43,098:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@20:33:43,121:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@20:33:43,169:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at HW2_Part2.java:50) finished in 0.385 s
[INFO ]20161116@20:33:43,171:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@20:33:43,172:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 3)
[INFO ]20161116@20:33:43,173:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 4)
[INFO ]20161116@20:33:43,174:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@20:33:43,178:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 390 ms on localhost (1/1)
[INFO ]20161116@20:33:43,186:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@20:33:43,325:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@20:33:43,385:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:35845 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@20:33:43,406:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:35845 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@20:33:43,447:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@20:33:43,483:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:64) finished in 0.630 s
[INFO ]20161116@20:33:43,484:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@20:33:43,484:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@20:33:43,484:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 4)
[INFO ]20161116@20:33:43,484:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@20:33:43,488:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[8] at join at HW2_Part2.java:78), which has no missing parents
[INFO ]20161116@20:33:43,492:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 385 ms on localhost (1/1)
[INFO ]20161116@20:33:43,493:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@20:33:43,500:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 321.7 KB)
[INFO ]20161116@20:33:43,518:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1964.0 B, free 323.6 KB)
[INFO ]20161116@20:33:43,523:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:35845 (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@20:33:43,525:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:33:43,528:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[8] at join at HW2_Part2.java:78)
[INFO ]20161116@20:33:43,529:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@20:33:43,535:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@20:33:43,535:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@20:33:43,602:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@20:33:43,606:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
[INFO ]20161116@20:33:43,630:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@20:33:43,631:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@20:33:44,434:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1124 bytes result sent to driver
[INFO ]20161116@20:33:44,446:org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (take at HW2_Part2.java:82) finished in 0.901 s
[INFO ]20161116@20:33:44,451:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:82, took 1.743447 s
[INFO ]20161116@20:33:44,455:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 912 ms on localhost (1/1)
[INFO ]20161116@20:33:44,456:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@20:33:44,476:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@20:33:44,478:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@20:33:44,479:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@20:33:44,482:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:35845 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@20:33:44,485:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@20:33:44,488:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:35845 in memory (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@20:33:44,558:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@20:33:44,586:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@20:33:44,636:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@20:33:44,644:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@20:33:44,651:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@20:33:44,657:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@20:33:44,679:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@20:33:44,685:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@20:33:44,697:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@20:33:44,699:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@20:33:44,703:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-679c94eb-f57a-48a1-b9c2-53961f54c922
[INFO ]20161116@20:37:03,784:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@20:37:04,627:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@20:37:05,323:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@20:37:05,325:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@20:37:05,327:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@20:37:06,001:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 52631.
[INFO ]20161116@20:37:06,917:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@20:37:07,089:Remoting - Starting remoting
[INFO ]20161116@20:37:07,627:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:37603]
[INFO ]20161116@20:37:07,643:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:37603]
[INFO ]20161116@20:37:07,674:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 37603.
[INFO ]20161116@20:37:07,802:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@20:37:07,893:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@20:37:07,919:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3c58dec0-bbb7-445c-8063-15afa1b42189
[INFO ]20161116@20:37:07,944:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@20:37:08,153:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@20:37:08,979:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@20:37:08,986:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@20:37:09,365:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@20:37:09,446:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40677.
[INFO ]20161116@20:37:09,448:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 40677
[INFO ]20161116@20:37:09,451:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@20:37:09,460:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:40677 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 40677)
[INFO ]20161116@20:37:09,468:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@20:37:11,981:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@20:37:12,750:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@20:37:12,760:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@20:37:12,777:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@20:37:13,042:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@20:37:13,160:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@20:37:13,167:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@20:37:13,169:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@20:37:14,071:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@20:37:14,217:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@20:37:14,508:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:80
[INFO ]20161116@20:37:14,565:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:80) with 1 output partitions
[INFO ]20161116@20:37:14,566:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:80)
[INFO ]20161116@20:37:14,567:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@20:37:14,573:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@20:37:14,617:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@20:37:14,701:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@20:37:14,755:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@20:37:14,760:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@20:37:14,767:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@20:37:14,780:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@20:37:14,796:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@20:37:15,197:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@20:37:15,242:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@20:37:15,329:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@20:37:15,376:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@20:37:15,377:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@20:37:15,379:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@20:37:15,379:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@20:37:15,379:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[WARN ]20161116@21:55:06,012:org.apache.spark.HeartbeatReceiver - Removing executor driver with no recent heartbeats: 4623091 ms exceeds timeout 120000 ms
[ERROR]20161116@21:55:06,040:org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor driver on localhost: Executor heartbeat timed out after 4623091 ms
[WARN ]20161116@21:55:06,273:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 4623091 ms
[ERROR]20161116@21:55:06,320:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
[INFO ]20161116@21:55:06,361:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[WARN ]20161116@21:55:06,467:org.apache.spark.SparkContext - Killing executors is only supported in coarse-grained mode
[INFO ]20161116@21:55:06,489:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
[INFO ]20161116@21:55:06,521:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:80) failed in 4671.596 s
[INFO ]20161116@21:55:06,609:org.apache.spark.scheduler.DAGScheduler - Job 0 failed: take at HW2_Part2.java:80, took 55.239656 s
[INFO ]20161116@21:55:06,697:org.apache.spark.scheduler.DAGScheduler - Executor lost: driver (epoch 0)
[INFO ]20161116@21:55:06,707:org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor driver from BlockManagerMaster.
[INFO ]20161116@21:55:06,721:org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(driver, localhost, 40677)
[INFO ]20161116@21:55:06,725:org.apache.spark.storage.BlockManagerMaster - Removed driver successfully in removeExecutor
[INFO ]20161116@21:55:06,748:org.apache.spark.scheduler.DAGScheduler - Host added was in lost list earlier: localhost
[INFO ]20161116@21:55:09,786:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:55:09,791:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:55:09,794:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:55:09,796:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:40677 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 40677)
[INFO ]20161116@21:55:09,799:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:55:09,802:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:55:09,812:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:55:09,817:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:09,820:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:19,780:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:55:19,781:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:55:19,781:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:55:19,786:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:55:19,787:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:55:19,788:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:55:19,793:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:19,797:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:29,470:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:55:29,473:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:55:29,473:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:55:29,478:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:55:29,483:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:55:29,500:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:55:29,504:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:29,507:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:39,454:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:55:39,454:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:55:39,454:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:55:39,456:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:55:39,457:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:55:39,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:55:39,462:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:39,463:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:49,451:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:55:49,451:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:55:49,452:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:55:49,455:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:55:49,455:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:55:49,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:55:49,457:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:49,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:59,450:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:55:59,453:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:55:59,453:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:55:59,454:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:55:59,455:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:55:59,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:55:59,457:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:55:59,462:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:09,453:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:56:09,455:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:56:09,455:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:56:09,459:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:56:09,459:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:56:09,462:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:56:09,464:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:09,468:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:19,451:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:56:19,453:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:56:19,454:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:56:19,457:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:56:19,460:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:56:19,461:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:56:19,464:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:19,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:29,450:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:56:29,451:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:56:29,451:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:56:29,453:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:56:29,453:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:56:29,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:56:29,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:29,462:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:39,462:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:56:39,463:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:56:39,464:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:56:39,465:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:56:39,466:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:56:39,473:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:56:39,476:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:39,479:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:49,459:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:56:49,461:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:56:49,463:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:56:49,467:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:56:49,471:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:56:49,474:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:56:49,479:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:49,486:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:59,457:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:56:59,459:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:56:59,460:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:56:59,462:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:56:59,466:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:56:59,468:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:56:59,472:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:56:59,474:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:09,464:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:57:09,468:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:57:09,468:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:57:09,470:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:57:09,473:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:57:09,474:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:57:09,482:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:09,485:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:19,450:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:57:19,451:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:57:19,451:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:57:19,451:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:57:19,451:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:57:19,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:57:19,457:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:19,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:29,460:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:57:29,464:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:57:29,464:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:57:29,466:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:57:29,466:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:57:29,469:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:57:29,479:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:29,485:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:39,445:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:57:39,446:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:57:39,446:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:57:39,449:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:57:39,449:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:57:39,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:57:39,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:39,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:49,453:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:57:49,456:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:57:49,457:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:57:49,462:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:57:49,463:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:57:49,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:57:49,466:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:49,467:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:59,457:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:57:59,461:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:57:59,461:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:57:59,463:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:57:59,463:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:57:59,467:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:57:59,469:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:57:59,469:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:09,448:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:58:09,449:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:58:09,449:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:58:09,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:58:09,450:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:58:09,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:58:09,454:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:09,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:19,459:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:58:19,462:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:58:19,463:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:58:19,466:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:58:19,466:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:58:19,468:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:58:19,472:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:19,474:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:29,449:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:58:29,450:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:58:29,450:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:58:29,451:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:58:29,451:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:58:29,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:58:29,459:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:29,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:39,452:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:58:39,453:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:58:39,453:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:58:39,454:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:58:39,455:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:58:39,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:58:39,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:39,468:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:49,446:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:58:49,447:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:58:49,447:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:58:49,448:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:58:49,448:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:58:49,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:58:49,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:49,454:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:59,449:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:58:59,450:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:58:59,454:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:58:59,458:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:58:59,459:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:58:59,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:58:59,462:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:58:59,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:09,448:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:59:09,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:59:09,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:59:09,449:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:59:09,449:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:59:09,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:59:09,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:09,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:19,451:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:59:19,453:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:59:19,456:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:59:19,459:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:59:19,460:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:59:19,461:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:59:19,464:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:19,468:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:29,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:59:29,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:59:29,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:59:29,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:59:29,452:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:59:29,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:59:29,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:29,466:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:39,450:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:59:39,451:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:59:39,451:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:59:39,452:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:59:39,453:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:59:39,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:59:39,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:39,462:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:49,453:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:59:49,454:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:59:49,454:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:59:49,456:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:59:49,457:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:59:49,459:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:59:49,463:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:49,467:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:59,446:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@21:59:59,447:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@21:59:59,447:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@21:59:59,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@21:59:59,450:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@21:59:59,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@21:59:59,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@21:59:59,457:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:09,445:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:00:09,446:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:00:09,446:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:00:09,446:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:00:09,446:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:00:09,448:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:00:09,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:09,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:19,449:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:00:19,450:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:00:19,450:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:00:19,455:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:00:19,455:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:00:19,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:00:19,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:19,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:29,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:00:29,447:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:00:29,447:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:00:29,449:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:00:29,450:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:00:29,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:00:29,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:29,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:39,452:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:00:39,456:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:00:39,456:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:00:39,459:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:00:39,463:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:00:39,467:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:00:39,470:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:39,473:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:49,446:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:00:49,447:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:00:49,447:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:00:49,448:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:00:49,448:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:00:49,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:00:49,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:49,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:59,445:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:00:59,446:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:00:59,447:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:00:59,448:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:00:59,449:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:00:59,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:00:59,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:00:59,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:09,449:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:01:09,450:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:01:09,451:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:01:09,454:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:01:09,455:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:01:09,461:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:01:09,464:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:09,466:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:19,445:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:01:19,446:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:01:19,446:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:01:19,446:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:01:19,447:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:01:19,447:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:01:19,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:19,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:29,453:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:01:29,454:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:01:29,455:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:01:29,456:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:01:29,460:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:01:29,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:01:29,471:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:29,475:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:39,465:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:01:39,466:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:01:39,466:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:01:39,477:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:01:39,477:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:01:39,486:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:01:39,487:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:39,488:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:49,450:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:01:49,451:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:01:49,451:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:01:49,451:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:01:49,451:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:01:49,454:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:01:49,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:49,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:59,448:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:01:59,450:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:01:59,450:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:01:59,455:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:01:59,455:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:01:59,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:01:59,457:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:01:59,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:09,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:02:09,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:02:09,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:02:09,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:02:09,450:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:02:09,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:02:09,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:09,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:19,451:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:02:19,451:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:02:19,451:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:02:19,453:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:02:19,454:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:02:19,457:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:02:19,459:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:19,461:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:29,454:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:02:29,456:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:02:29,460:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:02:29,461:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:02:29,462:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:02:29,463:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:02:29,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:29,468:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:39,450:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:02:39,452:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:02:39,452:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:02:39,455:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:02:39,456:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:02:39,457:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:02:39,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:39,461:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:49,458:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:02:49,460:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:02:49,460:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:02:49,462:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:02:49,464:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:02:49,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:02:49,467:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:49,471:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:59,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:02:59,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:02:59,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:02:59,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:02:59,451:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:02:59,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:02:59,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:02:59,461:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:09,454:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:03:09,455:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:03:09,455:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:03:09,457:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:03:09,458:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:03:09,462:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:03:09,465:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:09,468:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:19,446:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:03:19,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:03:19,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:03:19,449:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:03:19,449:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:03:19,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:03:19,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:19,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:29,446:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:03:29,446:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:03:29,447:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:03:29,449:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:03:29,449:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:03:29,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:03:29,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:29,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:39,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:03:39,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:03:39,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:03:39,448:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:03:39,449:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:03:39,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:03:39,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:39,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:49,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:03:49,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:03:49,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:03:49,449:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:03:49,449:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:03:49,453:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:03:49,460:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:49,464:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:59,448:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:03:59,449:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:03:59,449:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:03:59,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:03:59,451:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:03:59,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:03:59,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:03:59,458:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:09,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:04:09,447:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:04:09,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:04:09,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:04:09,450:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:04:09,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:04:09,452:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:09,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:19,453:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:04:19,454:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:04:19,454:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:04:19,459:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:04:19,460:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:04:19,466:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:04:19,470:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:19,473:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:29,445:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:04:29,445:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:04:29,445:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:04:29,446:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:04:29,446:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:04:29,447:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:04:29,447:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:29,448:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:39,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:04:39,447:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:04:39,447:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:04:39,448:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:04:39,448:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:04:39,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:04:39,451:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:39,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:49,447:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161116@22:04:49,448:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161116@22:04:49,448:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:04:49,448:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:04:49,448:org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
[INFO ]20161116@22:04:49,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40677 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:04:49,449:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:04:49,450:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40677 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:30:39,596:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@22:30:40,641:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@22:30:41,377:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@22:30:41,382:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@22:30:41,384:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@22:30:42,574:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38543.
[INFO ]20161116@22:30:43,695:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@22:30:43,852:Remoting - Starting remoting
[INFO ]20161116@22:30:44,515:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:39483]
[INFO ]20161116@22:30:44,527:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:39483]
[INFO ]20161116@22:30:44,567:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 39483.
[INFO ]20161116@22:30:44,703:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@22:30:44,788:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@22:30:44,845:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-b32b39ba-5a90-4f63-97e8-1882589022df
[INFO ]20161116@22:30:44,891:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@22:30:45,252:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@22:30:46,352:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@22:30:46,363:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@22:30:46,800:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@22:30:46,895:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52301.
[INFO ]20161116@22:30:46,898:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 52301
[INFO ]20161116@22:30:46,901:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:30:46,910:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:52301 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 52301)
[INFO ]20161116@22:30:46,922:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:30:50,192:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@22:30:51,060:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@22:30:51,066:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:52301 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:30:51,092:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@22:30:51,286:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@22:30:51,391:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@22:30:51,400:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:52301 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:30:51,403:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@22:30:52,397:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:30:52,592:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:30:52,892:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:80
[INFO ]20161116@22:30:52,949:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:80) with 1 output partitions
[INFO ]20161116@22:30:52,950:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:80)
[INFO ]20161116@22:30:52,955:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:30:52,957:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:30:53,012:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:30:53,142:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@22:30:53,224:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@22:30:53,233:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:52301 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:30:53,234:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:30:53,258:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:30:53,284:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@22:30:53,561:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@22:30:53,591:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@22:30:53,652:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:30:53,695:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@22:30:53,702:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@22:30:53,705:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@22:30:53,707:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@22:30:53,709:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@22:32:23,334:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@22:32:23,470:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 89997 ms on localhost (1/1)
[INFO ]20161116@22:32:23,481:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:80) finished in 90.128 s
[INFO ]20161116@22:32:23,487:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:32:23,526:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:80, took 90.632654 s
[INFO ]20161116@22:32:23,612:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:81
[INFO ]20161116@22:32:23,620:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:81) with 1 output partitions
[INFO ]20161116@22:32:23,620:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:81)
[INFO ]20161116@22:32:23,620:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:32:23,621:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:32:23,622:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64), which has no missing parents
[INFO ]20161116@22:32:23,640:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@22:32:23,702:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@22:32:23,703:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:52301 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:32:23,704:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:32:23,707:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64)
[INFO ]20161116@22:32:23,711:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@22:32:23,720:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@22:32:23,720:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@22:32:23,731:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:33:05,575:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@22:33:06,616:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@22:33:07,324:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@22:33:07,326:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@22:33:07,328:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@22:33:08,186:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 47497.
[INFO ]20161116@22:33:09,286:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@22:33:09,434:Remoting - Starting remoting
[INFO ]20161116@22:33:10,129:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:40359]
[INFO ]20161116@22:33:10,135:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:40359]
[INFO ]20161116@22:33:10,175:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 40359.
[INFO ]20161116@22:33:10,298:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@22:33:10,385:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@22:33:10,409:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-731a0e16-df81-4504-b136-34734912238f
[INFO ]20161116@22:33:10,439:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@22:33:10,669:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161116@22:33:11,609:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161116@22:33:11,699:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161116@22:33:11,710:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4041
[INFO ]20161116@22:33:12,157:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@22:33:12,243:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47758.
[INFO ]20161116@22:33:12,245:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 47758
[INFO ]20161116@22:33:12,248:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:33:12,253:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:47758 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 47758)
[INFO ]20161116@22:33:12,268:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:33:14,909:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@22:33:15,752:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@22:33:15,761:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:47758 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:33:15,783:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@22:33:16,063:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@22:33:16,198:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@22:33:16,203:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:47758 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:33:16,211:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@22:40:05,880:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@22:40:06,889:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@22:40:07,583:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@22:40:07,584:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@22:40:07,586:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@22:40:08,359:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37679.
[INFO ]20161116@22:40:09,357:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@22:40:09,530:Remoting - Starting remoting
[INFO ]20161116@22:40:10,243:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:49781]
[INFO ]20161116@22:40:10,256:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:49781]
[INFO ]20161116@22:40:10,300:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 49781.
[INFO ]20161116@22:40:10,410:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@22:40:10,477:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@22:40:10,502:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-93346a09-2070-462e-af9c-44a99a5fe884
[INFO ]20161116@22:40:10,540:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@22:40:10,808:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161116@22:40:11,737:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161116@22:40:11,850:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO ]20161116@22:40:11,983:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
[INFO ]20161116@22:40:12,010:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4042
[INFO ]20161116@22:40:12,466:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@22:40:12,551:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34826.
[INFO ]20161116@22:40:12,553:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 34826
[INFO ]20161116@22:40:12,556:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:40:12,570:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:34826 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 34826)
[INFO ]20161116@22:40:12,593:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:40:15,677:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@22:40:16,563:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@22:40:16,569:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:34826 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:40:16,594:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@22:40:16,949:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@22:40:17,143:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@22:40:17,147:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:34826 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:40:17,152:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@22:41:04,390:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3143 bytes result sent to driver
[INFO ]20161116@22:41:04,487:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:81) finished in 520.759 s
[INFO ]20161116@22:41:04,494:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:81, took 521.086598 s
[INFO ]20161116@22:41:04,503:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 520764 ms on localhost (1/1)
[INFO ]20161116@22:41:04,504:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:41:04,667:org.apache.spark.SparkContext - Starting job: take at null:-1
[INFO ]20161116@22:41:04,676:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:41:04,687:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:64)
[INFO ]20161116@22:41:04,692:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at null:-1) with 1 output partitions
[INFO ]20161116@22:41:04,694:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (take at null:-1)
[INFO ]20161116@22:41:04,694:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2, ShuffleMapStage 3)
[INFO ]20161116@22:41:04,696:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2, ShuffleMapStage 3)
[INFO ]20161116@22:41:04,712:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:41:04,749:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@22:41:04,868:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@22:41:04,928:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.6 KB)
[INFO ]20161116@22:41:04,936:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:52301 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:41:04,937:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:41:04,951:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:41:04,955:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:52301 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:41:04,960:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@22:41:04,964:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@22:41:04,965:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@22:41:04,977:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64), which has no missing parents
[INFO ]20161116@22:41:04,997:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:41:05,001:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 322.1 KB)
[INFO ]20161116@22:41:05,058:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 324.4 KB)
[INFO ]20161116@22:41:05,063:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:52301 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:41:05,065:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:41:05,065:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:64)
[INFO ]20161116@22:41:05,078:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@22:41:05,611:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161116@22:41:05,622:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@22:41:05,626:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@22:41:05,647:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:41:05,794:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at HW2_Part2.java:50) finished in 0.816 s
[INFO ]20161116@22:41:05,796:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:41:05,798:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 3)
[INFO ]20161116@22:41:05,799:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 821 ms on localhost (1/1)
[INFO ]20161116@22:41:05,800:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:41:05,810:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 4)
[INFO ]20161116@22:41:05,811:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:41:06,103:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:52301 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:41:06,200:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@22:41:06,242:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:64) finished in 1.163 s
[INFO ]20161116@22:41:06,243:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:41:06,243:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@22:41:06,243:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 4)
[INFO ]20161116@22:41:06,243:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:41:06,248:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[8] at join at HW2_Part2.java:78), which has no missing parents
[INFO ]20161116@22:41:06,249:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 620 ms on localhost (1/1)
[INFO ]20161116@22:41:06,250:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:41:06,264:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 321.7 KB)
[INFO ]20161116@22:41:06,287:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1964.0 B, free 323.6 KB)
[INFO ]20161116@22:41:06,297:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:52301 (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@22:41:06,299:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:41:06,300:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[8] at join at HW2_Part2.java:78)
[INFO ]20161116@22:41:06,302:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@22:41:06,313:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@22:41:06,313:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@22:41:06,389:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:41:06,402:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 26 ms
[INFO ]20161116@22:41:06,471:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:41:06,474:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161116@22:41:07,162:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 4052 bytes result sent to driver
[INFO ]20161116@22:41:07,183:org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (take at null:-1) finished in 0.864 s
[INFO ]20161116@22:41:07,185:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at null:-1, took 2.515299 s
[INFO ]20161116@22:41:07,189:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 875 ms on localhost (1/1)
[INFO ]20161116@22:41:07,197:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:41:07,220:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@22:41:07,320:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@22:41:07,415:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@22:41:07,464:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@22:41:07,467:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@22:41:07,477:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@22:41:07,493:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@22:41:07,532:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@22:41:07,611:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@22:41:07,611:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@22:41:07,613:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a0a157a0-cfe4-45b6-9a92-3f9cc9fb9b44
[INFO ]20161116@22:42:24,943:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@22:42:27,332:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@22:42:28,645:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@22:42:28,648:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@22:42:28,652:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@22:42:29,451:org.apache.spark.SparkContext - Running Spark version 1.6.0
[INFO ]20161116@22:42:29,983:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 46740.
[WARN ]20161116@22:42:32,293:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@22:42:32,307:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@22:42:32,560:Remoting - Starting remoting
[INFO ]20161116@22:42:33,592:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:49217]
[INFO ]20161116@22:42:33,600:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:49217]
[INFO ]20161116@22:42:33,647:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 49217.
[INFO ]20161116@22:42:33,784:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@22:42:33,894:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@22:42:33,975:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-1a44f4da-33de-4aa3-80a4-25a314bf9cd0
[INFO ]20161116@22:42:33,983:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@22:42:33,986:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@22:42:33,994:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@22:42:34,065:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@22:42:34,420:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@22:42:35,911:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 32884.
[INFO ]20161116@22:42:36,166:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@22:42:36,183:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@22:42:36,825:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@22:42:36,943:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33083.
[INFO ]20161116@22:42:36,948:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 33083
[INFO ]20161116@22:42:36,960:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:42:36,970:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:33083 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 33083)
[INFO ]20161116@22:42:37,018:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:42:38,591:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@22:42:38,979:Remoting - Starting remoting
[INFO ]20161116@22:42:40,113:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:47298]
[INFO ]20161116@22:42:40,130:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:47298]
[INFO ]20161116@22:42:40,175:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 47298.
[INFO ]20161116@22:42:40,339:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@22:42:40,491:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@22:42:40,578:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a5966971-2aa9-4793-b5fb-3ba0fac98085
[INFO ]20161116@22:42:40,689:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@22:42:40,873:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@22:42:41,122:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@22:42:42,330:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@22:42:42,348:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33083 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:42,374:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@22:42:42,810:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@22:42:42,924:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@22:42:42,925:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33083 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:42,954:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[WARN ]20161116@22:42:43,054:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161116@22:42:43,228:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO ]20161116@22:42:43,386:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
[INFO ]20161116@22:42:43,421:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4042
[INFO ]20161116@22:42:44,256:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@22:42:44,394:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37429.
[INFO ]20161116@22:42:44,396:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 37429
[INFO ]20161116@22:42:44,403:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:42:44,479:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:37429 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 37429)
[INFO ]20161116@22:42:44,490:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:42:44,594:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:42:45,004:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:64
[INFO ]20161116@22:42:45,067:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:64) with 1 output partitions
[INFO ]20161116@22:42:45,069:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:64)
[INFO ]20161116@22:42:45,073:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:42:45,077:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:42:45,115:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:42:45,186:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@22:42:45,244:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@22:42:45,246:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33083 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:45,248:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:42:45,265:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:42:45,271:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@22:42:45,565:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@22:42:45,633:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@22:42:45,898:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:42:46,008:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@22:42:46,013:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@22:42:46,013:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@22:42:46,013:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@22:42:46,014:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@22:42:46,387:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@22:42:46,523:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:64) finished in 1.197 s
[INFO ]20161116@22:42:46,520:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1080 ms on localhost (1/1)
[INFO ]20161116@22:42:46,526:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:42:46,581:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:64, took 1.571647 s
[INFO ]20161116@22:42:46,788:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@22:42:46,898:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:33083 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:47,044:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:42:47,247:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:82
[INFO ]20161116@22:42:47,249:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:82) with 1 output partitions
[INFO ]20161116@22:42:47,249:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:82)
[INFO ]20161116@22:42:47,249:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:42:47,250:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:42:47,252:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:42:47,267:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@22:42:47,305:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@22:42:47,314:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:33083 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:47,327:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:42:47,328:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:42:47,328:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@22:42:47,335:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@22:42:47,336:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@22:42:47,359:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:42:47,435:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@22:42:47,503:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:82) finished in 0.159 s
[INFO ]20161116@22:42:47,505:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:82, took 0.256994 s
[INFO ]20161116@22:42:47,514:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 169 ms on localhost (1/1)
[INFO ]20161116@22:42:47,514:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:42:47,562:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:83
[INFO ]20161116@22:42:47,577:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:83) with 1 output partitions
[INFO ]20161116@22:42:47,577:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:83)
[INFO ]20161116@22:42:47,578:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:42:47,578:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:42:47,580:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65), which has no missing parents
[INFO ]20161116@22:42:47,590:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@22:42:47,639:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@22:42:47,647:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:33083 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:47,649:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:42:47,649:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:42:47,650:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@22:42:47,660:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@22:42:47,661:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@22:42:47,680:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:42:47,759:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@22:42:47,812:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:83) finished in 0.147 s
[INFO ]20161116@22:42:47,813:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 152 ms on localhost (1/1)
[INFO ]20161116@22:42:47,826:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:83, took 0.249811 s
[INFO ]20161116@22:42:47,830:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:42:48,041:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:84
[INFO ]20161116@22:42:48,048:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:42:48,059:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:42:48,061:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:84) with 1 output partitions
[INFO ]20161116@22:42:48,069:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (take at HW2_Part2.java:84)
[INFO ]20161116@22:42:48,069:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@22:42:48,071:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@22:42:48,083:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:42:48,116:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@22:42:48,176:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.6 KB)
[INFO ]20161116@22:42:48,178:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:33083 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:48,179:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:42:48,182:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:42:48,193:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@22:42:48,201:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@22:42:48,202:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@22:42:48,207:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65), which has no missing parents
[INFO ]20161116@22:42:48,229:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@22:42:48,246:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:42:48,280:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@22:42:48,290:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:33083 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:48,291:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:42:48,302:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:42:48,311:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@22:42:48,560:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@22:42:48,564:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:33083 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:48,566:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@22:42:48,567:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:33083 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:48,878:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@22:42:48,886:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@22:42:48,890:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@22:42:48,924:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:42:49,011:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:50) finished in 0.805 s
[INFO ]20161116@22:42:49,018:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:42:49,023:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@22:42:49,024:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161116@22:42:49,031:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 811 ms on localhost (1/1)
[INFO ]20161116@22:42:49,032:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:42:49,037:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:42:49,601:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@22:42:49,633:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:65) finished in 1.321 s
[INFO ]20161116@22:42:49,633:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:42:49,633:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@22:42:49,633:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161116@22:42:49,634:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:42:49,635:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:80), which has no missing parents
[INFO ]20161116@22:42:49,637:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 322.5 KB)
[INFO ]20161116@22:42:49,640:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 738 ms on localhost (1/1)
[INFO ]20161116@22:42:49,640:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:42:49,671:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:33083 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:49,683:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1964.0 B, free 318.3 KB)
[INFO ]20161116@22:42:49,692:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:33083 (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:49,694:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:42:49,695:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:80)
[INFO ]20161116@22:42:49,708:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@22:42:49,711:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@22:42:49,712:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@22:42:49,840:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:42:49,860:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 39 ms
[INFO ]20161116@22:42:49,975:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:42:49,988:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
[INFO ]20161116@22:42:51,403:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 4052 bytes result sent to driver
[INFO ]20161116@22:42:51,424:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (take at HW2_Part2.java:84) finished in 1.702 s
[INFO ]20161116@22:42:51,427:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: take at HW2_Part2.java:84, took 3.384760 s
[INFO ]20161116@22:42:51,430:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 1712 ms on localhost (1/1)
[INFO ]20161116@22:42:51,431:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:42:51,777:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@22:42:51,782:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@22:42:51,979:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:86
[INFO ]20161116@22:42:51,999:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 142 bytes
[INFO ]20161116@22:42:52,014:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 142 bytes
[INFO ]20161116@22:42:52,027:org.apache.spark.scheduler.DAGScheduler - Got job 4 (saveAsTextFile at HW2_Part2.java:86) with 1 output partitions
[INFO ]20161116@22:42:52,028:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (saveAsTextFile at HW2_Part2.java:86)
[INFO ]20161116@22:42:52,028:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@22:42:52,028:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:42:52,041:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[9] at saveAsTextFile at HW2_Part2.java:86), which has no missing parents
[INFO ]20161116@22:42:52,146:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@22:42:52,146:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@22:42:52,162:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:33083 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:52,163:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161116@22:42:52,174:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:33083 in memory (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@22:42:52,319:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 71.2 KB, free 378.0 KB)
[INFO ]20161116@22:42:52,363:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.8 KB, free 402.7 KB)
[INFO ]20161116@22:42:52,366:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:33083 (size: 24.8 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:52,370:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:42:52,381:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[9] at saveAsTextFile at HW2_Part2.java:86)
[INFO ]20161116@22:42:52,381:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@22:42:52,383:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@22:42:52,383:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 6)
[INFO ]20161116@22:42:52,554:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:42:52,554:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@22:42:52,574:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:42:52,580:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
[INFO ]20161116@22:42:53,007:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@22:42:53,239:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611162242_0008_m_000000_6' to file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv_1479364957934/_temporary/0/task_201611162242_0008_m_000000
[INFO ]20161116@22:42:53,241:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611162242_0008_m_000000_6: Committed
[INFO ]20161116@22:42:53,276:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 6). 2038 bytes result sent to driver
[INFO ]20161116@22:42:53,295:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (saveAsTextFile at HW2_Part2.java:86) finished in 0.900 s
[INFO ]20161116@22:42:53,297:org.apache.spark.scheduler.DAGScheduler - Job 4 finished: saveAsTextFile at HW2_Part2.java:86, took 1.312237 s
[INFO ]20161116@22:42:53,307:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 6) in 912 ms on localhost (1/1)
[INFO ]20161116@22:42:53,307:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:42:53,425:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@22:42:53,481:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@22:42:53,525:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@22:42:53,533:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@22:42:53,536:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@22:42:53,564:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@22:42:53,587:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@22:42:53,595:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@22:42:53,613:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@22:42:53,637:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@22:42:53,645:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-f6cab18e-18be-4106-91b9-ebf8968f42c2
[INFO ]20161116@22:42:53,733:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@22:42:53,742:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:37429 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:53,771:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@22:42:54,135:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@22:42:54,267:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@22:42:54,274:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:37429 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:42:54,281:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@22:48:44,889:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@22:48:46,398:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@22:48:47,756:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@22:48:47,757:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@22:48:47,782:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@22:48:49,246:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39007.
[INFO ]20161116@22:48:50,576:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@22:48:50,708:Remoting - Starting remoting
[INFO ]20161116@22:48:51,525:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:40468]
[INFO ]20161116@22:48:51,534:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:40468]
[INFO ]20161116@22:48:51,570:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 40468.
[INFO ]20161116@22:48:51,635:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@22:48:51,703:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@22:48:51,753:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-4d52487f-1a3e-45bb-bc7f-64c5b2342b4c
[INFO ]20161116@22:48:51,805:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@22:48:52,018:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@22:48:52,857:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@22:48:52,863:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@22:48:53,163:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@22:48:53,226:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40218.
[INFO ]20161116@22:48:53,232:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 40218
[INFO ]20161116@22:48:53,235:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:48:53,242:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:40218 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 40218)
[INFO ]20161116@22:48:53,250:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:48:55,070:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@22:48:55,658:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@22:48:55,667:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40218 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:48:55,684:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@22:48:55,919:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@22:48:56,006:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@22:48:56,008:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40218 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:48:56,013:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@22:48:56,799:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:48:56,997:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:64
[INFO ]20161116@22:48:57,039:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:64) with 1 output partitions
[INFO ]20161116@22:48:57,044:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:64)
[INFO ]20161116@22:48:57,045:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:48:57,047:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:48:57,079:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:48:57,165:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@22:48:57,201:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@22:48:57,203:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40218 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:48:57,206:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:48:57,214:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:48:57,222:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@22:48:57,365:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@22:48:57,389:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@22:48:57,455:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:48:57,476:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@22:48:57,477:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@22:48:57,479:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@22:48:57,480:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@22:48:57,480:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@22:48:57,653:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@22:48:57,719:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 409 ms on localhost (1/1)
[INFO ]20161116@22:48:57,727:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:64) finished in 0.469 s
[INFO ]20161116@22:48:57,726:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:48:57,750:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:64, took 0.750831 s
[INFO ]20161116@22:48:57,841:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@22:48:57,936:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:40218 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:48:57,997:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:48:58,085:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:82
[INFO ]20161116@22:48:58,090:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:82) with 1 output partitions
[INFO ]20161116@22:48:58,090:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:82)
[INFO ]20161116@22:48:58,091:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:48:58,091:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:48:58,092:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:48:58,095:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@22:48:58,114:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@22:48:58,116:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:40218 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:48:58,122:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:48:58,125:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:48:58,125:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@22:48:58,127:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@22:48:58,127:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@22:48:58,132:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:48:58,174:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@22:48:58,216:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:82) finished in 0.083 s
[INFO ]20161116@22:48:58,217:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:82, took 0.127996 s
[INFO ]20161116@22:48:58,221:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 89 ms on localhost (1/1)
[INFO ]20161116@22:48:58,222:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:48:58,245:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:83
[INFO ]20161116@22:48:58,253:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:83) with 1 output partitions
[INFO ]20161116@22:48:58,254:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:83)
[INFO ]20161116@22:48:58,254:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:48:58,254:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:48:58,255:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65), which has no missing parents
[INFO ]20161116@22:48:58,263:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@22:48:58,284:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@22:48:58,286:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:40218 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:48:58,289:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:48:58,290:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:48:58,290:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@22:48:58,298:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@22:48:58,299:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@22:48:58,313:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:48:58,350:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@22:48:58,382:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:83) finished in 0.076 s
[INFO ]20161116@22:48:58,383:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:83, took 0.137365 s
[INFO ]20161116@22:48:58,387:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 83 ms on localhost (1/1)
[INFO ]20161116@22:48:58,390:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:48:58,476:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:84
[INFO ]20161116@22:48:58,483:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:48:58,483:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:48:58,489:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:84) with 1 output partitions
[INFO ]20161116@22:48:58,495:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (take at HW2_Part2.java:84)
[INFO ]20161116@22:48:58,495:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@22:48:58,495:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@22:48:58,506:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:48:58,522:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@22:48:58,541:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.6 KB)
[INFO ]20161116@22:48:58,546:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:40218 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:48:58,547:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:48:58,553:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:48:58,555:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@22:48:58,559:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@22:48:58,560:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@22:48:58,566:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65), which has no missing parents
[INFO ]20161116@22:48:58,575:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@22:48:58,595:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:48:58,621:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@22:48:58,630:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:40218 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:48:58,631:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:48:58,636:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:48:58,637:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@22:48:58,808:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@22:48:58,810:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:40218 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:48:58,811:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@22:48:58,813:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:40218 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:48:59,016:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@22:48:59,020:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@22:48:59,024:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@22:48:59,039:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:48:59,104:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:50) finished in 0.538 s
[INFO ]20161116@22:48:59,105:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:48:59,106:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@22:48:59,107:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161116@22:48:59,108:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:48:59,112:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 544 ms on localhost (1/1)
[INFO ]20161116@22:48:59,115:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:48:59,464:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@22:48:59,482:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:65) finished in 0.840 s
[INFO ]20161116@22:48:59,483:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:48:59,483:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@22:48:59,483:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161116@22:48:59,483:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:48:59,484:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:80), which has no missing parents
[INFO ]20161116@22:48:59,486:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 322.5 KB)
[INFO ]20161116@22:48:59,490:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 464 ms on localhost (1/1)
[INFO ]20161116@22:48:59,490:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:48:59,511:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:40218 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:48:59,522:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1964.0 B, free 318.3 KB)
[INFO ]20161116@22:48:59,526:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:40218 (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@22:48:59,529:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:48:59,529:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:80)
[INFO ]20161116@22:48:59,530:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@22:48:59,538:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@22:48:59,539:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@22:48:59,603:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:48:59,611:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
[INFO ]20161116@22:48:59,690:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:48:59,691:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@22:49:00,780:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 4052 bytes result sent to driver
[INFO ]20161116@22:49:00,799:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (take at HW2_Part2.java:84) finished in 1.255 s
[INFO ]20161116@22:49:00,800:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: take at HW2_Part2.java:84, took 2.321386 s
[INFO ]20161116@22:49:00,809:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 1256 ms on localhost (1/1)
[INFO ]20161116@22:49:00,809:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:49:01,089:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@22:49:01,290:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:86
[INFO ]20161116@22:49:01,305:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@22:49:01,316:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@22:49:01,324:org.apache.spark.scheduler.DAGScheduler - Got job 4 (saveAsTextFile at HW2_Part2.java:86) with 1 output partitions
[INFO ]20161116@22:49:01,325:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (saveAsTextFile at HW2_Part2.java:86)
[INFO ]20161116@22:49:01,327:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@22:49:01,328:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:49:01,333:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[9] at saveAsTextFile at HW2_Part2.java:86), which has no missing parents
[INFO ]20161116@22:49:01,401:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@22:49:01,401:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@22:49:01,404:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:40218 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:49:01,408:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161116@22:49:01,410:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:40218 in memory (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@22:49:01,540:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 71.2 KB, free 378.0 KB)
[INFO ]20161116@22:49:01,574:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.8 KB, free 402.7 KB)
[INFO ]20161116@22:49:01,578:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:40218 (size: 24.8 KB, free: 1027.3 MB)
[INFO ]20161116@22:49:01,579:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:49:01,582:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[9] at saveAsTextFile at HW2_Part2.java:86)
[INFO ]20161116@22:49:01,585:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@22:49:01,587:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@22:49:01,587:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 6)
[INFO ]20161116@22:49:01,706:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:49:01,711:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
[INFO ]20161116@22:49:01,724:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:49:01,730:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
[INFO ]20161116@22:49:02,005:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@22:49:02,136:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611162249_0008_m_000000_6' to file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv_1479365333767/_temporary/0/task_201611162249_0008_m_000000
[INFO ]20161116@22:49:02,138:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611162249_0008_m_000000_6: Committed
[INFO ]20161116@22:49:02,156:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 6). 2038 bytes result sent to driver
[INFO ]20161116@22:49:02,174:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (saveAsTextFile at HW2_Part2.java:86) finished in 0.574 s
[INFO ]20161116@22:49:02,177:org.apache.spark.scheduler.DAGScheduler - Job 4 finished: saveAsTextFile at HW2_Part2.java:86, took 0.884988 s
[INFO ]20161116@22:49:02,184:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 6) in 588 ms on localhost (1/1)
[INFO ]20161116@22:49:02,185:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:49:02,284:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@22:49:02,315:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@22:49:02,335:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@22:49:02,336:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@22:49:02,343:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@22:49:02,360:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@22:49:02,385:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@22:49:02,394:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@22:49:02,401:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@22:49:02,416:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@22:49:02,418:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-7b99af1e-1f5e-44d9-905f-fd7952e42c51
[INFO ]20161116@22:50:25,756:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@22:50:26,696:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@22:50:27,449:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@22:50:27,450:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@22:50:27,453:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@22:50:28,248:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34782.
[INFO ]20161116@22:50:29,118:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@22:50:29,391:Remoting - Starting remoting
[INFO ]20161116@22:50:30,066:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:41596]
[INFO ]20161116@22:50:30,084:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:41596]
[INFO ]20161116@22:50:30,120:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 41596.
[INFO ]20161116@22:50:30,214:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@22:50:30,314:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@22:50:30,401:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-bf6ba868-0aa0-40ea-87af-6f955736beaa
[INFO ]20161116@22:50:30,501:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@22:50:30,794:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@22:50:32,352:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@22:50:32,361:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@22:50:32,726:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@22:50:32,796:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35751.
[INFO ]20161116@22:50:32,797:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 35751
[INFO ]20161116@22:50:32,800:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@22:50:32,809:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:35751 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 35751)
[INFO ]20161116@22:50:32,815:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@22:50:35,192:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@22:50:35,803:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@22:50:35,814:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:35751 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:50:35,826:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:46
[INFO ]20161116@22:50:36,043:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@22:50:36,139:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@22:50:36,146:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:35751 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@22:50:36,156:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:48
[INFO ]20161116@22:50:37,032:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:50:37,226:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:64
[INFO ]20161116@22:50:37,285:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:64) with 1 output partitions
[INFO ]20161116@22:50:37,286:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:64)
[INFO ]20161116@22:50:37,287:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:50:37,292:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:50:37,317:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:50:37,395:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@22:50:37,437:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@22:50:37,443:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:35751 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:37,445:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:50:37,455:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:50:37,460:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@22:50:37,724:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@22:50:37,783:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@22:50:37,916:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:50:37,994:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@22:50:38,001:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@22:50:38,006:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@22:50:38,007:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@22:50:38,010:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@22:50:38,339:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@22:50:38,472:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:64) finished in 0.978 s
[INFO ]20161116@22:50:38,473:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 905 ms on localhost (1/1)
[INFO ]20161116@22:50:38,492:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:50:38,519:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:64, took 1.287698 s
[INFO ]20161116@22:50:38,666:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@22:50:38,753:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:35751 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:38,792:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@22:50:39,001:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:82
[INFO ]20161116@22:50:39,008:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:82) with 1 output partitions
[INFO ]20161116@22:50:39,009:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:82)
[INFO ]20161116@22:50:39,010:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:50:39,011:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:50:39,013:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:50:39,028:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@22:50:39,043:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@22:50:39,048:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:35751 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:39,053:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:50:39,055:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:50:39,057:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@22:50:39,061:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@22:50:39,062:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@22:50:39,073:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:50:39,129:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@22:50:39,170:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:82) finished in 0.102 s
[INFO ]20161116@22:50:39,171:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:82, took 0.166640 s
[INFO ]20161116@22:50:39,175:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 104 ms on localhost (1/1)
[INFO ]20161116@22:50:39,175:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:50:39,204:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:83
[INFO ]20161116@22:50:39,214:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:83) with 1 output partitions
[INFO ]20161116@22:50:39,214:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:83)
[INFO ]20161116@22:50:39,214:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@22:50:39,214:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:50:39,216:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65), which has no missing parents
[INFO ]20161116@22:50:39,219:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@22:50:39,242:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@22:50:39,244:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:35751 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:39,248:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:50:39,249:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:50:39,249:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@22:50:39,251:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@22:50:39,251:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@22:50:39,263:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:50:39,314:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@22:50:39,335:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:83) finished in 0.081 s
[INFO ]20161116@22:50:39,337:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:83, took 0.130187 s
[INFO ]20161116@22:50:39,346:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on localhost (1/1)
[INFO ]20161116@22:50:39,347:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:50:39,444:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:84
[INFO ]20161116@22:50:39,453:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:50:39,462:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:50:39,469:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:84) with 1 output partitions
[INFO ]20161116@22:50:39,470:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (take at HW2_Part2.java:84)
[INFO ]20161116@22:50:39,470:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@22:50:39,471:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@22:50:39,481:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50), which has no missing parents
[INFO ]20161116@22:50:39,495:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@22:50:39,530:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.6 KB)
[INFO ]20161116@22:50:39,531:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:35751 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:50:39,532:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:50:39,542:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:50)
[INFO ]20161116@22:50:39,545:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@22:50:39,549:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@22:50:39,558:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@22:50:39,557:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65), which has no missing parents
[INFO ]20161116@22:50:39,572:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@22:50:39,574:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@22:50:39,593:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@22:50:39,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:35751 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:50:39,608:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:50:39,609:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:65)
[INFO ]20161116@22:50:39,611:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@22:50:39,792:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@22:50:39,794:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:35751 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:39,795:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@22:50:39,797:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:35751 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:39,903:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@22:50:39,917:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@22:50:39,919:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@22:50:39,939:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@22:50:40,004:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:50) finished in 0.447 s
[INFO ]20161116@22:50:40,005:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:50:40,006:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@22:50:40,007:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161116@22:50:40,008:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:50:40,008:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 447 ms on localhost (1/1)
[INFO ]20161116@22:50:40,013:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:50:40,234:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@22:50:40,256:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:65) finished in 0.637 s
[INFO ]20161116@22:50:40,256:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@22:50:40,256:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@22:50:40,256:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161116@22:50:40,256:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@22:50:40,258:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:80), which has no missing parents
[INFO ]20161116@22:50:40,260:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 341 ms on localhost (1/1)
[INFO ]20161116@22:50:40,260:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:50:40,264:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.4 KB, free 322.5 KB)
[INFO ]20161116@22:50:40,279:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1964.0 B, free 324.4 KB)
[INFO ]20161116@22:50:40,282:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:35751 (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:40,290:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:50:40,292:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:80)
[INFO ]20161116@22:50:40,295:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@22:50:40,299:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@22:50:40,299:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@22:50:40,348:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:35751 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:50:40,351:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:35751 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@22:50:40,399:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:50:40,407:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 21 ms
[INFO ]20161116@22:50:40,452:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:50:40,454:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161116@22:50:41,193:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 4052 bytes result sent to driver
[INFO ]20161116@22:50:41,207:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (take at HW2_Part2.java:84) finished in 0.895 s
[INFO ]20161116@22:50:41,211:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: take at HW2_Part2.java:84, took 1.764254 s
[INFO ]20161116@22:50:41,215:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 906 ms on localhost (1/1)
[INFO ]20161116@22:50:41,216:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:50:41,414:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@22:50:41,534:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:86
[INFO ]20161116@22:50:41,550:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@22:50:41,560:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@22:50:41,565:org.apache.spark.scheduler.DAGScheduler - Got job 4 (saveAsTextFile at HW2_Part2.java:86) with 1 output partitions
[INFO ]20161116@22:50:41,569:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (saveAsTextFile at HW2_Part2.java:86)
[INFO ]20161116@22:50:41,569:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@22:50:41,570:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@22:50:41,577:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[9] at saveAsTextFile at HW2_Part2.java:86), which has no missing parents
[INFO ]20161116@22:50:41,693:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 71.2 KB, free 383.3 KB)
[INFO ]20161116@22:50:41,711:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@22:50:41,719:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@22:50:41,721:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161116@22:50:41,729:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:35751 in memory (size: 1964.0 B, free: 1027.3 MB)
[INFO ]20161116@22:50:41,744:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.8 KB, free 402.7 KB)
[INFO ]20161116@22:50:41,747:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:35751 (size: 24.8 KB, free: 1027.3 MB)
[INFO ]20161116@22:50:41,751:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@22:50:41,751:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[9] at saveAsTextFile at HW2_Part2.java:86)
[INFO ]20161116@22:50:41,754:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@22:50:41,755:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@22:50:41,756:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 6)
[INFO ]20161116@22:50:41,856:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:50:41,865:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms
[INFO ]20161116@22:50:41,876:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@22:50:41,877:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@22:50:42,079:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@22:50:42,202:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611162250_0008_m_000000_6' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479365433391/_temporary/0/task_201611162250_0008_m_000000
[INFO ]20161116@22:50:42,208:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611162250_0008_m_000000_6: Committed
[INFO ]20161116@22:50:42,224:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 6). 2038 bytes result sent to driver
[INFO ]20161116@22:50:42,239:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (saveAsTextFile at HW2_Part2.java:86) finished in 0.478 s
[INFO ]20161116@22:50:42,242:org.apache.spark.scheduler.DAGScheduler - Job 4 finished: saveAsTextFile at HW2_Part2.java:86, took 0.701424 s
[INFO ]20161116@22:50:42,247:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 6) in 484 ms on localhost (1/1)
[INFO ]20161116@22:50:42,248:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@22:50:42,359:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@22:50:42,396:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@22:50:42,424:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@22:50:42,425:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@22:50:42,431:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@22:50:42,443:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@22:50:42,457:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@22:50:42,463:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@22:50:42,465:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@22:50:42,488:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@22:50:42,489:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-07afe7ff-358a-4dd6-b431-f0a42175c235
[INFO ]20161116@23:25:32,545:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:25:33,494:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:25:34,150:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:25:34,151:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:25:34,154:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:25:34,945:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 54215.
[INFO ]20161116@23:25:36,568:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:25:36,777:Remoting - Starting remoting
[INFO ]20161116@23:25:37,390:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:33084]
[INFO ]20161116@23:25:37,402:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:33084]
[INFO ]20161116@23:25:37,434:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 33084.
[INFO ]20161116@23:25:37,509:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:25:37,553:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:25:37,600:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-2a6b284f-434b-4f16-9eea-f0bcd7bf3ebd
[INFO ]20161116@23:25:37,622:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:25:37,825:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:25:38,496:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:25:38,505:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:25:38,790:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:25:38,854:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47778.
[INFO ]20161116@23:25:38,855:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 47778
[INFO ]20161116@23:25:38,857:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:25:38,864:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:47778 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 47778)
[INFO ]20161116@23:25:38,870:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:25:40,934:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:25:41,739:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:25:41,749:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:47778 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:25:41,770:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:25:42,024:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:25:42,142:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:25:42,149:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:47778 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:25:42,151:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:25:42,926:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:25:43,101:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:25:43,140:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:25:43,144:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:25:43,146:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:25:43,149:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:25:43,184:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:25:43,260:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:25:43,308:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:25:43,310:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:47778 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:25:43,311:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:25:43,323:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:25:43,331:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:25:43,482:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:25:43,510:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:25:43,599:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:25:43,647:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:25:43,647:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:25:43,648:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:25:43,649:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:25:43,650:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:25:43,862:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:25:43,929:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 491 ms on localhost (1/1)
[INFO ]20161116@23:25:43,939:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.556 s
[INFO ]20161116@23:25:43,943:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:25:43,966:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 0.862934 s
[INFO ]20161116@23:25:44,085:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@23:25:44,180:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:47778 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:25:44,268:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:25:44,565:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:108
[INFO ]20161116@23:25:44,571:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:108) with 1 output partitions
[INFO ]20161116@23:25:44,571:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:108)
[INFO ]20161116@23:25:44,572:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:25:44,572:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:25:44,573:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:25:44,576:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:25:44,611:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:25:44,612:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:47778 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:25:44,613:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:25:44,618:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:25:44,618:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:25:44,620:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:25:44,621:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:25:44,629:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:25:44,678:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:25:44,720:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:108) finished in 0.093 s
[INFO ]20161116@23:25:44,721:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:108, took 0.150915 s
[INFO ]20161116@23:25:44,724:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 100 ms on localhost (1/1)
[INFO ]20161116@23:25:44,725:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:25:44,751:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:109
[INFO ]20161116@23:25:44,759:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:109) with 1 output partitions
[INFO ]20161116@23:25:44,759:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:109)
[INFO ]20161116@23:25:44,759:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:25:44,760:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:25:44,761:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:25:44,781:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:25:44,811:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2001.0 B, free 317.4 KB)
[INFO ]20161116@23:25:44,812:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:47778 (size: 2001.0 B, free: 1027.3 MB)
[INFO ]20161116@23:25:44,823:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:25:44,828:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:25:44,828:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:25:44,830:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:25:44,830:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:25:44,849:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:25:44,904:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:25:44,945:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:109) finished in 0.106 s
[INFO ]20161116@23:25:44,947:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:109, took 0.189092 s
[INFO ]20161116@23:25:44,954:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 116 ms on localhost (1/1)
[INFO ]20161116@23:25:44,954:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:25:45,110:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:110
[INFO ]20161116@23:25:45,152:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:25:45,157:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:25:45,166:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:101)
[INFO ]20161116@23:25:45,167:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:110) with 1 output partitions
[INFO ]20161116@23:25:45,174:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:110)
[INFO ]20161116@23:25:45,174:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:25:45,176:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:25:45,182:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:25:45,228:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@23:25:45,246:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161116@23:25:45,256:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:47778 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:25:45,257:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:25:45,271:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:25:45,273:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:25:45,278:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:25:45,279:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:25:45,285:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:25:45,313:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:25:45,335:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@23:25:45,386:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:25:45,390:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:47778 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:25:45,392:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:25:45,395:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:47778 in memory (size: 2001.0 B, free: 1027.3 MB)
[INFO ]20161116@23:25:45,415:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@23:25:45,426:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:47778 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:25:45,427:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:25:45,428:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:25:45,428:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:25:46,024:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:25:46,032:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:25:46,035:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:25:46,053:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:25:46,093:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.809 s
[INFO ]20161116@23:25:46,095:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:25:46,096:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:25:46,097:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:25:46,097:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:25:46,102:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 816 ms on localhost (1/1)
[INFO ]20161116@23:25:46,102:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:25:46,138:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:25:46,157:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.720 s
[INFO ]20161116@23:25:46,157:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:25:46,158:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:25:46,159:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:25:46,159:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:25:46,160:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:101), which has no missing parents
[INFO ]20161116@23:25:46,162:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 124 ms on localhost (1/1)
[INFO ]20161116@23:25:46,162:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:25:46,215:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 323.6 KB)
[INFO ]20161116@23:25:46,228:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:47778 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:25:46,251:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.9 KB)
[INFO ]20161116@23:25:46,260:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:47778 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:25:46,264:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:25:46,265:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:101)
[INFO ]20161116@23:25:46,267:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:25:46,273:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:25:46,274:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:25:46,353:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:25:46,374:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 42 ms
[INFO ]20161116@23:25:46,412:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:25:46,412:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[ERROR]20161116@23:25:47,135:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.NumberFormatException: For input string: "n/a"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at java.lang.Double.valueOf(Double.java:502)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:90)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161116@23:25:47,240:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.NumberFormatException: For input string: "n/a"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at java.lang.Double.valueOf(Double.java:502)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:90)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161116@23:25:47,256:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 5.0 failed 1 times; aborting job
[INFO ]20161116@23:25:47,271:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:25:47,282:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 5
[INFO ]20161116@23:25:47,300:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:101) failed in 1.017 s
[INFO ]20161116@23:25:47,309:org.apache.spark.scheduler.DAGScheduler - Job 3 failed: take at HW2_Part2.java:110, took 2.190083 s
[INFO ]20161116@23:25:47,367:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@23:25:47,446:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@23:25:47,508:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@23:25:47,543:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@23:25:47,553:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@23:25:47,564:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@23:25:47,580:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@23:25:47,612:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@23:25:47,621:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@23:25:47,636:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-aebe376a-3543-4a39-aad9-4b3d8a0d7cd5
[INFO ]20161116@23:29:17,078:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:29:18,508:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:29:19,155:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:29:19,156:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:29:19,157:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:29:20,071:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42056.
[INFO ]20161116@23:29:21,366:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:29:21,500:Remoting - Starting remoting
[INFO ]20161116@23:29:21,924:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:60992]
[INFO ]20161116@23:29:21,941:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:60992]
[INFO ]20161116@23:29:21,975:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 60992.
[INFO ]20161116@23:29:22,036:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:29:22,108:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:29:22,167:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-1962c1b8-a89c-4bbe-aa93-2a6204b98533
[INFO ]20161116@23:29:22,224:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:29:22,422:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:29:23,309:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:29:23,323:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:29:23,650:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:29:23,720:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38461.
[INFO ]20161116@23:29:23,725:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 38461
[INFO ]20161116@23:29:23,727:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:29:23,737:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:38461 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 38461)
[INFO ]20161116@23:29:23,746:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:29:25,652:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:29:26,257:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:29:26,264:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:38461 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:29:26,283:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:29:26,520:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:29:26,585:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:29:26,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:38461 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:29:26,588:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:29:27,275:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:29:27,469:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:29:27,519:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:29:27,526:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:29:27,527:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:29:27,535:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:29:27,571:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:29:27,645:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:29:27,689:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:29:27,692:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:38461 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:29:27,694:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:29:27,705:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:29:27,712:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:29:27,856:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:29:27,885:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:29:27,951:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:29:27,982:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:29:27,983:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:29:27,985:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:29:27,985:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:29:27,985:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:29:28,171:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:29:28,240:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 436 ms on localhost (1/1)
[INFO ]20161116@23:29:28,246:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.488 s
[INFO ]20161116@23:29:28,258:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:29:28,289:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 0.816832 s
[INFO ]20161116@23:29:28,396:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@23:29:28,497:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:38461 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:29:28,558:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:29:28,773:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:109
[INFO ]20161116@23:29:28,778:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:109) with 1 output partitions
[INFO ]20161116@23:29:28,779:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:109)
[INFO ]20161116@23:29:28,779:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:29:28,779:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:29:28,781:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:29:28,791:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:29:28,813:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:29:28,815:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:38461 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:29:28,816:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:29:28,821:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:29:28,821:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:29:28,831:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:29:28,831:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:29:28,846:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:29:28,873:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:29:28,905:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:109) finished in 0.068 s
[INFO ]20161116@23:29:28,906:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:109, took 0.132316 s
[INFO ]20161116@23:29:28,921:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 74 ms on localhost (1/1)
[INFO ]20161116@23:29:28,922:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:29:28,957:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:110
[INFO ]20161116@23:29:28,962:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:110) with 1 output partitions
[INFO ]20161116@23:29:28,963:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:110)
[INFO ]20161116@23:29:28,963:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:29:28,964:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:29:28,966:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:29:28,978:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:29:29,004:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@23:29:29,010:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:38461 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:29:29,011:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:29:29,013:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:29:29,014:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:29:29,019:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:29:29,019:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:29:29,032:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:29:29,073:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:29:29,100:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:110) finished in 0.076 s
[INFO ]20161116@23:29:29,101:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:110, took 0.142983 s
[INFO ]20161116@23:29:29,102:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on localhost (1/1)
[INFO ]20161116@23:29:29,115:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:29:29,215:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:111
[INFO ]20161116@23:29:29,236:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:29:29,237:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:29:29,243:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:102)
[INFO ]20161116@23:29:29,246:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:111) with 1 output partitions
[INFO ]20161116@23:29:29,248:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:111)
[INFO ]20161116@23:29:29,248:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:29:29,250:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:29:29,258:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:29:29,288:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@23:29:29,303:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161116@23:29:29,304:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:38461 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:29:29,307:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:29:29,310:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:29:29,312:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:29:29,315:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:29:29,316:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:29:29,325:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:29:29,340:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@23:29:29,356:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:29:29,373:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:29:29,374:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:38461 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:29:29,376:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:29:29,377:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:38461 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:29:29,385:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@23:29:29,401:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:38461 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:29:29,402:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:29:29,403:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:29:29,414:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:29:30,210:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:29:30,220:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:29:30,222:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:29:30,250:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:29:30,299:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.975 s
[INFO ]20161116@23:29:30,301:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:29:30,302:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:29:30,303:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:29:30,303:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:29:30,304:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 985 ms on localhost (1/1)
[INFO ]20161116@23:29:30,317:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:29:30,386:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:29:30,402:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.987 s
[INFO ]20161116@23:29:30,402:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:29:30,402:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:29:30,402:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:29:30,402:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:29:30,404:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:102), which has no missing parents
[INFO ]20161116@23:29:30,412:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 183 ms on localhost (1/1)
[INFO ]20161116@23:29:30,412:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:29:30,466:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 323.6 KB)
[INFO ]20161116@23:29:30,492:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:38461 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:29:30,507:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.9 KB)
[INFO ]20161116@23:29:30,512:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:38461 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:29:30,516:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:29:30,516:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:102)
[INFO ]20161116@23:29:30,518:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:29:30,521:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:29:30,521:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:29:30,611:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:29:30,619:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
[INFO ]20161116@23:29:30,658:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:29:30,659:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[ERROR]20161116@23:29:31,466:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at java.lang.Double.valueOf(Double.java:502)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:93)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161116@23:29:31,545:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at java.lang.Double.valueOf(Double.java:502)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:93)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161116@23:29:31,552:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 5.0 failed 1 times; aborting job
[INFO ]20161116@23:29:31,567:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:29:31,578:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 5
[INFO ]20161116@23:29:31,583:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:102) failed in 1.057 s
[INFO ]20161116@23:29:31,593:org.apache.spark.scheduler.DAGScheduler - Job 3 failed: take at HW2_Part2.java:111, took 2.377574 s
[INFO ]20161116@23:29:31,629:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@23:29:31,722:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@23:29:31,773:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@23:29:31,812:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@23:29:31,816:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@23:29:31,821:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@23:29:31,829:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@23:29:31,845:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@23:29:31,852:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@23:29:31,855:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-67c24e53-b643-4822-a146-1119f3c5fab1
[INFO ]20161116@23:31:53,245:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:31:54,111:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:31:54,897:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:31:54,899:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:31:54,900:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:31:55,598:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53252.
[INFO ]20161116@23:31:56,642:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:31:56,807:Remoting - Starting remoting
[INFO ]20161116@23:31:57,469:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:60159]
[INFO ]20161116@23:31:57,489:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:60159]
[INFO ]20161116@23:31:57,534:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 60159.
[INFO ]20161116@23:31:57,640:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:31:57,704:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:31:57,730:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-12bf7fc5-7219-4f48-80ec-79eb05635075
[INFO ]20161116@23:31:57,760:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:31:57,976:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:31:58,859:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:31:58,866:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:31:59,206:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:31:59,256:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59309.
[INFO ]20161116@23:31:59,257:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59309
[INFO ]20161116@23:31:59,260:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:31:59,270:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:59309 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 59309)
[INFO ]20161116@23:31:59,301:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:32:01,787:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:32:02,595:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:32:02,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59309 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:32:02,635:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:32:02,890:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:32:03,017:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:32:03,023:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59309 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:32:03,025:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:32:08,604:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:32:08,893:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:32:08,955:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:32:08,958:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:32:08,961:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:32:08,968:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:32:09,008:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:32:09,098:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:32:09,153:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:32:09,160:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:59309 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:32:09,162:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:32:09,171:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:32:09,182:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:32:09,352:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:32:09,394:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:32:09,470:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:32:09,498:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:32:09,499:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:32:09,499:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:32:09,499:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:32:09,500:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:32:09,674:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:32:09,731:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 432 ms on localhost (1/1)
[INFO ]20161116@23:32:09,746:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.499 s
[INFO ]20161116@23:32:09,743:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:32:09,774:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 0.879302 s
[INFO ]20161116@23:32:10,007:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:32:10,276:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:113
[INFO ]20161116@23:32:10,280:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:113) with 1 output partitions
[INFO ]20161116@23:32:10,280:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:113)
[INFO ]20161116@23:32:10,280:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:32:10,280:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:32:10,283:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:32:10,289:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:32:10,324:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 317.4 KB)
[INFO ]20161116@23:32:10,326:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:59309 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:32:10,328:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:32:10,331:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:32:10,332:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:32:10,335:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:32:10,335:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:32:10,345:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:32:10,397:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:32:10,459:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:113) finished in 0.119 s
[INFO ]20161116@23:32:10,460:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:113, took 0.183280 s
[INFO ]20161116@23:32:10,466:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 124 ms on localhost (1/1)
[INFO ]20161116@23:32:10,466:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:32:10,488:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:114
[INFO ]20161116@23:32:10,495:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:114) with 1 output partitions
[INFO ]20161116@23:32:10,495:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:114)
[INFO ]20161116@23:32:10,495:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:32:10,495:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:32:10,497:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:32:10,503:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 320.8 KB)
[INFO ]20161116@23:32:10,555:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 322.7 KB)
[INFO ]20161116@23:32:10,558:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:59309 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:32:10,563:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:32:10,572:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:32:10,572:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:32:10,574:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:32:10,574:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:32:10,590:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:32:10,654:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:32:10,718:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:114) finished in 0.137 s
[INFO ]20161116@23:32:10,719:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:114, took 0.227391 s
[INFO ]20161116@23:32:10,726:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 142 ms on localhost (1/1)
[INFO ]20161116@23:32:10,727:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:32:10,872:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:115
[INFO ]20161116@23:32:10,902:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:32:10,908:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:32:10,912:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:106)
[INFO ]20161116@23:32:10,914:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:115) with 1 output partitions
[INFO ]20161116@23:32:10,915:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:115)
[INFO ]20161116@23:32:10,917:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:32:10,917:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:32:10,924:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:32:10,965:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 326.6 KB)
[INFO ]20161116@23:32:11,007:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 328.9 KB)
[INFO ]20161116@23:32:11,017:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:59309 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:32:11,010:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:32:11,023:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:32:11,045:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:32:11,045:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:32:11,053:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:32:11,064:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 332.7 KB)
[INFO ]20161116@23:32:11,088:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:32:11,092:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:32:11,120:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 335.0 KB)
[INFO ]20161116@23:32:11,129:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:32:11,139:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:59309 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:32:11,150:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:32:11,151:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:32:11,151:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:32:11,188:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:59309 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:32:11,233:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:32:11,235:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:59309 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:32:11,798:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:32:11,803:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:32:11,807:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:32:11,831:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:32:11,901:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.850 s
[INFO ]20161116@23:32:11,905:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:32:11,912:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:32:11,911:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 812 ms on localhost (1/1)
[INFO ]20161116@23:32:11,921:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:32:11,921:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:32:11,922:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:32:11,960:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:32:11,992:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.840 s
[INFO ]20161116@23:32:11,992:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:32:11,993:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:32:11,993:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:32:11,993:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:32:11,994:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:106), which has no missing parents
[INFO ]20161116@23:32:11,997:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 190 ms on localhost (1/1)
[INFO ]20161116@23:32:11,997:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:32:12,040:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 328.9 KB)
[INFO ]20161116@23:32:12,078:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:59309 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:32:12,136:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 325.2 KB)
[INFO ]20161116@23:32:12,148:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:59309 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:32:12,150:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:32:12,150:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:106)
[INFO ]20161116@23:32:12,151:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:32:12,158:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:32:12,159:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:32:12,309:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:32:12,316:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 24 ms
[INFO ]20161116@23:32:12,380:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:32:12,381:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[ERROR]20161116@23:32:13,095:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.ArrayIndexOutOfBoundsException: 8
	at homework2.HW2_Part2$3.call(HW2_Part2.java:92)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161116@23:32:13,201:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.ArrayIndexOutOfBoundsException: 8
	at homework2.HW2_Part2$3.call(HW2_Part2.java:92)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161116@23:32:13,223:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 5.0 failed 1 times; aborting job
[INFO ]20161116@23:32:13,236:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:32:13,250:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 5
[INFO ]20161116@23:32:13,255:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:106) failed in 1.083 s
[INFO ]20161116@23:32:13,263:org.apache.spark.scheduler.DAGScheduler - Job 3 failed: take at HW2_Part2.java:115, took 2.385713 s
[INFO ]20161116@23:32:56,308:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:32:57,191:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:32:57,886:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:32:57,887:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:32:57,889:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:32:58,722:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39346.
[INFO ]20161116@23:32:59,737:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:32:59,896:Remoting - Starting remoting
[INFO ]20161116@23:33:00,517:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:33465]
[INFO ]20161116@23:33:00,526:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:33465]
[INFO ]20161116@23:33:00,563:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 33465.
[INFO ]20161116@23:33:00,673:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:33:00,745:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:33:00,786:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-52b807e7-443a-4833-b334-91daff75dba5
[INFO ]20161116@23:33:00,836:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:33:01,053:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161116@23:33:02,097:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161116@23:33:02,205:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO ]20161116@23:33:02,324:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
[INFO ]20161116@23:33:02,340:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4042
[INFO ]20161116@23:33:02,692:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:33:02,824:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35094.
[INFO ]20161116@23:33:02,826:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 35094
[INFO ]20161116@23:33:02,829:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:33:02,835:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:35094 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 35094)
[INFO ]20161116@23:33:02,839:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:33:05,351:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:33:06,169:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:33:06,180:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:35094 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:33:06,206:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:33:06,481:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:33:06,595:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:33:06,597:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:35094 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:33:06,617:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:33:12,055:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:33:12,274:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:33:12,319:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:33:12,320:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:33:12,321:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:33:12,329:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:33:12,351:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:33:12,423:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:33:12,484:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:33:12,491:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:35094 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:33:12,492:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:33:12,517:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:33:12,535:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:33:12,783:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:33:12,821:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:33:12,930:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:33:13,005:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:33:13,007:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:33:13,007:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:33:13,007:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:33:13,008:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:33:13,207:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:33:13,291:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 591 ms on localhost (1/1)
[INFO ]20161116@23:33:13,300:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.689 s
[INFO ]20161116@23:33:13,298:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:33:13,343:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 1.067726 s
[INFO ]20161116@23:33:13,567:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:33:13,865:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:113
[INFO ]20161116@23:33:13,868:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:113) with 1 output partitions
[INFO ]20161116@23:33:13,868:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:113)
[INFO ]20161116@23:33:13,869:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:33:13,870:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:33:13,872:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:33:13,880:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:33:13,912:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 317.4 KB)
[INFO ]20161116@23:33:13,915:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:35094 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:33:13,916:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:33:13,918:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:33:13,919:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:33:13,921:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:33:13,922:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:33:13,930:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:33:13,985:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:33:14,032:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:113) finished in 0.107 s
[INFO ]20161116@23:33:14,034:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:113, took 0.167675 s
[INFO ]20161116@23:33:14,040:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 111 ms on localhost (1/1)
[INFO ]20161116@23:33:14,040:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:33:14,062:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:114
[INFO ]20161116@23:33:14,066:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:114) with 1 output partitions
[INFO ]20161116@23:33:14,066:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:114)
[INFO ]20161116@23:33:14,066:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:33:14,067:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:33:14,069:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:33:14,075:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 320.8 KB)
[INFO ]20161116@23:33:14,117:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 322.7 KB)
[INFO ]20161116@23:33:14,121:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:35094 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:33:14,124:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:33:14,126:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:33:14,127:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:33:14,132:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:33:14,133:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:33:14,146:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:33:14,192:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:33:14,233:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:114) finished in 0.091 s
[INFO ]20161116@23:33:14,234:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:114, took 0.168851 s
[INFO ]20161116@23:33:14,239:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 100 ms on localhost (1/1)
[INFO ]20161116@23:33:14,246:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:33:14,336:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:115
[INFO ]20161116@23:33:14,361:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:33:14,366:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:33:14,372:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:106)
[INFO ]20161116@23:33:14,374:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:115) with 1 output partitions
[INFO ]20161116@23:33:14,374:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:115)
[INFO ]20161116@23:33:14,375:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:33:14,376:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:33:14,385:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:33:14,409:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 326.6 KB)
[INFO ]20161116@23:33:14,440:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:33:14,475:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 328.9 KB)
[INFO ]20161116@23:33:14,490:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:35094 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:33:14,492:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:33:14,505:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:33:14,512:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:33:14,513:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:33:14,551:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 332.7 KB)
[INFO ]20161116@23:33:14,569:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:33:14,582:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:33:14,601:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:35094 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:33:14,617:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@23:33:14,618:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:35094 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:33:14,628:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:33:14,628:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:33:14,629:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:33:14,641:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:33:14,643:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:35094 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:33:14,645:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:33:15,235:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:33:15,238:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:33:15,240:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:33:15,259:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:33:15,351:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.839 s
[INFO ]20161116@23:33:15,354:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:33:15,357:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:33:15,356:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 780 ms on localhost (1/1)
[INFO ]20161116@23:33:15,359:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:33:15,414:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:33:15,415:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:33:15,442:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:33:15,478:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.839 s
[INFO ]20161116@23:33:15,478:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:33:15,478:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:33:15,478:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:33:15,478:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:33:15,483:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:106), which has no missing parents
[INFO ]20161116@23:33:15,487:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 240 ms on localhost (1/1)
[INFO ]20161116@23:33:15,487:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:33:15,578:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 328.9 KB)
[INFO ]20161116@23:33:15,611:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 331.3 KB)
[INFO ]20161116@23:33:15,615:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:35094 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:33:15,618:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:33:15,621:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:106)
[INFO ]20161116@23:33:15,624:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:33:15,629:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:33:15,629:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:33:15,783:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:33:15,794:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 29 ms
[INFO ]20161116@23:33:15,877:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:33:15,879:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161116@23:37:25,144:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:37:26,157:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:37:26,760:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:37:26,761:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:37:26,763:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:37:27,438:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43106.
[INFO ]20161116@23:37:28,275:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:37:28,406:Remoting - Starting remoting
[INFO ]20161116@23:37:28,854:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:52346]
[INFO ]20161116@23:37:28,866:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:52346]
[INFO ]20161116@23:37:28,895:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 52346.
[INFO ]20161116@23:37:28,960:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:37:29,012:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:37:29,058:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3121e4aa-ef2b-4154-b8d1-e27a8526ce92
[INFO ]20161116@23:37:29,091:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:37:29,282:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:37:29,977:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:37:29,984:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:37:30,281:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:37:30,351:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36970.
[INFO ]20161116@23:37:30,353:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 36970
[INFO ]20161116@23:37:30,360:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:37:30,371:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:36970 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 36970)
[INFO ]20161116@23:37:30,382:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:37:32,450:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:37:33,231:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:37:33,239:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:36970 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:33,256:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:37:33,487:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:37:33,605:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:37:33,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:36970 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:33,613:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:37:34,317:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:37:34,545:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:37:34,590:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:37:34,593:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:37:34,594:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:37:34,598:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:37:34,631:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:37:34,697:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:37:34,749:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:37:34,756:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:36970 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:37:34,758:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:34,774:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:37:34,780:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:37:34,907:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:37:34,923:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:37:35,018:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:37:35,067:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:37:35,069:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:37:35,069:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:37:35,071:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:37:35,073:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:37:35,266:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:37:35,336:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 470 ms on localhost (1/1)
[INFO ]20161116@23:37:35,347:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.523 s
[INFO ]20161116@23:37:35,352:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:35,373:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 0.824318 s
[INFO ]20161116@23:37:35,504:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@23:37:35,566:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:36970 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:37:35,598:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:37:35,797:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:113
[INFO ]20161116@23:37:35,802:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:113) with 1 output partitions
[INFO ]20161116@23:37:35,803:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:113)
[INFO ]20161116@23:37:35,803:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:37:35,803:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:37:35,807:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:37:35,814:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:37:35,837:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:37:35,843:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:36970 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:37:35,847:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:35,848:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:37:35,848:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:37:35,852:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:37:35,853:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:37:35,862:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:37:35,894:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:37:35,930:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:113) finished in 0.073 s
[INFO ]20161116@23:37:35,932:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:113, took 0.133138 s
[INFO ]20161116@23:37:35,937:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 75 ms on localhost (1/1)
[INFO ]20161116@23:37:35,937:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:35,960:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:114
[INFO ]20161116@23:37:35,964:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:114) with 1 output partitions
[INFO ]20161116@23:37:35,964:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:114)
[INFO ]20161116@23:37:35,964:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:37:35,965:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:37:35,966:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:37:35,970:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:37:36,011:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@23:37:36,016:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:36970 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:37:36,017:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:36,019:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:37:36,020:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:37:36,022:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:37:36,023:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:37:36,033:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:37:36,062:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:37:36,090:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:114) finished in 0.063 s
[INFO ]20161116@23:37:36,092:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:114, took 0.129002 s
[INFO ]20161116@23:37:36,097:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 68 ms on localhost (1/1)
[INFO ]20161116@23:37:36,097:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:36,211:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:115
[INFO ]20161116@23:37:36,231:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:37:36,235:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:37:36,238:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:106)
[INFO ]20161116@23:37:36,239:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:115) with 1 output partitions
[INFO ]20161116@23:37:36,242:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:115)
[INFO ]20161116@23:37:36,242:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:37:36,244:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:37:36,256:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:37:36,277:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@23:37:36,294:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161116@23:37:36,298:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:36970 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:36,300:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:36,314:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:37:36,316:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:37:36,321:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:37:36,321:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:37:36,326:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:37:36,346:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@23:37:36,353:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:37:36,375:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@23:37:36,376:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:36970 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:36,385:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:36,386:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:37:36,386:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:37:36,471:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:37:36,474:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:36970 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:37:36,475:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:37:36,478:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:36970 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:37:36,841:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:37:36,846:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:37:36,848:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:37:36,860:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:37:36,900:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.574 s
[INFO ]20161116@23:37:36,903:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:37:36,904:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:37:36,905:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:37:36,905:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:37:36,906:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 578 ms on localhost (1/1)
[INFO ]20161116@23:37:36,910:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:36,951:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:37:36,966:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.579 s
[INFO ]20161116@23:37:36,966:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:37:36,966:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:37:36,966:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:37:36,966:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:37:36,968:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:106), which has no missing parents
[INFO ]20161116@23:37:36,970:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 120 ms on localhost (1/1)
[INFO ]20161116@23:37:36,971:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:37,001:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 323.6 KB)
[INFO ]20161116@23:37:37,014:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 326.0 KB)
[INFO ]20161116@23:37:37,019:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:36970 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:37,021:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:37,021:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:106)
[INFO ]20161116@23:37:37,022:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:37:37,026:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:37:37,026:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:37:37,118:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:37:37,123:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms
[INFO ]20161116@23:37:37,173:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:37:37,176:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161116@23:37:37,361:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:36970 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:37,365:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:36970 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:37,967:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 1303 bytes result sent to driver
[INFO ]20161116@23:37:37,981:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:106) finished in 0.950 s
[INFO ]20161116@23:37:37,981:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:37:37,981:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:37:37,982:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 6)
[INFO ]20161116@23:37:37,984:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:37:37,985:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[12] at sortBy at HW2_Part2.java:106), which has no missing parents
[INFO ]20161116@23:37:37,988:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 954 ms on localhost (1/1)
[INFO ]20161116@23:37:37,988:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:38,016:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.6 KB, free 317.4 KB)
[INFO ]20161116@23:37:38,030:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 319.4 KB)
[INFO ]20161116@23:37:38,032:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:36970 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:38,037:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:38,040:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[12] at sortBy at HW2_Part2.java:106)
[INFO ]20161116@23:37:38,040:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161116@23:37:38,057:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161116@23:37:38,058:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 6)
[INFO ]20161116@23:37:38,086:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:37:38,088:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161116@23:37:38,268:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 6). 1293 bytes result sent to driver
[INFO ]20161116@23:37:38,281:org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (take at HW2_Part2.java:115) finished in 0.224 s
[INFO ]20161116@23:37:38,282:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: take at HW2_Part2.java:115, took 2.069573 s
[INFO ]20161116@23:37:38,293:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 6) in 226 ms on localhost (1/1)
[INFO ]20161116@23:37:38,293:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:38,460:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@23:37:38,530:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@23:37:38,531:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@23:37:38,531:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161116@23:37:38,535:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:36970 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:38,536:org.apache.spark.ContextCleaner - Cleaned accumulator 7
[INFO ]20161116@23:37:38,538:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on localhost:36970 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:38,603:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:117
[INFO ]20161116@23:37:38,626:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@23:37:38,638:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161116@23:37:38,640:org.apache.spark.scheduler.DAGScheduler - Got job 4 (saveAsTextFile at HW2_Part2.java:117) with 1 output partitions
[INFO ]20161116@23:37:38,641:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (saveAsTextFile at HW2_Part2.java:117)
[INFO ]20161116@23:37:38,642:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7, ShuffleMapStage 8)
[INFO ]20161116@23:37:38,642:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:37:38,648:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[13] at saveAsTextFile at HW2_Part2.java:117), which has no missing parents
[INFO ]20161116@23:37:38,762:org.apache.spark.storage.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 71.2 KB, free 377.9 KB)
[INFO ]20161116@23:37:38,778:org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 24.8 KB, free 402.7 KB)
[INFO ]20161116@23:37:38,780:org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on localhost:36970 (size: 24.8 KB, free: 1027.3 MB)
[INFO ]20161116@23:37:38,782:org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:37:38,782:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at saveAsTextFile at HW2_Part2.java:117)
[INFO ]20161116@23:37:38,785:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks
[INFO ]20161116@23:37:38,787:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 7, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@23:37:38,788:org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 7)
[INFO ]20161116@23:37:38,884:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:37:38,886:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161116@23:37:38,897:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:37:38,898:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@23:37:39,089:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@23:37:39,210:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611162337_0009_m_000000_7' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479368250912/_temporary/0/task_201611162337_0009_m_000000
[INFO ]20161116@23:37:39,215:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611162337_0009_m_000000_7: Committed
[INFO ]20161116@23:37:39,231:org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 7). 2038 bytes result sent to driver
[INFO ]20161116@23:37:39,239:org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (saveAsTextFile at HW2_Part2.java:117) finished in 0.444 s
[INFO ]20161116@23:37:39,242:org.apache.spark.scheduler.DAGScheduler - Job 4 finished: saveAsTextFile at HW2_Part2.java:117, took 0.632075 s
[INFO ]20161116@23:37:39,252:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 7) in 451 ms on localhost (1/1)
[INFO ]20161116@23:37:39,252:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:37:39,354:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@23:37:39,388:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@23:37:39,403:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@23:37:39,410:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@23:37:39,413:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@23:37:39,418:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@23:37:39,439:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@23:37:39,442:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@23:37:39,452:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@23:37:39,453:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@23:37:39,469:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-55dda80d-dcbf-4aff-b8ae-89b28c91bc31
[INFO ]20161116@23:41:46,333:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:41:47,196:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:41:47,897:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:41:47,898:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:41:47,900:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:41:48,612:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35530.
[INFO ]20161116@23:41:49,442:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:41:49,588:Remoting - Starting remoting
[INFO ]20161116@23:41:50,049:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:57714]
[INFO ]20161116@23:41:50,058:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:57714]
[INFO ]20161116@23:41:50,093:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 57714.
[INFO ]20161116@23:41:50,164:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:41:50,196:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:41:50,236:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-c0ac37dc-031e-43cf-b92a-f0b7bae10a89
[INFO ]20161116@23:41:50,258:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:41:50,446:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:41:51,208:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:41:51,218:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:41:51,521:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:41:51,587:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35978.
[INFO ]20161116@23:41:51,594:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 35978
[INFO ]20161116@23:41:51,599:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:41:51,608:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:35978 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 35978)
[INFO ]20161116@23:41:51,614:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:41:54,285:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:41:55,093:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:41:55,105:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:35978 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:41:55,127:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:41:55,399:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:41:55,474:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:41:55,482:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:35978 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:41:55,484:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:41:56,205:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:41:56,415:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:41:56,444:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:41:56,446:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:41:56,447:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:41:56,450:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:41:56,498:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:41:56,559:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:41:56,597:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:41:56,600:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:35978 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:41:56,602:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:41:56,610:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:41:56,614:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:41:56,775:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:41:56,830:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:41:56,965:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:41:56,998:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:41:56,999:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:41:57,001:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:41:57,001:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:41:57,001:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:41:57,172:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:41:57,277:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 587 ms on localhost (1/1)
[INFO ]20161116@23:41:57,295:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:41:57,303:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.656 s
[INFO ]20161116@23:41:57,325:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 0.909440 s
[INFO ]20161116@23:41:57,470:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@23:41:57,543:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:35978 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:41:57,607:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:41:58,040:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:114
[INFO ]20161116@23:41:58,050:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:114) with 1 output partitions
[INFO ]20161116@23:41:58,050:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:114)
[INFO ]20161116@23:41:58,051:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:41:58,051:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:41:58,052:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:41:58,057:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:41:58,120:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:41:58,122:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:35978 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:41:58,124:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:41:58,125:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:41:58,132:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:41:58,134:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:41:58,136:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:41:58,167:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:41:58,258:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:41:58,344:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:114) finished in 0.202 s
[INFO ]20161116@23:41:58,346:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:114, took 0.304513 s
[INFO ]20161116@23:41:58,351:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 200 ms on localhost (1/1)
[INFO ]20161116@23:41:58,351:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:41:58,388:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:115
[INFO ]20161116@23:41:58,398:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:115) with 1 output partitions
[INFO ]20161116@23:41:58,398:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:115)
[INFO ]20161116@23:41:58,398:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:41:58,399:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:41:58,401:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:41:58,413:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:41:58,471:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@23:41:58,472:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:35978 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:41:58,474:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:41:58,485:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:41:58,486:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:41:58,494:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:41:58,495:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:41:58,517:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:41:58,599:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:41:58,645:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:115) finished in 0.141 s
[INFO ]20161116@23:41:58,647:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:115, took 0.250585 s
[INFO ]20161116@23:41:58,652:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 151 ms on localhost (1/1)
[INFO ]20161116@23:41:58,652:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:41:58,832:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:116
[INFO ]20161116@23:41:58,873:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:41:58,877:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:41:58,894:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:41:58,895:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:116) with 1 output partitions
[INFO ]20161116@23:41:58,896:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:116)
[INFO ]20161116@23:41:58,896:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:41:58,905:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:41:58,921:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:41:58,974:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@23:41:59,009:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161116@23:41:59,019:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:35978 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:41:59,032:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:41:59,042:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:41:59,044:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:41:59,050:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:41:59,051:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:41:59,061:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:41:59,109:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@23:41:59,122:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:41:59,164:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@23:41:59,165:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:35978 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:41:59,166:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:41:59,176:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:41:59,177:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:41:59,374:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:41:59,378:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:35978 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:41:59,379:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:41:59,380:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:35978 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:42:00,224:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:42:00,229:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:42:00,230:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:42:00,247:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:42:00,305:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 1.245 s
[INFO ]20161116@23:42:00,307:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:42:00,308:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:42:00,309:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:42:00,309:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:42:00,313:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 1256 ms on localhost (1/1)
[INFO ]20161116@23:42:00,316:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:00,363:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:42:00,392:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 1.214 s
[INFO ]20161116@23:42:00,393:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:42:00,393:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:42:00,393:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:42:00,393:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:42:00,394:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107), which has no missing parents
[INFO ]20161116@23:42:00,398:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 160 ms on localhost (1/1)
[INFO ]20161116@23:42:00,398:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:00,454:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 323.6 KB)
[INFO ]20161116@23:42:00,479:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 326.0 KB)
[INFO ]20161116@23:42:00,484:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:35978 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:00,486:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:42:00,488:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:42:00,488:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:42:00,494:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:42:00,494:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:42:00,708:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:42:00,717:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 33 ms
[INFO ]20161116@23:42:00,893:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:42:00,893:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@23:42:01,313:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:35978 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:01,318:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:35978 in memory (size: 2.3 KB, free: 1027.3 MB)
[ERROR]20161116@23:42:02,289:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161116@23:42:02,383:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161116@23:42:02,401:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 5.0 failed 1 times; aborting job
[INFO ]20161116@23:42:02,418:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:02,424:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 5
[INFO ]20161116@23:42:02,437:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:107) failed in 1.927 s
[INFO ]20161116@23:42:02,441:org.apache.spark.scheduler.DAGScheduler - Job 3 failed: take at HW2_Part2.java:116, took 3.608257 s
[INFO ]20161116@23:42:02,466:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@23:42:02,556:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@23:42:02,603:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@23:42:02,635:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@23:42:02,643:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@23:42:02,652:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@23:42:02,666:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@23:42:02,688:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@23:42:02,690:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@23:42:02,698:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-2b253a41-5c77-4830-914d-06395e15df86
[INFO ]20161116@23:42:02,705:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@23:42:20,823:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:42:21,556:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:42:22,143:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:42:22,144:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:42:22,146:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:42:22,814:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 59015.
[INFO ]20161116@23:42:23,620:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:42:23,764:Remoting - Starting remoting
[INFO ]20161116@23:42:24,226:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:48067]
[INFO ]20161116@23:42:24,237:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:48067]
[INFO ]20161116@23:42:24,264:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 48067.
[INFO ]20161116@23:42:24,325:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:42:24,385:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:42:24,448:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ec418c9e-28b7-4b14-ac5a-b960cd06cbbd
[INFO ]20161116@23:42:24,497:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:42:24,681:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:42:25,522:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:42:25,530:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:42:25,822:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:42:25,887:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52633.
[INFO ]20161116@23:42:25,888:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 52633
[INFO ]20161116@23:42:25,892:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:42:25,901:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:52633 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 52633)
[INFO ]20161116@23:42:25,904:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:42:27,870:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:42:28,492:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:42:28,501:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:52633 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:28,521:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:42:28,726:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:42:28,783:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:42:28,788:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:52633 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:28,790:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:42:29,474:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:42:29,658:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:42:29,699:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:42:29,702:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:42:29,703:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:42:29,707:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:42:29,736:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:42:29,810:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:42:29,856:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:42:29,859:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:52633 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:42:29,861:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:42:29,871:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:42:29,875:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:42:30,014:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:42:30,046:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:42:30,133:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:42:30,180:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:42:30,182:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:42:30,182:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:42:30,183:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:42:30,189:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:42:30,351:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:42:30,417:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 455 ms on localhost (1/1)
[INFO ]20161116@23:42:30,421:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.512 s
[INFO ]20161116@23:42:30,419:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:30,447:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 0.786126 s
[INFO ]20161116@23:42:30,542:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@23:42:30,626:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:52633 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:42:30,693:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:42:30,906:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:114
[INFO ]20161116@23:42:30,912:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:114) with 1 output partitions
[INFO ]20161116@23:42:30,912:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:114)
[INFO ]20161116@23:42:30,912:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:42:30,913:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:42:30,914:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:42:30,917:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:42:30,942:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:42:30,944:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:52633 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:42:30,946:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:42:30,947:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:42:30,948:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:42:30,951:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:42:30,952:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:42:30,965:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:42:30,993:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:42:31,032:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:114) finished in 0.076 s
[INFO ]20161116@23:42:31,034:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:114, took 0.126240 s
[INFO ]20161116@23:42:31,038:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 78 ms on localhost (1/1)
[INFO ]20161116@23:42:31,039:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:31,056:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:115
[INFO ]20161116@23:42:31,065:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:115) with 1 output partitions
[INFO ]20161116@23:42:31,065:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:115)
[INFO ]20161116@23:42:31,066:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:42:31,066:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:42:31,067:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:42:31,070:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:42:31,089:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@23:42:31,091:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:52633 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:42:31,093:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:42:31,094:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:42:31,095:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:42:31,101:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:42:31,102:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:42:31,109:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:42:31,151:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:42:31,179:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:115) finished in 0.073 s
[INFO ]20161116@23:42:31,180:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:115, took 0.121237 s
[INFO ]20161116@23:42:31,193:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 78 ms on localhost (1/1)
[INFO ]20161116@23:42:31,194:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:31,313:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:116
[INFO ]20161116@23:42:31,336:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:42:31,341:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:42:31,346:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:42:31,348:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:116) with 1 output partitions
[INFO ]20161116@23:42:31,349:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:116)
[INFO ]20161116@23:42:31,349:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:42:31,352:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:42:31,357:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:42:31,395:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161116@23:42:31,406:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161116@23:42:31,411:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:52633 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:31,413:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:42:31,424:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:42:31,427:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:42:31,434:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:42:31,435:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:42:31,440:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:42:31,450:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161116@23:42:31,473:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:42:31,483:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:42:31,485:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:52633 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:42:31,486:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:42:31,488:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:52633 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:42:31,502:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@23:42:31,511:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:52633 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:31,512:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:42:31,513:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:42:31,514:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:42:31,969:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:42:31,974:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:42:31,976:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:42:31,992:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:42:32,036:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.597 s
[INFO ]20161116@23:42:32,039:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:42:32,040:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:42:32,040:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:42:32,041:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:42:32,042:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 601 ms on localhost (1/1)
[INFO ]20161116@23:42:32,042:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:32,093:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:42:32,108:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.586 s
[INFO ]20161116@23:42:32,108:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:42:32,109:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:42:32,109:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:42:32,109:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:42:32,110:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107), which has no missing parents
[INFO ]20161116@23:42:32,113:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 134 ms on localhost (1/1)
[INFO ]20161116@23:42:32,113:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:32,143:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 323.6 KB)
[INFO ]20161116@23:42:32,161:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:52633 in memory (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:32,166:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.9 KB)
[INFO ]20161116@23:42:32,168:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:52633 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:42:32,170:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:42:32,176:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:42:32,177:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:42:32,182:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:42:32,184:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:42:32,267:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:42:32,279:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 17 ms
[INFO ]20161116@23:42:32,327:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:42:32,330:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
[ERROR]20161116@23:42:33,123:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161116@23:42:33,187:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161116@23:42:33,203:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 5.0 failed 1 times; aborting job
[INFO ]20161116@23:42:33,219:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:42:33,227:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 5
[INFO ]20161116@23:42:33,232:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:107) failed in 1.041 s
[INFO ]20161116@23:42:33,238:org.apache.spark.scheduler.DAGScheduler - Job 3 failed: take at HW2_Part2.java:116, took 1.924363 s
[INFO ]20161116@23:42:33,270:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@23:42:33,353:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@23:42:33,381:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@23:42:33,410:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@23:42:33,413:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@23:42:33,419:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@23:42:33,426:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@23:42:33,442:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@23:42:33,458:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@23:42:33,450:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@23:42:33,470:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@23:42:33,477:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-9b015138-8df7-4933-931d-8f4506e557bd
[INFO ]20161116@23:44:05,604:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:44:06,470:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:44:07,146:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:44:07,147:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:44:07,149:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:44:07,876:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50086.
[INFO ]20161116@23:44:08,942:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:44:09,138:Remoting - Starting remoting
[INFO ]20161116@23:44:09,757:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:58288]
[INFO ]20161116@23:44:09,775:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:58288]
[INFO ]20161116@23:44:09,812:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 58288.
[INFO ]20161116@23:44:09,909:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:44:09,964:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:44:09,990:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f01ec7b7-202a-43d7-80e4-4252fb6c886b
[INFO ]20161116@23:44:10,027:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:44:10,294:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:44:11,180:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:44:11,189:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:44:11,515:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:44:11,578:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51729.
[INFO ]20161116@23:44:11,579:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 51729
[INFO ]20161116@23:44:11,588:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:44:11,595:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:51729 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 51729)
[INFO ]20161116@23:44:11,603:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:44:14,260:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:44:15,174:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:44:15,183:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:51729 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:44:15,206:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:44:15,473:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:44:15,635:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:44:15,645:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:51729 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:44:15,646:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:44:21,848:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:44:22,071:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:44:22,129:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:44:22,131:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:44:22,134:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:44:22,141:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:44:22,186:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:44:22,282:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:44:22,334:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:44:22,341:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:51729 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:44:22,342:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:44:22,354:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:44:22,366:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:44:22,539:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:44:22,581:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:44:22,686:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:44:22,751:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:44:22,752:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:44:22,754:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:44:22,755:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:44:22,755:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:44:23,005:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:44:23,121:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 626 ms on localhost (1/1)
[INFO ]20161116@23:44:23,134:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:44:23,141:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.716 s
[INFO ]20161116@23:44:23,180:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 1.108186 s
[INFO ]20161116@23:44:23,441:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:44:23,842:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:114
[INFO ]20161116@23:44:23,846:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:114) with 1 output partitions
[INFO ]20161116@23:44:23,847:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:114)
[INFO ]20161116@23:44:23,847:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:44:23,847:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:44:23,852:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:44:23,863:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:44:23,906:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 317.4 KB)
[INFO ]20161116@23:44:23,907:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:51729 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:44:23,909:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:44:23,911:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:44:23,913:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:44:23,919:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:44:23,923:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:44:23,942:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:44:24,021:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:44:24,089:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:114) finished in 0.159 s
[INFO ]20161116@23:44:24,090:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:114, took 0.246391 s
[INFO ]20161116@23:44:24,099:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 170 ms on localhost (1/1)
[INFO ]20161116@23:44:24,100:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:44:24,125:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:115
[INFO ]20161116@23:44:24,139:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:115) with 1 output partitions
[INFO ]20161116@23:44:24,139:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:115)
[INFO ]20161116@23:44:24,139:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:44:24,140:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:44:24,141:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:44:24,151:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 320.8 KB)
[INFO ]20161116@23:44:24,200:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2002.0 B, free 322.7 KB)
[INFO ]20161116@23:44:24,201:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:51729 (size: 2002.0 B, free: 1027.3 MB)
[INFO ]20161116@23:44:24,208:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:44:24,211:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:44:24,213:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:44:24,219:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:44:24,219:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:44:24,231:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:44:24,277:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:44:24,323:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:115) finished in 0.096 s
[INFO ]20161116@23:44:24,324:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:115, took 0.192052 s
[INFO ]20161116@23:44:24,328:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 104 ms on localhost (1/1)
[INFO ]20161116@23:44:24,329:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:44:24,476:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:116
[INFO ]20161116@23:44:24,508:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:44:24,512:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:44:24,517:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:44:24,522:org.apache.spark.scheduler.DAGScheduler - Got job 3 (take at HW2_Part2.java:116) with 1 output partitions
[INFO ]20161116@23:44:24,523:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:116)
[INFO ]20161116@23:44:24,523:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:44:24,525:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:44:24,536:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:44:24,558:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 326.6 KB)
[INFO ]20161116@23:44:24,615:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:44:24,645:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 328.9 KB)
[INFO ]20161116@23:44:24,653:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:51729 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:44:24,657:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:44:24,678:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:44:24,680:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:44:24,692:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:44:24,703:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 332.7 KB)
[INFO ]20161116@23:44:24,716:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:44:24,717:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:44:24,741:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:51729 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:44:24,763:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:44:24,781:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:44:24,789:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161116@23:44:24,790:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:51729 in memory (size: 2002.0 B, free: 1027.3 MB)
[INFO ]20161116@23:44:24,799:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:51729 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:44:24,800:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:44:24,800:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:44:24,801:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:44:25,342:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:44:25,346:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:44:25,349:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:44:25,368:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:44:25,449:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 757 ms on localhost (1/1)
[INFO ]20161116@23:44:25,449:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:44:25,492:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.808 s
[INFO ]20161116@23:44:25,495:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:44:25,501:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:44:25,508:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:44:25,512:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:44:25,517:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:44:25,546:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.737 s
[INFO ]20161116@23:44:25,546:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:44:25,546:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:44:25,546:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:44:25,546:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:44:25,547:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107), which has no missing parents
[INFO ]20161116@23:44:25,550:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 196 ms on localhost (1/1)
[INFO ]20161116@23:44:25,551:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:44:25,592:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 328.9 KB)
[INFO ]20161116@23:44:25,625:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 331.3 KB)
[INFO ]20161116@23:44:25,626:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:51729 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:44:25,627:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:44:25,628:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:44:25,628:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:44:25,632:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:44:25,635:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:44:25,751:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:44:25,764:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 32 ms
[INFO ]20161116@23:44:25,863:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:44:25,870:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
[ERROR]20161116@23:45:48,012:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161116@23:45:48,120:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161116@23:45:48,140:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 5.0 failed 1 times; aborting job
[INFO ]20161116@23:45:48,151:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:45:48,162:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 5
[INFO ]20161116@23:45:48,174:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:107) failed in 82.532 s
[INFO ]20161116@23:45:48,179:org.apache.spark.scheduler.DAGScheduler - Job 3 failed: take at HW2_Part2.java:116, took 83.702300 s
[INFO ]20161116@23:46:38,711:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@23:46:39,500:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@23:46:40,168:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@23:46:40,169:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@23:46:40,171:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@23:46:40,886:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49794.
[INFO ]20161116@23:46:41,755:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@23:46:41,875:Remoting - Starting remoting
[INFO ]20161116@23:46:42,451:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:49185]
[INFO ]20161116@23:46:42,471:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:49185]
[INFO ]20161116@23:46:42,525:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 49185.
[INFO ]20161116@23:46:42,625:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@23:46:42,717:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@23:46:42,779:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-7c522883-6386-406c-a49a-87240766ab92
[INFO ]20161116@23:46:42,856:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161116@23:46:43,088:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@23:46:44,421:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@23:46:44,435:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@23:46:44,715:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@23:46:44,755:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53250.
[INFO ]20161116@23:46:44,756:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 53250
[INFO ]20161116@23:46:44,760:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@23:46:44,765:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:53250 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 53250)
[INFO ]20161116@23:46:44,768:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@23:46:47,459:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@23:46:48,276:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@23:46:48,284:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:53250 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:46:48,304:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161116@23:46:48,532:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@23:46:48,657:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@23:46:48,662:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:53250 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161116@23:46:48,665:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161116@23:46:49,353:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:46:49,545:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:61
[INFO ]20161116@23:46:49,588:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:61) with 1 output partitions
[INFO ]20161116@23:46:49,590:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:61)
[INFO ]20161116@23:46:49,594:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:46:49,618:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:46:49,657:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:46:49,746:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:46:49,784:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:46:49,790:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:53250 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:46:49,792:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:46:49,804:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:46:49,811:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@23:46:49,955:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:46:49,984:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@23:46:50,075:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:46:50,119:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@23:46:50,121:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@23:46:50,121:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@23:46:50,122:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@23:46:50,123:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@23:46:50,309:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161116@23:46:50,379:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 465 ms on localhost (1/1)
[INFO ]20161116@23:46:50,387:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:61) finished in 0.530 s
[INFO ]20161116@23:46:50,386:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:46:50,416:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:61, took 0.870486 s
[INFO ]20161116@23:46:50,543:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161116@23:46:50,628:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:53250 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:46:50,699:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@23:46:50,973:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:114
[INFO ]20161116@23:46:50,979:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:114) with 1 output partitions
[INFO ]20161116@23:46:50,979:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:114)
[INFO ]20161116@23:46:50,979:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:46:50,980:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:46:50,981:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:46:50,985:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161116@23:46:51,017:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161116@23:46:51,019:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:53250 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:46:51,022:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:46:51,024:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:46:51,025:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@23:46:51,029:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161116@23:46:51,031:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@23:46:51,048:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:46:51,104:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3715 bytes result sent to driver
[INFO ]20161116@23:46:51,163:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:114) finished in 0.125 s
[INFO ]20161116@23:46:51,165:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:114, took 0.191809 s
[INFO ]20161116@23:46:51,175:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 135 ms on localhost (1/1)
[INFO ]20161116@23:46:51,175:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:46:51,206:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:115
[INFO ]20161116@23:46:51,214:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:115) with 1 output partitions
[INFO ]20161116@23:46:51,214:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (take at HW2_Part2.java:115)
[INFO ]20161116@23:46:51,214:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161116@23:46:51,215:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@23:46:51,216:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:46:51,218:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161116@23:46:51,248:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161116@23:46:51,249:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:53250 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:46:51,250:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:46:51,265:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:46:51,266:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@23:46:51,274:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161116@23:46:51,275:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@23:46:51,288:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:46:51,334:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3125 bytes result sent to driver
[INFO ]20161116@23:46:51,384:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (take at HW2_Part2.java:115) finished in 0.104 s
[INFO ]20161116@23:46:51,386:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:115, took 0.172626 s
[INFO ]20161116@23:46:51,393:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 104 ms on localhost (1/1)
[INFO ]20161116@23:46:51,393:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:46:51,659:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161116@23:46:51,993:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161116@23:46:51,995:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:53250 in memory (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161116@23:46:51,996:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161116@23:46:52,000:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:53250 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161116@23:46:52,029:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:118
[INFO ]20161116@23:46:52,061:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:46:52,065:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:46:52,074:org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:46:52,078:org.apache.spark.scheduler.DAGScheduler - Got job 3 (saveAsTextFile at HW2_Part2.java:118) with 1 output partitions
[INFO ]20161116@23:46:52,078:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (saveAsTextFile at HW2_Part2.java:118)
[INFO ]20161116@23:46:52,079:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161116@23:46:52,079:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161116@23:46:52,091:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161116@23:46:52,133:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161116@23:46:52,165:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161116@23:46:52,172:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:53250 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:46:52,178:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:46:52,192:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161116@23:46:52,193:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161116@23:46:52,203:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161116@23:46:52,204:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161116@23:46:52,212:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161116@23:46:52,221:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161116@23:46:52,238:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@23:46:52,255:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@23:46:52,261:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:53250 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161116@23:46:52,262:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:46:52,263:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161116@23:46:52,267:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161116@23:46:52,856:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161116@23:46:52,862:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161116@23:46:52,864:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161116@23:46:52,883:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@23:46:52,925:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:62) finished in 0.714 s
[INFO ]20161116@23:46:52,926:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:46:52,927:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
[INFO ]20161116@23:46:52,936:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:46:52,927:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 729 ms on localhost (1/1)
[INFO ]20161116@23:46:52,936:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:46:52,951:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:46:52,995:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 2182 bytes result sent to driver
[INFO ]20161116@23:46:53,009:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at HW2_Part2.java:48) finished in 0.739 s
[INFO ]20161116@23:46:53,010:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@23:46:53,010:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@23:46:53,010:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 5, ResultStage 6)
[INFO ]20161116@23:46:53,010:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@23:46:53,016:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107), which has no missing parents
[INFO ]20161116@23:46:53,019:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 148 ms on localhost (1/1)
[INFO ]20161116@23:46:53,020:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:46:53,051:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.5 KB, free 323.6 KB)
[INFO ]20161116@23:46:53,066:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 326.0 KB)
[INFO ]20161116@23:46:53,069:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:53250 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161116@23:46:53,070:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@23:46:53,073:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[10] at sortBy at HW2_Part2.java:107)
[INFO ]20161116@23:46:53,073:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@23:46:53,077:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161116@23:46:53,077:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161116@23:46:53,156:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:46:53,169:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 20 ms
[INFO ]20161116@23:46:53,225:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@23:46:53,230:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
[ERROR]20161116@23:46:53,945:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161116@23:46:54,012:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:99)
	at homework2.HW2_Part2$3.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161116@23:46:54,035:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 5.0 failed 1 times; aborting job
[INFO ]20161116@23:46:54,045:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@23:46:54,050:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 5
[INFO ]20161116@23:46:54,056:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:107) failed in 0.973 s
[INFO ]20161116@23:46:54,081:org.apache.spark.scheduler.DAGScheduler - Job 3 failed: saveAsTextFile at HW2_Part2.java:118, took 2.048465 s
[INFO ]20161116@23:46:54,142:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@23:46:54,238:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@23:46:54,320:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@23:46:54,372:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@23:46:54,373:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@23:46:54,379:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@23:46:54,397:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@23:46:54,418:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@23:46:54,434:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@23:46:54,437:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-df9a3b33-c73c-489d-a63d-527231bd8ecd
[INFO ]20161116@23:46:54,454:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@09:50:48,627:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@09:50:49,238:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@09:50:49,670:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@09:50:49,671:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@09:50:49,672:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@09:50:50,179:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36307.
[INFO ]20161117@09:50:50,758:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@09:50:50,858:Remoting - Starting remoting
[INFO ]20161117@09:50:51,171:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:58881]
[INFO ]20161117@09:50:51,183:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:58881]
[INFO ]20161117@09:50:51,203:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 58881.
[INFO ]20161117@09:50:51,245:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@09:50:51,293:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@09:50:51,338:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-c2e5a7ab-b622-4779-8962-dbd5861882c1
[INFO ]20161117@09:50:51,370:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@09:50:51,509:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@09:50:52,138:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@09:50:52,141:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@09:50:52,374:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@09:50:52,421:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53359.
[INFO ]20161117@09:50:52,427:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 53359
[INFO ]20161117@09:50:52,436:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@09:50:52,440:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:53359 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 53359)
[INFO ]20161117@09:50:52,450:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@09:50:53,946:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@09:50:54,367:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@09:50:54,376:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:53359 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:50:54,389:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@09:50:54,554:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@09:50:54,609:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@09:50:54,612:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:53359 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:50:54,616:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@09:50:55,179:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:50:55,270:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:50:55,574:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:127
[INFO ]20161117@09:50:55,603:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:127) with 1 output partitions
[INFO ]20161117@09:50:55,605:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:127)
[INFO ]20161117@09:50:55,606:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:50:55,608:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:50:55,635:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161117@09:50:55,675:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161117@09:50:55,702:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161117@09:50:55,704:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:53359 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161117@09:50:55,705:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:50:55,713:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161117@09:50:55,719:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@09:50:55,834:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161117@09:50:55,861:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@09:50:55,948:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:50:55,966:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@09:50:55,967:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@09:50:55,969:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@09:50:55,969:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@09:50:55,969:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@09:50:56,094:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161117@09:50:56,149:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:127) finished in 0.400 s
[INFO ]20161117@09:50:56,145:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 360 ms on localhost (1/1)
[INFO ]20161117@09:50:56,152:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:50:56,169:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:127, took 0.593724 s
[INFO ]20161117@09:50:56,208:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:128
[INFO ]20161117@09:50:56,215:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:128) with 1 output partitions
[INFO ]20161117@09:50:56,215:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:128)
[INFO ]20161117@09:50:56,215:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:50:56,215:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:50:56,216:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:50:56,218:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161117@09:50:56,244:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161117@09:50:56,245:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:53359 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:50:56,254:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:50:56,255:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:50:56,255:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@09:50:56,262:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161117@09:50:56,263:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@09:50:56,273:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:50:56,296:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3125 bytes result sent to driver
[INFO ]20161117@09:50:56,323:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:128) finished in 0.057 s
[INFO ]20161117@09:50:56,324:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:128, took 0.110288 s
[INFO ]20161117@09:50:56,328:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 60 ms on localhost (1/1)
[INFO ]20161117@09:50:56,328:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:50:56,468:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@09:50:56,642:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:131
[INFO ]20161117@09:50:56,665:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:50:56,666:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:48)
[INFO ]20161117@09:50:56,670:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:50:56,672:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part2.java:131) with 1 output partitions
[INFO ]20161117@09:50:56,673:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (saveAsTextFile at HW2_Part2.java:131)
[INFO ]20161117@09:50:56,673:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[INFO ]20161117@09:50:56,674:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
[INFO ]20161117@09:50:56,680:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:50:56,704:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161117@09:50:56,719:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161117@09:50:56,721:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:53359 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@09:50:56,722:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:50:56,725:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:50:56,728:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@09:50:56,730:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@09:50:56,731:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@09:50:56,736:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161117@09:50:56,747:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.9 KB, free 327.4 KB)
[INFO ]20161117@09:50:56,778:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:50:56,788:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.3 KB, free 329.7 KB)
[INFO ]20161117@09:50:56,792:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161117@09:50:56,804:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:53359 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@09:50:56,806:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:50:56,806:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161117@09:50:56,807:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161117@09:50:56,844:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:53359 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:50:57,138:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161117@09:50:57,143:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@09:50:57,146:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161117@09:50:57,159:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:50:57,195:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at HW2_Part2.java:61) finished in 0.460 s
[INFO ]20161117@09:50:57,197:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:50:57,197:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 3)
[INFO ]20161117@09:50:57,198:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:50:57,199:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:50:57,200:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 454 ms on localhost (1/1)
[INFO ]20161117@09:50:57,200:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:50:57,234:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161117@09:50:57,255:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (mapToPair at HW2_Part2.java:48) finished in 0.448 s
[INFO ]20161117@09:50:57,256:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:50:57,256:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@09:50:57,256:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:50:57,256:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:50:57,256:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120), which has no missing parents
[INFO ]20161117@09:50:57,264:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 112 ms on localhost (1/1)
[INFO ]20161117@09:50:57,265:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:50:57,292:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 328.9 KB)
[INFO ]20161117@09:50:57,326:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.4 KB, free 331.3 KB)
[INFO ]20161117@09:50:57,330:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:53359 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:50:57,336:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:50:57,338:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:50:57,338:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161117@09:50:57,343:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@09:50:57,343:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161117@09:50:57,427:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:50:57,432:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161117@09:50:57,459:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:50:57,461:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[ERROR]20161117@09:50:58,010:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 4.0 (TID 4)
java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:112)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161117@09:50:58,076:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 4.0 (TID 4, localhost): java.lang.NumberFormatException: For input string: "52 week high"
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:112)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161117@09:50:58,090:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 4.0 failed 1 times; aborting job
[INFO ]20161117@09:50:58,102:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:50:58,104:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 4
[INFO ]20161117@09:50:58,112:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (sortBy at HW2_Part2.java:120) failed in 0.759 s
[INFO ]20161117@09:50:58,116:org.apache.spark.scheduler.DAGScheduler - Job 2 failed: saveAsTextFile at HW2_Part2.java:131, took 1.470865 s
[INFO ]20161117@09:50:58,132:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161117@09:50:58,206:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@09:50:58,232:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@09:50:58,249:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@09:50:58,251:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@09:50:58,256:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@09:50:58,262:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@09:50:58,270:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@09:50:58,280:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@09:50:58,283:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@09:50:58,285:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a0eec1eb-fab2-4afe-842b-05bf463137f1
[INFO ]20161117@09:50:58,289:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@09:51:05,231:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@09:51:05,770:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@09:51:06,173:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@09:51:06,174:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@09:51:06,175:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@09:51:06,655:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39901.
[INFO ]20161117@09:51:07,216:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@09:51:07,301:Remoting - Starting remoting
[INFO ]20161117@09:51:07,623:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:34902]
[INFO ]20161117@09:51:07,630:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:34902]
[INFO ]20161117@09:51:07,653:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 34902.
[INFO ]20161117@09:51:07,699:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@09:51:07,735:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@09:51:07,778:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-13fe95c3-86ab-4767-a05b-022220bf9626
[INFO ]20161117@09:51:07,815:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@09:51:07,964:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@09:51:08,572:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@09:51:08,575:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@09:51:08,805:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@09:51:08,855:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52164.
[INFO ]20161117@09:51:08,855:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 52164
[INFO ]20161117@09:51:08,857:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@09:51:08,861:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:52164 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 52164)
[INFO ]20161117@09:51:08,864:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@09:51:10,242:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@09:51:10,647:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@09:51:10,652:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:52164 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:51:10,666:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@09:51:10,775:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@09:51:10,819:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@09:51:10,832:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:52164 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:51:10,833:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@09:51:11,470:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:51:11,584:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:51:11,843:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:127
[INFO ]20161117@09:51:11,873:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:127) with 1 output partitions
[INFO ]20161117@09:51:11,874:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:127)
[INFO ]20161117@09:51:11,875:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:51:11,879:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:51:11,896:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161117@09:51:11,957:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161117@09:51:11,992:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161117@09:51:11,998:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:52164 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161117@09:51:12,000:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:51:12,007:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161117@09:51:12,014:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@09:51:12,131:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161117@09:51:12,149:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@09:51:12,199:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:51:12,224:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@09:51:12,225:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@09:51:12,226:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@09:51:12,227:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@09:51:12,227:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@09:51:12,346:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161117@09:51:12,392:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:127) finished in 0.355 s
[INFO ]20161117@09:51:12,401:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 309 ms on localhost (1/1)
[INFO ]20161117@09:51:12,414:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:51:12,426:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:127, took 0.580077 s
[INFO ]20161117@09:51:12,455:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:128
[INFO ]20161117@09:51:12,464:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:128) with 1 output partitions
[INFO ]20161117@09:51:12,464:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:128)
[INFO ]20161117@09:51:12,464:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:51:12,464:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:51:12,465:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:51:12,474:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161117@09:51:12,494:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161117@09:51:12,494:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:52164 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:51:12,497:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:51:12,500:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:51:12,502:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@09:51:12,506:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161117@09:51:12,506:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@09:51:12,510:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:51:12,534:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3125 bytes result sent to driver
[INFO ]20161117@09:51:12,551:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:128) finished in 0.041 s
[INFO ]20161117@09:51:12,553:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:128, took 0.090255 s
[INFO ]20161117@09:51:12,566:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 45 ms on localhost (1/1)
[INFO ]20161117@09:51:12,566:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:51:12,699:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@09:51:12,864:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:131
[INFO ]20161117@09:51:12,888:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:51:12,891:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:76)
[INFO ]20161117@09:51:12,895:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:51:12,896:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part2.java:131) with 1 output partitions
[INFO ]20161117@09:51:12,898:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (saveAsTextFile at HW2_Part2.java:131)
[INFO ]20161117@09:51:12,898:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[INFO ]20161117@09:51:12,899:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
[INFO ]20161117@09:51:12,904:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:51:12,917:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161117@09:51:12,929:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161117@09:51:12,931:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:52164 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@09:51:12,938:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:51:12,942:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:51:12,945:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@09:51:12,950:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@09:51:12,950:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@09:51:12,954:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[6] at filter at HW2_Part2.java:76), which has no missing parents
[INFO ]20161117@09:51:12,963:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:51:12,969:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.2 KB, free 327.7 KB)
[INFO ]20161117@09:51:12,994:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161117@09:51:13,001:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.4 KB, free 330.1 KB)
[INFO ]20161117@09:51:13,016:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:52164 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:51:13,018:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:51:13,018:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[6] at filter at HW2_Part2.java:76)
[INFO ]20161117@09:51:13,018:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161117@09:51:13,058:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:52164 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:51:13,301:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161117@09:51:13,308:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@09:51:13,312:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161117@09:51:13,325:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:51:13,354:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at HW2_Part2.java:61) finished in 0.400 s
[INFO ]20161117@09:51:13,356:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:51:13,357:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 3)
[INFO ]20161117@09:51:13,358:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:51:13,358:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:51:13,359:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 404 ms on localhost (1/1)
[INFO ]20161117@09:51:13,360:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:51:13,391:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161117@09:51:13,419:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (filter at HW2_Part2.java:76) finished in 0.400 s
[INFO ]20161117@09:51:13,419:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:51:13,419:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@09:51:13,419:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:51:13,419:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:51:13,420:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120), which has no missing parents
[INFO ]20161117@09:51:13,424:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 110 ms on localhost (1/1)
[INFO ]20161117@09:51:13,424:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:51:13,444:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 329.3 KB)
[INFO ]20161117@09:51:13,465:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.4 KB, free 331.7 KB)
[INFO ]20161117@09:51:13,475:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:52164 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:51:13,479:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:51:13,481:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:51:13,482:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161117@09:51:13,490:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@09:51:13,490:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161117@09:51:13,555:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:51:13,562:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
[INFO ]20161117@09:51:13,601:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:51:13,601:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[INFO ]20161117@09:51:14,120:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1303 bytes result sent to driver
[INFO ]20161117@09:51:14,129:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (sortBy at HW2_Part2.java:120) finished in 0.634 s
[INFO ]20161117@09:51:14,130:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:51:14,130:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@09:51:14,130:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161117@09:51:14,131:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:51:14,131:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at saveAsTextFile at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@09:51:14,132:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 640 ms on localhost (1/1)
[INFO ]20161117@09:51:14,132:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:51:14,217:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 71.4 KB, free 403.1 KB)
[INFO ]20161117@09:51:14,227:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.9 KB, free 428.0 KB)
[INFO ]20161117@09:51:14,228:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:52164 (size: 24.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:51:14,231:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:51:14,232:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at saveAsTextFile at HW2_Part2.java:131)
[INFO ]20161117@09:51:14,234:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161117@09:51:14,244:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@09:51:14,244:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161117@09:51:14,340:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:52164 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:51:14,351:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:51:14,354:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161117@09:51:14,433:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@09:51:14,505:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611170951_0005_m_000000_5' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479405069259/_temporary/0/task_201611170951_0005_m_000000
[INFO ]20161117@09:51:14,512:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611170951_0005_m_000000_5: Committed
[INFO ]20161117@09:51:14,525:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 2038 bytes result sent to driver
[INFO ]20161117@09:51:14,530:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (saveAsTextFile at HW2_Part2.java:131) finished in 0.286 s
[INFO ]20161117@09:51:14,532:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part2.java:131, took 1.665685 s
[INFO ]20161117@09:51:14,537:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 288 ms on localhost (1/1)
[INFO ]20161117@09:51:14,542:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:51:14,649:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@09:51:14,676:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@09:51:14,693:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@09:51:14,694:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@09:51:14,699:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@09:51:14,710:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@09:51:14,719:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@09:51:14,722:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@09:51:14,731:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@09:51:14,732:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@09:51:14,740:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-38834bff-b68e-4d01-b489-e8fe6b6a3c30
[INFO ]20161117@09:54:27,094:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@09:54:27,780:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@09:54:28,251:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@09:54:28,252:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@09:54:28,253:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@09:54:29,129:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35533.
[INFO ]20161117@09:54:29,878:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@09:54:30,068:Remoting - Starting remoting
[INFO ]20161117@09:54:30,687:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:52203]
[INFO ]20161117@09:54:30,691:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:52203]
[INFO ]20161117@09:54:30,731:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 52203.
[INFO ]20161117@09:54:30,822:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@09:54:30,886:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@09:54:30,964:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8b9ccd78-849d-486a-924b-c37f477455ab
[INFO ]20161117@09:54:31,046:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@09:54:31,258:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@09:54:32,063:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@09:54:32,069:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@09:54:32,322:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@09:54:32,424:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42640.
[INFO ]20161117@09:54:32,431:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 42640
[INFO ]20161117@09:54:32,444:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@09:54:32,447:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:42640 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 42640)
[INFO ]20161117@09:54:32,456:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@09:54:34,154:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@09:54:35,062:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@09:54:35,071:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:42640 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:54:35,092:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@09:54:35,382:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@09:54:35,505:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@09:54:35,511:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:42640 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:54:35,520:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@09:54:36,560:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:54:36,669:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:54:36,957:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:127
[INFO ]20161117@09:54:36,991:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:127) with 1 output partitions
[INFO ]20161117@09:54:36,995:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:127)
[INFO ]20161117@09:54:36,996:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:54:36,999:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:54:37,027:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161117@09:54:37,081:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161117@09:54:37,110:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161117@09:54:37,123:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:42640 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161117@09:54:37,123:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:54:37,141:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161117@09:54:37,145:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@09:54:37,297:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161117@09:54:37,314:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@09:54:37,389:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:54:37,404:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@09:54:37,404:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@09:54:37,405:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@09:54:37,406:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@09:54:37,407:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@09:54:37,539:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161117@09:54:37,582:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:127) finished in 0.388 s
[INFO ]20161117@09:54:37,585:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 346 ms on localhost (1/1)
[INFO ]20161117@09:54:37,596:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:54:37,607:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:127, took 0.649150 s
[INFO ]20161117@09:54:37,644:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:128
[INFO ]20161117@09:54:37,658:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:128) with 1 output partitions
[INFO ]20161117@09:54:37,658:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:128)
[INFO ]20161117@09:54:37,658:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:54:37,658:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:54:37,659:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:54:37,662:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161117@09:54:37,699:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161117@09:54:37,702:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:42640 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:54:37,703:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:54:37,703:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:54:37,703:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@09:54:37,713:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161117@09:54:37,713:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@09:54:37,723:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:54:37,764:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3125 bytes result sent to driver
[INFO ]20161117@09:54:37,803:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:128) finished in 0.082 s
[INFO ]20161117@09:54:37,804:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:128, took 0.148291 s
[INFO ]20161117@09:54:37,808:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 80 ms on localhost (1/1)
[INFO ]20161117@09:54:37,808:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:54:38,034:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@09:54:38,162:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:131
[INFO ]20161117@09:54:38,186:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:54:38,188:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:76)
[INFO ]20161117@09:54:38,192:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:54:38,196:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part2.java:131) with 1 output partitions
[INFO ]20161117@09:54:38,196:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (saveAsTextFile at HW2_Part2.java:131)
[INFO ]20161117@09:54:38,196:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[INFO ]20161117@09:54:38,197:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
[INFO ]20161117@09:54:38,201:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:54:38,226:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161117@09:54:38,251:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161117@09:54:38,252:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:42640 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@09:54:38,253:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:54:38,265:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:54:38,266:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@09:54:38,271:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@09:54:38,273:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@09:54:38,281:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[6] at filter at HW2_Part2.java:76), which has no missing parents
[INFO ]20161117@09:54:38,300:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:54:38,302:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.2 KB, free 327.7 KB)
[INFO ]20161117@09:54:38,347:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161117@09:54:38,364:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.4 KB, free 330.1 KB)
[INFO ]20161117@09:54:38,372:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:42640 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:54:38,373:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:54:38,373:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[6] at filter at HW2_Part2.java:76)
[INFO ]20161117@09:54:38,373:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161117@09:54:38,412:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:42640 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:54:39,037:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161117@09:54:39,052:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@09:54:39,054:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161117@09:54:39,078:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:54:39,109:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at HW2_Part2.java:61) finished in 0.829 s
[INFO ]20161117@09:54:39,110:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:54:39,111:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 3)
[INFO ]20161117@09:54:39,112:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:54:39,113:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:54:39,114:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 839 ms on localhost (1/1)
[INFO ]20161117@09:54:39,114:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:54:39,176:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161117@09:54:39,212:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (filter at HW2_Part2.java:76) finished in 0.838 s
[INFO ]20161117@09:54:39,212:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:54:39,212:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@09:54:39,212:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:54:39,212:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:54:39,213:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120), which has no missing parents
[INFO ]20161117@09:54:39,221:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 160 ms on localhost (1/1)
[INFO ]20161117@09:54:39,221:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:54:39,269:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 329.3 KB)
[INFO ]20161117@09:54:39,308:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.4 KB, free 331.7 KB)
[INFO ]20161117@09:54:39,312:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:42640 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:54:39,324:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:54:39,325:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:54:39,329:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161117@09:54:39,334:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@09:54:39,334:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161117@09:54:39,412:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:54:39,423:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 19 ms
[INFO ]20161117@09:54:39,482:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:54:39,483:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@09:54:40,097:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1303 bytes result sent to driver
[INFO ]20161117@09:54:40,104:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (sortBy at HW2_Part2.java:120) finished in 0.774 s
[INFO ]20161117@09:54:40,105:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:54:40,106:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@09:54:40,106:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161117@09:54:40,106:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:54:40,106:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at saveAsTextFile at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@09:54:40,108:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 773 ms on localhost (1/1)
[INFO ]20161117@09:54:40,113:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:54:40,211:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 71.4 KB, free 403.1 KB)
[INFO ]20161117@09:54:40,232:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.9 KB, free 428.0 KB)
[INFO ]20161117@09:54:40,234:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:42640 (size: 24.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:54:40,238:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:54:40,240:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at saveAsTextFile at HW2_Part2.java:131)
[INFO ]20161117@09:54:40,240:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161117@09:54:40,253:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@09:54:40,254:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161117@09:54:40,348:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:42640 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:54:40,363:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:54:40,368:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
[INFO ]20161117@09:54:40,455:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@09:54:40,541:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611170954_0005_m_000000_5' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479405272913/_temporary/0/task_201611170954_0005_m_000000
[INFO ]20161117@09:54:40,544:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611170954_0005_m_000000_5: Committed
[INFO ]20161117@09:54:40,568:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 2038 bytes result sent to driver
[INFO ]20161117@09:54:40,576:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (saveAsTextFile at HW2_Part2.java:131) finished in 0.322 s
[INFO ]20161117@09:54:40,577:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part2.java:131, took 2.414979 s
[INFO ]20161117@09:54:40,584:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 324 ms on localhost (1/1)
[INFO ]20161117@09:54:40,585:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:54:40,680:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@09:54:40,697:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@09:54:40,709:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@09:54:40,710:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@09:54:40,713:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@09:54:40,715:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@09:54:40,740:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@09:54:40,749:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@09:54:40,756:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@09:54:40,784:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@09:54:40,790:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-40ed9851-28fd-48ae-9d5d-99b7e4a2b74b
[INFO ]20161117@09:55:42,826:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@09:55:43,443:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@09:55:44,174:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@09:55:44,174:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@09:55:44,176:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@09:55:44,797:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57035.
[INFO ]20161117@09:55:45,581:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@09:55:45,680:Remoting - Starting remoting
[INFO ]20161117@09:55:46,053:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:43444]
[INFO ]20161117@09:55:46,068:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:43444]
[INFO ]20161117@09:55:46,091:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 43444.
[INFO ]20161117@09:55:46,139:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@09:55:46,186:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@09:55:46,231:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-05a795fe-446f-4502-a070-9ee58a9e3330
[INFO ]20161117@09:55:46,278:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@09:55:46,443:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@09:55:47,119:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@09:55:47,126:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@09:55:47,413:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@09:55:47,469:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60192.
[INFO ]20161117@09:55:47,469:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 60192
[INFO ]20161117@09:55:47,474:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@09:55:47,481:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:60192 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 60192)
[INFO ]20161117@09:55:47,486:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@09:55:49,519:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@09:55:50,015:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@09:55:50,022:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:60192 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:55:50,038:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@09:55:50,185:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@09:55:50,237:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@09:55:50,243:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:60192 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:55:50,251:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@09:55:50,855:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:55:50,955:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@09:55:51,221:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:127
[INFO ]20161117@09:55:51,254:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:127) with 1 output partitions
[INFO ]20161117@09:55:51,255:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (take at HW2_Part2.java:127)
[INFO ]20161117@09:55:51,255:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:55:51,258:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:55:51,272:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48), which has no missing parents
[INFO ]20161117@09:55:51,311:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 310.1 KB)
[INFO ]20161117@09:55:51,351:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2022.0 B, free 312.1 KB)
[INFO ]20161117@09:55:51,353:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:60192 (size: 2022.0 B, free: 1027.3 MB)
[INFO ]20161117@09:55:51,355:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:55:51,365:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:48)
[INFO ]20161117@09:55:51,370:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@09:55:51,492:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2198 bytes)
[INFO ]20161117@09:55:51,513:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@09:55:51,592:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:55:51,610:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@09:55:51,611:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@09:55:51,611:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@09:55:51,611:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@09:55:51,611:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@09:55:51,755:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 3715 bytes result sent to driver
[INFO ]20161117@09:55:51,810:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 371 ms on localhost (1/1)
[INFO ]20161117@09:55:51,817:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (take at HW2_Part2.java:127) finished in 0.418 s
[INFO ]20161117@09:55:51,821:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:55:51,832:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:127, took 0.610466 s
[INFO ]20161117@09:55:51,875:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:128
[INFO ]20161117@09:55:51,884:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:128) with 1 output partitions
[INFO ]20161117@09:55:51,884:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (take at HW2_Part2.java:128)
[INFO ]20161117@09:55:51,884:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161117@09:55:51,885:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@09:55:51,885:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:55:51,887:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.4 KB, free 315.5 KB)
[INFO ]20161117@09:55:51,918:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2000.0 B, free 317.4 KB)
[INFO ]20161117@09:55:51,919:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:60192 (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:55:51,926:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:55:51,926:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:55:51,927:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@09:55:51,933:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2186 bytes)
[INFO ]20161117@09:55:51,934:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@09:55:51,943:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:55:51,999:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 3125 bytes result sent to driver
[INFO ]20161117@09:55:52,029:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 93 ms on localhost (1/1)
[INFO ]20161117@09:55:52,031:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (take at HW2_Part2.java:128) finished in 0.087 s
[INFO ]20161117@09:55:52,031:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:128, took 0.153988 s
[INFO ]20161117@09:55:52,034:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:55:52,177:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@09:55:52,314:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:131
[INFO ]20161117@09:55:52,339:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:55:52,349:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:76)
[INFO ]20161117@09:55:52,354:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:55:52,358:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part2.java:131) with 1 output partitions
[INFO ]20161117@09:55:52,358:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (saveAsTextFile at HW2_Part2.java:131)
[INFO ]20161117@09:55:52,359:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[INFO ]20161117@09:55:52,359:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
[INFO ]20161117@09:55:52,367:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@09:55:52,393:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.9 KB, free 321.3 KB)
[INFO ]20161117@09:55:52,409:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.3 KB, free 323.5 KB)
[INFO ]20161117@09:55:52,412:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:60192 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@09:55:52,417:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:55:52,425:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@09:55:52,426:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@09:55:52,429:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@09:55:52,430:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@09:55:52,436:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[6] at filter at HW2_Part2.java:76), which has no missing parents
[INFO ]20161117@09:55:52,452:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.2 KB, free 327.7 KB)
[INFO ]20161117@09:55:52,453:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@09:55:52,499:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161117@09:55:52,508:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.4 KB, free 330.1 KB)
[INFO ]20161117@09:55:52,522:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:60192 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@09:55:52,526:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:55:52,526:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[6] at filter at HW2_Part2.java:76)
[INFO ]20161117@09:55:52,526:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161117@09:55:52,564:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:60192 in memory (size: 2000.0 B, free: 1027.3 MB)
[INFO ]20161117@09:55:53,004:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161117@09:55:53,011:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@09:55:53,013:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161117@09:55:53,027:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@09:55:53,080:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at HW2_Part2.java:61) finished in 0.644 s
[INFO ]20161117@09:55:53,082:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:55:53,083:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 3)
[INFO ]20161117@09:55:53,083:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:55:53,084:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:55:53,084:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 645 ms on localhost (1/1)
[INFO ]20161117@09:55:53,085:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:55:53,143:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2182 bytes result sent to driver
[INFO ]20161117@09:55:53,181:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (filter at HW2_Part2.java:76) finished in 0.653 s
[INFO ]20161117@09:55:53,181:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:55:53,181:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@09:55:53,181:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5, ShuffleMapStage 4)
[INFO ]20161117@09:55:53,181:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:55:53,182:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120), which has no missing parents
[INFO ]20161117@09:55:53,186:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 170 ms on localhost (1/1)
[INFO ]20161117@09:55:53,187:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:55:53,218:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.6 KB, free 329.4 KB)
[INFO ]20161117@09:55:53,243:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KB, free 331.8 KB)
[INFO ]20161117@09:55:53,260:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:60192 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@09:55:53,261:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:55:53,269:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:120)
[INFO ]20161117@09:55:53,269:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161117@09:55:53,271:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@09:55:53,271:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
[INFO ]20161117@09:55:53,363:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:55:53,371:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 27 ms
[INFO ]20161117@09:55:53,442:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:55:53,443:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@09:55:54,402:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 1303 bytes result sent to driver
[INFO ]20161117@09:55:54,407:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (sortBy at HW2_Part2.java:120) finished in 1.126 s
[INFO ]20161117@09:55:54,408:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@09:55:54,408:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@09:55:54,409:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
[INFO ]20161117@09:55:54,409:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@09:55:54,410:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at saveAsTextFile at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@09:55:54,414:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 1137 ms on localhost (1/1)
[INFO ]20161117@09:55:54,415:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:55:54,532:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 71.4 KB, free 403.3 KB)
[INFO ]20161117@09:55:54,556:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.9 KB, free 428.2 KB)
[INFO ]20161117@09:55:54,560:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:60192 (size: 24.9 KB, free: 1027.3 MB)
[INFO ]20161117@09:55:54,562:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@09:55:54,563:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at saveAsTextFile at HW2_Part2.java:131)
[INFO ]20161117@09:55:54,563:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161117@09:55:54,573:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@09:55:54,574:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
[INFO ]20161117@09:55:54,672:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:60192 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@09:55:54,701:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@09:55:54,702:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@09:55:54,817:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@09:55:54,917:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611170955_0005_m_000000_5' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479405348056/_temporary/0/task_201611170955_0005_m_000000
[INFO ]20161117@09:55:54,920:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611170955_0005_m_000000_5: Committed
[INFO ]20161117@09:55:54,951:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 2038 bytes result sent to driver
[INFO ]20161117@09:55:54,963:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (saveAsTextFile at HW2_Part2.java:131) finished in 0.381 s
[INFO ]20161117@09:55:54,965:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part2.java:131, took 2.650922 s
[INFO ]20161117@09:55:54,970:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 388 ms on localhost (1/1)
[INFO ]20161117@09:55:54,970:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161117@09:55:55,069:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@09:55:55,100:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@09:55:55,114:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@09:55:55,115:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@09:55:55,120:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@09:55:55,124:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@09:55:55,137:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@09:55:55,149:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@09:55:55,158:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@09:55:55,173:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@09:55:55,174:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-0579fe73-da2a-48d2-a700-78fdfc692247
[INFO ]20161117@10:15:51,826:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@10:15:52,428:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@10:15:52,885:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@10:15:52,886:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@10:15:52,887:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@10:15:53,519:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 52160.
[INFO ]20161117@10:15:54,380:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@10:15:54,486:Remoting - Starting remoting
[INFO ]20161117@10:15:54,935:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:41208]
[INFO ]20161117@10:15:54,943:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:41208]
[INFO ]20161117@10:15:54,968:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 41208.
[INFO ]20161117@10:15:55,032:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@10:15:55,074:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@10:15:55,119:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d4eef096-d132-4534-ba82-d430be490a92
[INFO ]20161117@10:15:55,150:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@10:15:55,292:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@10:15:56,003:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@10:15:56,009:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@10:15:56,243:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@10:15:56,290:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42059.
[INFO ]20161117@10:15:56,293:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 42059
[INFO ]20161117@10:15:56,294:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@10:15:56,303:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:42059 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 42059)
[INFO ]20161117@10:15:56,306:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@10:15:57,933:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@10:15:58,441:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@10:15:58,455:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:42059 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:15:58,468:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@10:15:58,596:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@10:15:58,649:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@10:15:58,657:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:42059 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:15:58,661:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@10:15:59,256:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:15:59,355:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:15:59,664:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:129
[INFO ]20161117@10:15:59,718:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:15:59,720:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:77)
[INFO ]20161117@10:15:59,726:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:121)
[INFO ]20161117@10:15:59,731:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:129) with 1 output partitions
[INFO ]20161117@10:15:59,734:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (take at HW2_Part2.java:129)
[INFO ]20161117@10:15:59,735:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161117@10:15:59,739:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161117@10:15:59,777:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@10:15:59,882:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161117@10:15:59,904:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161117@10:15:59,911:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:42059 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@10:15:59,912:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:15:59,920:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:15:59,928:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@10:15:59,963:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77), which has no missing parents
[INFO ]20161117@10:15:59,983:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.2 KB, free 317.1 KB)
[INFO ]20161117@10:16:00,029:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.5 KB)
[INFO ]20161117@10:16:00,088:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@10:16:00,104:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:42059 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@10:16:00,106:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:16:00,106:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77)
[INFO ]20161117@10:16:00,106:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@10:16:00,114:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@10:16:00,194:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@10:16:00,231:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@10:16:00,231:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@10:16:00,231:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@10:16:00,232:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@10:16:00,233:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@10:16:00,732:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@10:16:00,746:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@10:16:00,747:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@10:16:00,757:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@10:16:00,842:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 796 ms on localhost (1/1)
[INFO ]20161117@10:16:00,850:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:16:00,854:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:61) finished in 0.882 s
[INFO ]20161117@10:16:00,855:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:16:00,856:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@10:16:00,867:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:16:00,868:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:16:00,887:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@10:16:00,913:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (filter at HW2_Part2.java:77) finished in 0.806 s
[INFO ]20161117@10:16:00,913:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:16:00,914:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:16:00,914:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:16:00,914:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:16:00,916:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121), which has no missing parents
[INFO ]20161117@10:16:00,921:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 173 ms on localhost (1/1)
[INFO ]20161117@10:16:00,921:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:16:00,972:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.6 KB, free 324.0 KB)
[INFO ]20161117@10:16:00,985:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@10:16:00,986:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:42059 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:16:00,988:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:16:00,991:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121)
[INFO ]20161117@10:16:00,994:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@10:16:00,998:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:16:00,998:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@10:16:01,046:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:16:01,051:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 12 ms
[INFO ]20161117@10:16:01,086:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:16:01,089:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[ERROR]20161117@10:16:01,672:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:115)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161117@10:16:01,730:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:115)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161117@10:16:01,743:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ]20161117@10:16:01,754:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:16:01,759:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 2
[INFO ]20161117@10:16:01,767:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part2.java:121) failed in 0.757 s
[INFO ]20161117@10:16:01,770:org.apache.spark.scheduler.DAGScheduler - Job 0 failed: take at HW2_Part2.java:129, took 2.101568 s
[INFO ]20161117@10:16:01,791:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161117@10:16:01,869:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@10:16:01,899:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@10:16:01,915:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@10:16:01,916:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@10:16:01,932:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@10:16:01,947:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@10:16:01,952:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@10:16:01,972:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@10:16:01,975:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@10:16:01,976:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@10:16:01,981:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-58cc1c12-eedb-4f38-b3cc-95240c1f6366
[INFO ]20161117@10:18:06,160:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@10:18:06,967:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@10:18:07,489:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@10:18:07,490:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@10:18:07,492:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@10:18:08,077:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34035.
[INFO ]20161117@10:18:08,834:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@10:18:08,972:Remoting - Starting remoting
[INFO ]20161117@10:18:09,394:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:44485]
[INFO ]20161117@10:18:09,404:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:44485]
[INFO ]20161117@10:18:09,435:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 44485.
[INFO ]20161117@10:18:09,506:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@10:18:09,553:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@10:18:09,572:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-460db0f6-9d9f-4cac-a1e6-64761108ac59
[INFO ]20161117@10:18:09,601:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@10:18:09,754:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@10:18:10,404:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@10:18:10,412:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@10:18:10,736:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@10:18:10,798:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58379.
[INFO ]20161117@10:18:10,801:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58379
[INFO ]20161117@10:18:10,802:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@10:18:10,808:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:58379 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 58379)
[INFO ]20161117@10:18:10,811:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@10:18:12,684:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@10:18:13,270:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@10:18:13,273:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58379 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:18:13,290:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@10:18:13,499:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@10:18:13,581:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@10:18:13,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58379 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:18:13,592:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@10:18:21,006:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:18:21,143:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:18:21,567:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:129
[INFO ]20161117@10:18:21,606:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:18:21,608:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:77)
[INFO ]20161117@10:18:21,610:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:121)
[INFO ]20161117@10:18:21,615:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:129) with 1 output partitions
[INFO ]20161117@10:18:21,615:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (take at HW2_Part2.java:129)
[INFO ]20161117@10:18:21,626:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161117@10:18:21,633:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161117@10:18:21,676:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@10:18:21,800:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161117@10:18:21,872:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161117@10:18:21,874:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:58379 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@10:18:21,879:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:18:21,907:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:18:21,925:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@10:18:22,019:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77), which has no missing parents
[INFO ]20161117@10:18:22,041:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.2 KB, free 317.1 KB)
[INFO ]20161117@10:18:22,106:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.5 KB)
[INFO ]20161117@10:18:22,173:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@10:18:22,181:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:58379 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@10:18:22,182:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:18:22,183:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77)
[INFO ]20161117@10:18:22,183:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@10:18:22,199:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@10:18:22,273:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@10:18:22,314:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@10:18:22,315:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@10:18:22,315:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@10:18:22,316:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@10:18:22,318:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@10:18:22,861:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@10:18:22,896:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@10:18:22,898:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@10:18:22,909:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@10:18:23,030:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 878 ms on localhost (1/1)
[INFO ]20161117@10:18:23,037:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:61) finished in 1.025 s
[INFO ]20161117@10:18:23,048:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:18:23,055:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@10:18:23,049:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:18:23,056:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:18:23,060:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:18:23,070:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@10:18:23,120:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (filter at HW2_Part2.java:77) finished in 0.933 s
[INFO ]20161117@10:18:23,120:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:18:23,120:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:18:23,120:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:18:23,120:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:18:23,121:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121), which has no missing parents
[INFO ]20161117@10:18:23,127:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 229 ms on localhost (1/1)
[INFO ]20161117@10:18:23,127:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:18:23,176:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.6 KB, free 324.0 KB)
[INFO ]20161117@10:18:23,209:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@10:18:23,213:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:58379 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:18:23,214:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:18:23,215:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121)
[INFO ]20161117@10:18:23,215:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@10:18:23,218:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:18:23,218:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@10:18:23,272:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:18:23,276:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
[INFO ]20161117@10:18:23,319:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:18:23,319:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[ERROR]20161117@10:18:23,912:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:115)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[INFO ]20161117@10:18:23,938:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:58379 in memory (size: 2.4 KB, free: 1027.3 MB)
[WARN ]20161117@10:18:24,013:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:115)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161117@10:18:24,019:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ]20161117@10:18:24,029:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:18:24,033:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 2
[INFO ]20161117@10:18:24,043:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part2.java:121) failed in 0.815 s
[INFO ]20161117@10:18:24,049:org.apache.spark.scheduler.DAGScheduler - Job 0 failed: take at HW2_Part2.java:129, took 2.481915 s
[INFO ]20161117@10:19:35,605:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@10:19:36,154:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@10:19:36,651:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@10:19:36,652:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@10:19:36,653:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@10:19:37,203:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 55216.
[INFO ]20161117@10:19:37,829:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@10:19:37,944:Remoting - Starting remoting
[INFO ]20161117@10:19:38,253:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:44350]
[INFO ]20161117@10:19:38,257:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:44350]
[INFO ]20161117@10:19:38,284:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 44350.
[INFO ]20161117@10:19:38,329:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@10:19:38,370:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@10:19:38,408:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-cd913ec4-b3d5-49bf-838c-ee66113bd49c
[INFO ]20161117@10:19:38,450:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@10:19:38,616:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@10:19:39,235:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@10:19:39,241:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@10:19:39,465:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@10:19:39,512:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34615.
[INFO ]20161117@10:19:39,517:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 34615
[INFO ]20161117@10:19:39,519:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@10:19:39,526:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:34615 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 34615)
[INFO ]20161117@10:19:39,528:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@10:19:40,869:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@10:19:41,304:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@10:19:41,310:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:34615 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:19:41,324:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@10:19:41,460:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@10:19:41,522:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@10:19:41,525:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:34615 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:19:41,528:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@10:19:42,054:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:19:42,152:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:19:42,449:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:130
[INFO ]20161117@10:19:42,482:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:19:42,484:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:77)
[INFO ]20161117@10:19:42,487:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:122)
[INFO ]20161117@10:19:42,490:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:130) with 1 output partitions
[INFO ]20161117@10:19:42,491:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (take at HW2_Part2.java:130)
[INFO ]20161117@10:19:42,491:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161117@10:19:42,493:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161117@10:19:42,515:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@10:19:42,596:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161117@10:19:42,630:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161117@10:19:42,632:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:34615 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@10:19:42,633:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:19:42,646:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:19:42,648:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@10:19:42,685:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77), which has no missing parents
[INFO ]20161117@10:19:42,693:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.2 KB, free 317.1 KB)
[INFO ]20161117@10:19:42,740:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.5 KB)
[INFO ]20161117@10:19:42,797:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@10:19:42,817:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:34615 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@10:19:42,818:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:19:42,819:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77)
[INFO ]20161117@10:19:42,819:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@10:19:42,829:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@10:19:42,920:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@10:19:42,953:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@10:19:42,953:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@10:19:42,955:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@10:19:42,956:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@10:19:42,956:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@10:19:43,371:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@10:19:43,380:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@10:19:43,389:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@10:19:43,393:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@10:19:43,477:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 715 ms on localhost (1/1)
[INFO ]20161117@10:19:43,488:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:61) finished in 0.800 s
[INFO ]20161117@10:19:43,489:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:19:43,490:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@10:19:43,491:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:19:43,504:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@10:19:43,505:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:19:43,513:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:19:43,540:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (filter at HW2_Part2.java:77) finished in 0.719 s
[INFO ]20161117@10:19:43,540:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:19:43,540:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:19:43,540:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:19:43,540:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:19:43,541:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@10:19:43,543:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 162 ms on localhost (1/1)
[INFO ]20161117@10:19:43,543:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:19:43,588:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.6 KB, free 324.0 KB)
[INFO ]20161117@10:19:43,602:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@10:19:43,603:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:34615 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:19:43,606:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:19:43,609:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@10:19:43,611:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@10:19:43,615:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:19:43,616:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@10:19:43,694:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:19:43,699:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 23 ms
[INFO ]20161117@10:19:43,750:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:19:43,754:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[ERROR]20161117@10:19:44,251:org.apache.spark.executor.Executor - Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:116)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[WARN ]20161117@10:19:44,308:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.NumberFormatException: empty String
	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)
	at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
	at java.lang.Double.parseDouble(Double.java:538)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:116)
	at homework2.HW2_Part2$4.call(HW2_Part2.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

[ERROR]20161117@10:19:44,316:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 2.0 failed 1 times; aborting job
[INFO ]20161117@10:19:44,324:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:19:44,326:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 2
[INFO ]20161117@10:19:44,337:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part2.java:122) failed in 0.704 s
[INFO ]20161117@10:19:44,343:org.apache.spark.scheduler.DAGScheduler - Job 0 failed: take at HW2_Part2.java:130, took 1.893831 s
[INFO ]20161117@10:19:44,368:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161117@10:19:44,440:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@10:19:44,478:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@10:19:44,490:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@10:19:44,491:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@10:19:44,508:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@10:19:44,519:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@10:19:44,529:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@10:19:44,532:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@10:19:44,529:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@10:19:44,538:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-40de3be9-2e84-46c7-a665-084c876dc818
[INFO ]20161117@10:20:47,249:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@10:20:47,900:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@10:20:48,454:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@10:20:48,455:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@10:20:48,456:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@10:20:49,049:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 38352.
[INFO ]20161117@10:20:49,784:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@10:20:49,899:Remoting - Starting remoting
[INFO ]20161117@10:20:50,337:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:55731]
[INFO ]20161117@10:20:50,345:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:55731]
[INFO ]20161117@10:20:50,371:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55731.
[INFO ]20161117@10:20:50,450:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@10:20:50,502:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@10:20:50,538:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a732d414-984c-4048-b71b-4a1df2ad8f1f
[INFO ]20161117@10:20:50,578:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@10:20:50,737:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@10:20:51,362:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@10:20:51,366:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@10:20:51,650:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@10:20:51,693:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37253.
[INFO ]20161117@10:20:51,694:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 37253
[INFO ]20161117@10:20:51,697:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@10:20:51,700:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:37253 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 37253)
[INFO ]20161117@10:20:51,703:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@10:20:53,587:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@10:20:54,233:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@10:20:54,240:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:37253 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:20:54,263:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@10:20:54,454:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@10:20:54,544:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@10:20:54,550:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:37253 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:20:54,554:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@10:20:55,229:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:20:55,338:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:20:55,716:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:130
[INFO ]20161117@10:20:55,769:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:20:55,773:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:77)
[INFO ]20161117@10:20:55,777:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:122)
[INFO ]20161117@10:20:55,783:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:130) with 1 output partitions
[INFO ]20161117@10:20:55,785:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (take at HW2_Part2.java:130)
[INFO ]20161117@10:20:55,788:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161117@10:20:55,793:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161117@10:20:55,832:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@10:20:55,952:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161117@10:20:56,000:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161117@10:20:56,005:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:37253 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@10:20:56,006:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:20:56,016:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:20:56,024:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@10:20:56,063:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77), which has no missing parents
[INFO ]20161117@10:20:56,082:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.2 KB, free 317.1 KB)
[INFO ]20161117@10:20:56,115:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.5 KB)
[INFO ]20161117@10:20:56,183:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@10:20:56,199:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:37253 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@10:20:56,200:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:20:56,200:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77)
[INFO ]20161117@10:20:56,201:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@10:20:56,218:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@10:20:56,312:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@10:20:56,353:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@10:20:56,354:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@10:20:56,357:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@10:20:56,358:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@10:20:56,358:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@10:20:56,915:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@10:20:56,931:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@10:20:56,932:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@10:20:56,940:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@10:20:57,020:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 880 ms on localhost (1/1)
[INFO ]20161117@10:20:57,035:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:61) finished in 0.967 s
[INFO ]20161117@10:20:57,036:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:20:57,040:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:20:57,048:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@10:20:57,049:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:20:57,049:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:20:57,054:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@10:20:57,098:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (filter at HW2_Part2.java:77) finished in 0.897 s
[INFO ]20161117@10:20:57,098:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:20:57,098:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:20:57,098:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:20:57,098:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:20:57,099:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@10:20:57,105:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 172 ms on localhost (1/1)
[INFO ]20161117@10:20:57,106:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:20:57,157:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.6 KB, free 324.0 KB)
[INFO ]20161117@10:20:57,179:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@10:20:57,182:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:37253 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:20:57,188:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:20:57,193:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@10:20:57,198:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@10:20:57,201:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:20:57,201:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@10:20:57,267:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:20:57,280:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 22 ms
[INFO ]20161117@10:20:57,331:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:20:57,332:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@10:20:57,916:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:37253 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:08,480:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@10:22:09,028:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@10:22:09,681:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@10:22:09,682:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@10:22:09,684:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@10:22:10,258:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43907.
[INFO ]20161117@10:22:10,868:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@10:22:10,946:Remoting - Starting remoting
[INFO ]20161117@10:22:11,297:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:48478]
[INFO ]20161117@10:22:11,310:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:48478]
[INFO ]20161117@10:22:11,337:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 48478.
[INFO ]20161117@10:22:11,391:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@10:22:11,436:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@10:22:11,485:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e6dc3585-30fc-4bee-b133-28898656b111
[INFO ]20161117@10:22:11,525:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@10:22:11,676:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@10:22:12,297:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@10:22:12,304:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@10:22:12,538:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@10:22:12,583:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44742.
[INFO ]20161117@10:22:12,585:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 44742
[INFO ]20161117@10:22:12,590:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@10:22:12,599:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:44742 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 44742)
[INFO ]20161117@10:22:12,601:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@10:22:13,908:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@10:22:14,317:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@10:22:14,320:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:44742 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:14,336:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@10:22:14,465:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@10:22:14,528:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@10:22:14,530:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:44742 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:14,537:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@10:22:15,083:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:22:15,180:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:22:15,493:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:130
[INFO ]20161117@10:22:15,541:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:22:15,544:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:77)
[INFO ]20161117@10:22:15,549:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:122)
[INFO ]20161117@10:22:15,554:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:130) with 1 output partitions
[INFO ]20161117@10:22:15,557:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (take at HW2_Part2.java:130)
[INFO ]20161117@10:22:15,557:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161117@10:22:15,563:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161117@10:22:15,590:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@10:22:15,688:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161117@10:22:15,716:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161117@10:22:15,721:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:44742 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:15,722:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:22:15,730:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:22:15,734:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@10:22:15,768:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77), which has no missing parents
[INFO ]20161117@10:22:15,780:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.2 KB, free 317.1 KB)
[INFO ]20161117@10:22:15,829:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.5 KB)
[INFO ]20161117@10:22:15,886:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@10:22:15,901:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:44742 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:15,904:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:22:15,904:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77)
[INFO ]20161117@10:22:15,904:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@10:22:15,916:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@10:22:15,999:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@10:22:16,036:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@10:22:16,037:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@10:22:16,037:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@10:22:16,037:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@10:22:16,037:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@10:22:16,494:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@10:22:16,508:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@10:22:16,510:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@10:22:16,518:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@10:22:16,603:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 776 ms on localhost (1/1)
[INFO ]20161117@10:22:16,613:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:61) finished in 0.833 s
[INFO ]20161117@10:22:16,614:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:22:16,620:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@10:22:16,621:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:22:16,617:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:22:16,624:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:22:16,637:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@10:22:16,668:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (filter at HW2_Part2.java:77) finished in 0.762 s
[INFO ]20161117@10:22:16,668:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:22:16,668:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:22:16,668:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:22:16,668:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:22:16,669:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@10:22:16,671:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 164 ms on localhost (1/1)
[INFO ]20161117@10:22:16,672:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:22:16,710:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.6 KB, free 324.0 KB)
[INFO ]20161117@10:22:16,732:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@10:22:16,733:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:44742 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:16,734:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:22:16,740:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@10:22:16,742:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@10:22:16,746:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:22:16,746:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@10:22:16,796:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:22:16,801:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
[INFO ]20161117@10:22:16,843:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:22:16,844:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@10:22:17,465:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1303 bytes result sent to driver
[INFO ]20161117@10:22:17,484:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part2.java:122) finished in 0.733 s
[INFO ]20161117@10:22:17,488:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:22:17,489:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:22:17,489:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161117@10:22:17,489:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:22:17,489:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@10:22:17,493:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 739 ms on localhost (1/1)
[INFO ]20161117@10:22:17,498:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:22:17,520:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@10:22:17,546:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@10:22:17,551:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:44742 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:17,552:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:22:17,559:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@10:22:17,560:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161117@10:22:17,575:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@10:22:17,579:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161117@10:22:17,602:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:22:17,606:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@10:22:17,720:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 1402 bytes result sent to driver
[INFO ]20161117@10:22:17,733:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (take at HW2_Part2.java:130) finished in 0.162 s
[INFO ]20161117@10:22:17,737:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 164 ms on localhost (1/1)
[INFO ]20161117@10:22:17,738:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:22:17,749:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:130, took 2.255459 s
[INFO ]20161117@10:22:17,814:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:141
[INFO ]20161117@10:22:17,826:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@10:22:17,840:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@10:22:17,848:org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (sortBy at HW2_Part2.java:133)
[INFO ]20161117@10:22:17,849:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:141) with 1 output partitions
[INFO ]20161117@10:22:17,851:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (take at HW2_Part2.java:141)
[INFO ]20161117@10:22:17,852:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
[INFO ]20161117@10:22:17,853:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
[INFO ]20161117@10:22:17,855:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:133), which has no missing parents
[INFO ]20161117@10:22:17,873:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.6 KB, free 336.9 KB)
[INFO ]20161117@10:22:17,884:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KB, free 339.4 KB)
[INFO ]20161117@10:22:17,889:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:44742 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:17,890:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:22:17,891:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:133)
[INFO ]20161117@10:22:17,891:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161117@10:22:17,893:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:22:17,893:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161117@10:22:17,920:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:22:17,921:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@10:22:17,931:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:22:17,933:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161117@10:22:18,221:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1303 bytes result sent to driver
[INFO ]20161117@10:22:18,232:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (sortBy at HW2_Part2.java:133) finished in 0.330 s
[INFO ]20161117@10:22:18,232:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:22:18,232:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:22:18,232:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 7)
[INFO ]20161117@10:22:18,232:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:22:18,233:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:133), which has no missing parents
[INFO ]20161117@10:22:18,239:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 339 ms on localhost (1/1)
[INFO ]20161117@10:22:18,239:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:22:18,246:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 343.1 KB)
[INFO ]20161117@10:22:18,252:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:44742 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:18,258:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 338.1 KB)
[INFO ]20161117@10:22:18,259:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:44742 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:18,262:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:22:18,264:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:133)
[INFO ]20161117@10:22:18,265:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
[INFO ]20161117@10:22:18,274:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@10:22:18,274:org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 5)
[INFO ]20161117@10:22:18,278:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:44742 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:18,291:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:22:18,293:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161117@10:22:18,342:org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 5). 1402 bytes result sent to driver
[INFO ]20161117@10:22:18,346:org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (take at HW2_Part2.java:141) finished in 0.063 s
[INFO ]20161117@10:22:18,347:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:141, took 0.528400 s
[INFO ]20161117@10:22:18,349:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 5) in 73 ms on localhost (1/1)
[INFO ]20161117@10:22:18,352:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:22:18,482:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@10:22:18,551:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:143
[INFO ]20161117@10:22:18,554:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@10:22:18,556:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@10:22:18,557:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@10:22:18,559:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part2.java:143) with 1 output partitions
[INFO ]20161117@10:22:18,561:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@10:22:18,562:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
[INFO ]20161117@10:22:18,562:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@10:22:18,566:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143), which has no missing parents
[INFO ]20161117@10:22:18,649:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 71.4 KB, free 403.7 KB)
[INFO ]20161117@10:22:18,669:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 25.0 KB, free 428.7 KB)
[INFO ]20161117@10:22:18,673:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:44742 (size: 25.0 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:18,674:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:22:18,675:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@10:22:18,676:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161117@10:22:18,681:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@10:22:18,682:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 6)
[INFO ]20161117@10:22:18,773:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:22:18,784:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161117@10:22:18,840:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@10:22:18,929:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161117@10:22:18,931:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:44742 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:22:18,967:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611171022_0011_m_000000_6' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479406933000/_temporary/0/task_201611171022_0011_m_000000
[INFO ]20161117@10:22:18,968:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611171022_0011_m_000000_6: Committed
[INFO ]20161117@10:22:18,984:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 6). 2038 bytes result sent to driver
[INFO ]20161117@10:22:18,989:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (saveAsTextFile at HW2_Part2.java:143) finished in 0.302 s
[INFO ]20161117@10:22:18,991:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part2.java:143, took 0.438159 s
[INFO ]20161117@10:22:18,995:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 6) in 307 ms on localhost (1/1)
[INFO ]20161117@10:22:18,996:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:22:19,087:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@10:22:19,119:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@10:22:19,132:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@10:22:19,133:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@10:22:19,137:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@10:22:19,147:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@10:22:19,157:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@10:22:19,166:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@10:22:19,175:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@10:22:19,180:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@10:22:19,180:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-dbedb4f0-83a9-4857-a598-1d7df40ee92d
[INFO ]20161117@10:23:08,602:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@10:23:09,195:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@10:23:09,622:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@10:23:09,624:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@10:23:09,625:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@10:23:10,097:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50319.
[INFO ]20161117@10:23:10,667:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@10:23:10,767:Remoting - Starting remoting
[INFO ]20161117@10:23:11,068:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:38749]
[INFO ]20161117@10:23:11,077:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:38749]
[INFO ]20161117@10:23:11,099:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 38749.
[INFO ]20161117@10:23:11,140:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@10:23:11,195:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@10:23:11,226:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-65d7d468-8778-4786-9dc8-9df88a5d4f69
[INFO ]20161117@10:23:11,258:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@10:23:11,397:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@10:23:12,015:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@10:23:12,019:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@10:23:12,234:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@10:23:12,283:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 48642.
[INFO ]20161117@10:23:12,284:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 48642
[INFO ]20161117@10:23:12,286:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@10:23:12,289:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:48642 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 48642)
[INFO ]20161117@10:23:12,291:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@10:23:13,638:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@10:23:14,053:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@10:23:14,057:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:48642 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:14,072:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@10:23:14,187:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@10:23:14,243:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@10:23:14,247:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:48642 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:14,248:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@10:23:14,778:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:23:14,872:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@10:23:15,178:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:129
[INFO ]20161117@10:23:15,227:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:23:15,230:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:77)
[INFO ]20161117@10:23:15,233:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:121)
[INFO ]20161117@10:23:15,239:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:129) with 1 output partitions
[INFO ]20161117@10:23:15,240:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (take at HW2_Part2.java:129)
[INFO ]20161117@10:23:15,242:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161117@10:23:15,246:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161117@10:23:15,276:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@10:23:15,383:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161117@10:23:15,410:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161117@10:23:15,413:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:48642 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:15,415:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:23:15,422:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@10:23:15,433:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@10:23:15,469:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77), which has no missing parents
[INFO ]20161117@10:23:15,492:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.2 KB, free 317.1 KB)
[INFO ]20161117@10:23:15,551:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.5 KB)
[INFO ]20161117@10:23:15,621:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@10:23:15,628:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:48642 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:15,629:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:23:15,630:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77)
[INFO ]20161117@10:23:15,630:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@10:23:15,645:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@10:23:15,752:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@10:23:15,801:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@10:23:15,802:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@10:23:15,802:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@10:23:15,805:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@10:23:15,805:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@10:23:16,238:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@10:23:16,248:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@10:23:16,250:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@10:23:16,265:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@10:23:16,332:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 771 ms on localhost (1/1)
[INFO ]20161117@10:23:16,337:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:23:16,346:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:61) finished in 0.865 s
[INFO ]20161117@10:23:16,347:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:23:16,348:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@10:23:16,348:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:23:16,349:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:23:16,370:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@10:23:16,389:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (filter at HW2_Part2.java:77) finished in 0.759 s
[INFO ]20161117@10:23:16,389:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:23:16,389:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:23:16,389:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@10:23:16,389:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:23:16,390:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121), which has no missing parents
[INFO ]20161117@10:23:16,398:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 145 ms on localhost (1/1)
[INFO ]20161117@10:23:16,398:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:23:16,436:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.6 KB, free 324.0 KB)
[INFO ]20161117@10:23:16,457:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@10:23:16,459:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:48642 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:16,461:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:23:16,464:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121)
[INFO ]20161117@10:23:16,467:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@10:23:16,470:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:23:16,471:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@10:23:16,531:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:23:16,537:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161117@10:23:16,572:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:23:16,577:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
[INFO ]20161117@10:23:17,086:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1303 bytes result sent to driver
[INFO ]20161117@10:23:17,100:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part2.java:121) finished in 0.624 s
[INFO ]20161117@10:23:17,101:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:23:17,101:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:23:17,102:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161117@10:23:17,102:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:23:17,102:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 629 ms on localhost (1/1)
[INFO ]20161117@10:23:17,102:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:23:17,104:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:121), which has no missing parents
[INFO ]20161117@10:23:17,126:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@10:23:17,142:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@10:23:17,147:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:48642 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:17,150:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:23:17,152:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:121)
[INFO ]20161117@10:23:17,153:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161117@10:23:17,168:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@10:23:17,170:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161117@10:23:17,195:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:23:17,198:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161117@10:23:17,274:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 1402 bytes result sent to driver
[INFO ]20161117@10:23:17,289:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (take at HW2_Part2.java:129) finished in 0.124 s
[INFO ]20161117@10:23:17,296:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 125 ms on localhost (1/1)
[INFO ]20161117@10:23:17,299:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:23:17,309:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:129, took 2.130593 s
[INFO ]20161117@10:23:17,379:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:140
[INFO ]20161117@10:23:17,389:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@10:23:17,401:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@10:23:17,412:org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (sortBy at HW2_Part2.java:132)
[INFO ]20161117@10:23:17,413:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:140) with 1 output partitions
[INFO ]20161117@10:23:17,414:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (take at HW2_Part2.java:140)
[INFO ]20161117@10:23:17,415:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
[INFO ]20161117@10:23:17,415:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
[INFO ]20161117@10:23:17,420:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:132), which has no missing parents
[INFO ]20161117@10:23:17,431:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.6 KB, free 336.9 KB)
[INFO ]20161117@10:23:17,449:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KB, free 339.4 KB)
[INFO ]20161117@10:23:17,456:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:48642 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:17,457:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:23:17,458:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:132)
[INFO ]20161117@10:23:17,461:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161117@10:23:17,462:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@10:23:17,463:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161117@10:23:17,481:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:23:17,482:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@10:23:17,488:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:23:17,489:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161117@10:23:17,638:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1303 bytes result sent to driver
[INFO ]20161117@10:23:17,647:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (sortBy at HW2_Part2.java:132) finished in 0.180 s
[INFO ]20161117@10:23:17,647:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@10:23:17,647:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@10:23:17,647:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 7)
[INFO ]20161117@10:23:17,647:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@10:23:17,648:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:132), which has no missing parents
[INFO ]20161117@10:23:17,653:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 185 ms on localhost (1/1)
[INFO ]20161117@10:23:17,658:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:23:17,662:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 343.1 KB)
[INFO ]20161117@10:23:17,683:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 345.2 KB)
[INFO ]20161117@10:23:17,684:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:48642 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:17,688:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:23:17,692:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:132)
[INFO ]20161117@10:23:17,692:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
[INFO ]20161117@10:23:17,700:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@10:23:17,704:org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 5)
[INFO ]20161117@10:23:17,717:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:48642 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:17,727:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:23:17,727:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[INFO ]20161117@10:23:17,733:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:48642 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:17,764:org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 5). 1402 bytes result sent to driver
[INFO ]20161117@10:23:17,769:org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (take at HW2_Part2.java:140) finished in 0.076 s
[INFO ]20161117@10:23:17,770:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:140, took 0.386454 s
[INFO ]20161117@10:23:17,772:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 5) in 74 ms on localhost (1/1)
[INFO ]20161117@10:23:17,772:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:23:17,897:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@10:23:17,972:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:142
[INFO ]20161117@10:23:17,975:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@10:23:17,976:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@10:23:17,978:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@10:23:17,982:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part2.java:142) with 1 output partitions
[INFO ]20161117@10:23:17,982:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (saveAsTextFile at HW2_Part2.java:142)
[INFO ]20161117@10:23:17,983:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
[INFO ]20161117@10:23:17,984:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@10:23:17,987:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:142), which has no missing parents
[INFO ]20161117@10:23:18,073:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 71.4 KB, free 403.7 KB)
[INFO ]20161117@10:23:18,085:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.9 KB, free 428.7 KB)
[INFO ]20161117@10:23:18,089:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:48642 (size: 24.9 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:18,090:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@10:23:18,090:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:142)
[INFO ]20161117@10:23:18,090:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161117@10:23:18,096:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@10:23:18,096:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 6)
[INFO ]20161117@10:23:18,164:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@10:23:18,166:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161117@10:23:18,200:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@10:23:18,250:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611171023_0011_m_000000_6' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479406992667/_temporary/0/task_201611171023_0011_m_000000
[INFO ]20161117@10:23:18,252:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611171023_0011_m_000000_6: Committed
[INFO ]20161117@10:23:18,267:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 6). 2038 bytes result sent to driver
[INFO ]20161117@10:23:18,276:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (saveAsTextFile at HW2_Part2.java:142) finished in 0.172 s
[INFO ]20161117@10:23:18,278:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part2.java:142, took 0.305617 s
[INFO ]20161117@10:23:18,297:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 6) in 179 ms on localhost (1/1)
[INFO ]20161117@10:23:18,298:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161117@10:23:18,307:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161117@10:23:18,308:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:48642 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:18,309:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161117@10:23:18,310:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:48642 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@10:23:18,395:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@10:23:18,410:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@10:23:18,423:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@10:23:18,424:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@10:23:18,426:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@10:23:18,428:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@10:23:18,447:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@10:23:18,452:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@10:23:18,462:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@10:23:18,471:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@10:23:18,482:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-ebe82d5a-2d29-4d6c-b56e-20aaa87c672c
[INFO ]20161117@18:26:04,445:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@18:26:05,495:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@18:26:06,198:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@18:26:06,199:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@18:26:06,201:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@18:26:07,035:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44234.
[INFO ]20161117@18:26:07,989:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@18:26:08,141:Remoting - Starting remoting
[INFO ]20161117@18:26:08,830:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:33966]
[INFO ]20161117@18:26:08,837:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:33966]
[INFO ]20161117@18:26:08,876:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 33966.
[INFO ]20161117@18:26:08,938:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@18:26:08,985:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@18:26:09,034:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-aa4d1cc6-5179-4d6b-88c2-ee5664048ebc
[INFO ]20161117@18:26:09,064:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@18:26:09,302:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@18:26:10,181:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@18:26:10,192:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@18:26:10,645:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@18:26:10,779:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47179.
[INFO ]20161117@18:26:10,785:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 47179
[INFO ]20161117@18:26:10,794:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@18:26:10,809:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:47179 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 47179)
[INFO ]20161117@18:26:10,823:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@18:26:13,146:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@18:26:14,013:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@18:26:14,023:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:47179 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:14,046:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@18:26:14,309:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@18:26:14,433:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@18:26:14,436:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:47179 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:14,446:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:46
[INFO ]20161117@18:26:15,305:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:26:15,451:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:26:15,969:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:129
[INFO ]20161117@18:26:16,079:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:61)
[INFO ]20161117@18:26:16,094:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:77)
[INFO ]20161117@18:26:16,098:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:121)
[INFO ]20161117@18:26:16,113:org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at HW2_Part2.java:129) with 1 output partitions
[INFO ]20161117@18:26:16,115:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (take at HW2_Part2.java:129)
[INFO ]20161117@18:26:16,117:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161117@18:26:16,126:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161117@18:26:16,179:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61), which has no missing parents
[INFO ]20161117@18:26:16,358:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161117@18:26:16,408:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161117@18:26:16,413:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:47179 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:16,414:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:26:16,429:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:61)
[INFO ]20161117@18:26:16,435:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@18:26:16,502:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77), which has no missing parents
[INFO ]20161117@18:26:16,525:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.2 KB, free 317.1 KB)
[INFO ]20161117@18:26:16,587:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.4 KB, free 319.5 KB)
[INFO ]20161117@18:26:16,664:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@18:26:16,689:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:47179 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:16,690:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:26:16,691:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at filter at HW2_Part2.java:77)
[INFO ]20161117@18:26:16,691:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@18:26:16,703:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@18:26:16,863:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@18:26:16,913:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@18:26:16,913:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@18:26:16,914:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@18:26:16,915:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@18:26:16,917:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@18:26:17,709:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@18:26:17,725:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@18:26:17,732:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@18:26:17,766:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@18:26:17,904:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1299 ms on localhost (1/1)
[INFO ]20161117@18:26:17,922:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:61) finished in 1.419 s
[INFO ]20161117@18:26:17,920:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:26:17,931:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:26:17,937:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@18:26:17,938:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@18:26:17,939:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:26:17,987:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@18:26:18,016:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (filter at HW2_Part2.java:77) finished in 1.324 s
[INFO ]20161117@18:26:18,017:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:26:18,017:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:26:18,017:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ShuffleMapStage 2, ResultStage 3)
[INFO ]20161117@18:26:18,017:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:26:18,021:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121), which has no missing parents
[INFO ]20161117@18:26:18,032:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 300 ms on localhost (1/1)
[INFO ]20161117@18:26:18,033:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:26:18,083:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.6 KB, free 324.0 KB)
[INFO ]20161117@18:26:18,115:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@18:26:18,117:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:47179 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:18,120:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:26:18,126:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:121)
[INFO ]20161117@18:26:18,130:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@18:26:18,142:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:26:18,143:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@18:26:18,254:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:26:18,267:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 31 ms
[INFO ]20161117@18:26:18,353:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:26:18,358:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
[INFO ]20161117@18:26:19,280:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1303 bytes result sent to driver
[INFO ]20161117@18:26:19,312:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part2.java:121) finished in 1.165 s
[INFO ]20161117@18:26:19,313:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:26:19,314:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:26:19,314:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161117@18:26:19,314:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:26:19,315:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:121), which has no missing parents
[INFO ]20161117@18:26:19,315:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 1177 ms on localhost (1/1)
[INFO ]20161117@18:26:19,317:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:26:19,368:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@18:26:19,394:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@18:26:19,401:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:47179 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:19,413:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:26:19,415:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:121)
[INFO ]20161117@18:26:19,419:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161117@18:26:19,442:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:26:19,443:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161117@18:26:19,481:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:26:19,488:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 8 ms
[INFO ]20161117@18:26:19,631:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 1402 bytes result sent to driver
[INFO ]20161117@18:26:19,649:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (take at HW2_Part2.java:129) finished in 0.214 s
[INFO ]20161117@18:26:19,653:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 216 ms on localhost (1/1)
[INFO ]20161117@18:26:19,658:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:26:19,676:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at HW2_Part2.java:129, took 3.706193 s
[INFO ]20161117@18:26:19,812:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:140
[INFO ]20161117@18:26:19,829:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@18:26:19,843:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:26:19,849:org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (sortBy at HW2_Part2.java:132)
[INFO ]20161117@18:26:19,850:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:140) with 1 output partitions
[INFO ]20161117@18:26:19,855:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (take at HW2_Part2.java:140)
[INFO ]20161117@18:26:19,855:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
[INFO ]20161117@18:26:19,856:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
[INFO ]20161117@18:26:19,859:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:132), which has no missing parents
[INFO ]20161117@18:26:19,878:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.6 KB, free 336.9 KB)
[INFO ]20161117@18:26:19,896:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KB, free 339.4 KB)
[INFO ]20161117@18:26:19,900:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:47179 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:19,901:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:26:19,902:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:132)
[INFO ]20161117@18:26:19,902:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161117@18:26:19,905:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:26:19,906:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161117@18:26:19,943:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:26:19,944:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161117@18:26:19,955:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:26:19,958:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161117@18:26:20,344:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1303 bytes result sent to driver
[INFO ]20161117@18:26:20,363:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (sortBy at HW2_Part2.java:132) finished in 0.450 s
[INFO ]20161117@18:26:20,363:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:26:20,363:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:26:20,363:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 7)
[INFO ]20161117@18:26:20,364:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:26:20,366:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:132), which has no missing parents
[INFO ]20161117@18:26:20,373:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 459 ms on localhost (1/1)
[INFO ]20161117@18:26:20,378:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:26:20,396:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:47179 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:20,401:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 340.6 KB)
[INFO ]20161117@18:26:20,429:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 338.1 KB)
[INFO ]20161117@18:26:20,430:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:47179 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:20,433:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:47179 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:20,435:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:26:20,436:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:132)
[INFO ]20161117@18:26:20,440:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
[INFO ]20161117@18:26:20,443:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:26:20,444:org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 5)
[INFO ]20161117@18:26:20,474:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:26:20,483:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms
[INFO ]20161117@18:26:20,567:org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 5). 1402 bytes result sent to driver
[INFO ]20161117@18:26:20,578:org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (take at HW2_Part2.java:140) finished in 0.129 s
[INFO ]20161117@18:26:20,580:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:140, took 0.766493 s
[INFO ]20161117@18:26:20,587:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 5) in 135 ms on localhost (1/1)
[INFO ]20161117@18:26:20,587:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:26:20,838:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:26:20,983:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:142
[INFO ]20161117@18:26:20,988:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@18:26:20,989:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:26:20,997:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:26:21,003:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part2.java:142) with 1 output partitions
[INFO ]20161117@18:26:21,004:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (saveAsTextFile at HW2_Part2.java:142)
[INFO ]20161117@18:26:21,004:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
[INFO ]20161117@18:26:21,008:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@18:26:21,014:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:142), which has no missing parents
[INFO ]20161117@18:26:21,078:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 71.4 KB, free 403.7 KB)
[INFO ]20161117@18:26:21,092:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.9 KB, free 428.7 KB)
[INFO ]20161117@18:26:21,103:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:47179 (size: 24.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:21,104:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:26:21,106:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:142)
[INFO ]20161117@18:26:21,109:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161117@18:26:21,113:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:26:21,113:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 6)
[INFO ]20161117@18:26:21,216:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:26:21,219:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161117@18:26:21,291:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:26:21,345:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161117@18:26:21,351:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:47179 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:26:21,448:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611171826_0011_m_000000_6' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479435971398/_temporary/0/task_201611171826_0011_m_000000
[INFO ]20161117@18:26:21,452:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611171826_0011_m_000000_6: Committed
[INFO ]20161117@18:26:21,471:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 6). 2038 bytes result sent to driver
[INFO ]20161117@18:26:21,493:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (saveAsTextFile at HW2_Part2.java:142) finished in 0.373 s
[INFO ]20161117@18:26:21,496:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part2.java:142, took 0.510121 s
[INFO ]20161117@18:26:21,501:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 6) in 380 ms on localhost (1/1)
[INFO ]20161117@18:26:21,501:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:26:21,636:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@18:26:21,695:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@18:26:21,725:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@18:26:21,727:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@18:26:21,732:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@18:26:21,761:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@18:26:21,786:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@18:26:21,815:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@18:26:21,821:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@18:26:21,831:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-61bc81e8-4552-46d5-bac6-28d6519f1677
[INFO ]20161117@18:31:22,738:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@18:31:24,074:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@18:31:24,931:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@18:31:24,932:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@18:31:24,934:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@18:31:25,700:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44605.
[INFO ]20161117@18:31:26,578:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@18:31:26,707:Remoting - Starting remoting
[INFO ]20161117@18:31:27,222:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:56841]
[INFO ]20161117@18:31:27,239:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:56841]
[INFO ]20161117@18:31:27,273:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 56841.
[INFO ]20161117@18:31:27,343:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@18:31:27,398:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@18:31:27,471:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ac64aeb1-9271-4472-a258-f80d8d2cde15
[INFO ]20161117@18:31:27,521:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@18:31:27,717:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@18:31:28,610:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@18:31:28,617:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@18:31:28,963:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@18:31:29,034:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44467.
[INFO ]20161117@18:31:29,036:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 44467
[INFO ]20161117@18:31:29,038:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@18:31:29,043:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:44467 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 44467)
[INFO ]20161117@18:31:29,049:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@18:31:31,291:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@18:31:31,870:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@18:31:31,879:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:44467 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:31,899:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@18:31:32,123:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@18:31:32,187:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@18:31:32,189:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:44467 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:32,191:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:47
[INFO ]20161117@18:31:32,963:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:31:33,109:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:31:33,373:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:139
[INFO ]20161117@18:31:33,422:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:78)
[INFO ]20161117@18:31:33,430:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161117@18:31:33,437:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:139) with 1 output partitions
[INFO ]20161117@18:31:33,442:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:139)
[INFO ]20161117@18:31:33,443:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161117@18:31:33,451:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161117@18:31:33,487:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78), which has no missing parents
[INFO ]20161117@18:31:33,691:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 310.9 KB)
[INFO ]20161117@18:31:33,741:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 313.3 KB)
[INFO ]20161117@18:31:33,756:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:44467 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:33,757:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:33,785:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78)
[INFO ]20161117@18:31:33,794:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@18:31:33,856:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161117@18:31:33,880:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 317.2 KB)
[INFO ]20161117@18:31:33,953:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.5 KB)
[INFO ]20161117@18:31:34,116:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@18:31:34,164:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:44467 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:34,165:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:34,166:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161117@18:31:34,166:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@18:31:34,190:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@18:31:34,474:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@18:31:34,515:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@18:31:34,519:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@18:31:34,519:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@18:31:34,519:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@18:31:34,520:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@18:31:35,063:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@18:31:35,092:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@18:31:35,102:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@18:31:35,127:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@18:31:35,272:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (filter at HW2_Part2.java:78) finished in 1.420 s
[INFO ]20161117@18:31:35,266:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1266 ms on localhost (1/1)
[INFO ]20161117@18:31:35,296:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:31:35,295:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:35,308:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@18:31:35,324:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161117@18:31:35,326:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:31:35,995:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@18:31:36,049:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:62) finished in 1.880 s
[INFO ]20161117@18:31:36,049:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:31:36,049:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:31:36,050:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161117@18:31:36,051:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:31:36,052:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94), which has no missing parents
[INFO ]20161117@18:31:36,054:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 960 ms on localhost (1/1)
[INFO ]20161117@18:31:36,054:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:36,117:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 323.1 KB)
[INFO ]20161117@18:31:36,156:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2042.0 B, free 325.1 KB)
[INFO ]20161117@18:31:36,160:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:44467 (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161117@18:31:36,162:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:36,178:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94)
[INFO ]20161117@18:31:36,181:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@18:31:36,196:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161117@18:31:36,200:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@18:31:36,272:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:36,284:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 24 ms
[INFO ]20161117@18:31:36,357:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:36,361:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@18:31:37,234:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1124 bytes result sent to driver
[INFO ]20161117@18:31:37,264:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:139) finished in 1.076 s
[INFO ]20161117@18:31:37,270:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 1076 ms on localhost (1/1)
[INFO ]20161117@18:31:37,273:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:37,287:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:139, took 3.910864 s
[INFO ]20161117@18:31:37,410:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:140
[INFO ]20161117@18:31:37,446:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:31:37,473:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:31:37,480:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:31:37,481:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:140) with 1 output partitions
[INFO ]20161117@18:31:37,482:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:44467 in memory (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161117@18:31:37,484:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:140)
[INFO ]20161117@18:31:37,485:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161117@18:31:37,486:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161117@18:31:37,493:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@18:31:37,527:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.6 KB, free 324.1 KB)
[INFO ]20161117@18:31:37,546:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@18:31:37,547:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:44467 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:37,551:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:37,553:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:31:37,553:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161117@18:31:37,556:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:31:37,557:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161117@18:31:37,602:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:37,603:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@18:31:37,624:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:37,627:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@18:31:37,954:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 1303 bytes result sent to driver
[INFO ]20161117@18:31:37,976:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:122) finished in 0.414 s
[INFO ]20161117@18:31:37,977:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:31:37,977:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:31:37,977:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 6)
[INFO ]20161117@18:31:37,977:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:31:37,980:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@18:31:37,982:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 418 ms on localhost (1/1)
[INFO ]20161117@18:31:37,982:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:38,012:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@18:31:38,022:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@18:31:38,026:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:44467 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:38,029:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:38,030:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:31:38,031:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161117@18:31:38,045:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:31:38,048:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161117@18:31:38,082:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:38,084:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
[INFO ]20161117@18:31:38,215:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1402 bytes result sent to driver
[INFO ]20161117@18:31:38,223:org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (take at HW2_Part2.java:140) finished in 0.181 s
[INFO ]20161117@18:31:38,225:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:140, took 0.812378 s
[INFO ]20161117@18:31:38,233:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 183 ms on localhost (1/1)
[INFO ]20161117@18:31:38,234:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:38,274:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:141
[INFO ]20161117@18:31:38,279:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:31:38,286:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:31:38,294:org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:31:38,299:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:141) with 1 output partitions
[INFO ]20161117@18:31:38,300:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (take at HW2_Part2.java:141)
[INFO ]20161117@18:31:38,300:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
[INFO ]20161117@18:31:38,300:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
[INFO ]20161117@18:31:38,309:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@18:31:38,333:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.6 KB, free 336.9 KB)
[INFO ]20161117@18:31:38,354:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161117@18:31:38,357:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:44467 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:38,359:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161117@18:31:38,361:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:44467 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:38,365:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 326.5 KB)
[INFO ]20161117@18:31:38,375:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:44467 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:38,378:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:38,381:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:31:38,383:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks
[INFO ]20161117@18:31:38,387:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:31:38,387:org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 5)
[INFO ]20161117@18:31:38,422:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:38,422:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[INFO ]20161117@18:31:38,433:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:38,434:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@18:31:38,626:org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 5). 1303 bytes result sent to driver
[INFO ]20161117@18:31:38,640:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (sortBy at HW2_Part2.java:131) finished in 0.248 s
[INFO ]20161117@18:31:38,640:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:31:38,640:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:31:38,640:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
[INFO ]20161117@18:31:38,640:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:31:38,641:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@18:31:38,647:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 5) in 253 ms on localhost (1/1)
[INFO ]20161117@18:31:38,647:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:38,656:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@18:31:38,668:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@18:31:38,673:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:44467 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:38,674:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:38,674:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:31:38,675:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks
[INFO ]20161117@18:31:38,676:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:31:38,676:org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 6)
[INFO ]20161117@18:31:38,708:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:38,709:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161117@18:31:38,782:org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 6). 1402 bytes result sent to driver
[INFO ]20161117@18:31:38,793:org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (take at HW2_Part2.java:141) finished in 0.108 s
[INFO ]20161117@18:31:38,797:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:141, took 0.519901 s
[INFO ]20161117@18:31:38,803:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 6) in 117 ms on localhost (1/1)
[INFO ]20161117@18:31:38,803:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:38,992:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:31:39,040:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161117@18:31:39,042:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:44467 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:39,043:org.apache.spark.ContextCleaner - Cleaned accumulator 7
[INFO ]20161117@18:31:39,044:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on localhost:44467 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:39,129:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:143
[INFO ]20161117@18:31:39,133:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:31:39,137:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:31:39,140:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@18:31:39,142:org.apache.spark.scheduler.DAGScheduler - Got job 3 (saveAsTextFile at HW2_Part2.java:143) with 1 output partitions
[INFO ]20161117@18:31:39,142:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 14 (saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@18:31:39,142:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 13)
[INFO ]20161117@18:31:39,143:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@18:31:39,149:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143), which has no missing parents
[INFO ]20161117@18:31:39,192:org.apache.spark.storage.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 71.4 KB, free 390.9 KB)
[INFO ]20161117@18:31:39,217:org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.0 KB, free 415.8 KB)
[INFO ]20161117@18:31:39,225:org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on localhost:44467 (size: 25.0 KB, free: 1027.3 MB)
[INFO ]20161117@18:31:39,226:org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:31:39,226:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@18:31:39,231:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 1 tasks
[INFO ]20161117@18:31:39,236:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 7, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:31:39,236:org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 7)
[INFO ]20161117@18:31:39,367:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:31:39,371:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@18:31:39,418:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:31:39,514:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611171831_0014_m_000000_7' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2_2_1479436289741/_temporary/0/task_201611171831_0014_m_000000
[INFO ]20161117@18:31:39,525:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611171831_0014_m_000000_7: Committed
[INFO ]20161117@18:31:39,542:org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 7). 2038 bytes result sent to driver
[INFO ]20161117@18:31:39,551:org.apache.spark.scheduler.DAGScheduler - ResultStage 14 (saveAsTextFile at HW2_Part2.java:143) finished in 0.309 s
[INFO ]20161117@18:31:39,554:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: saveAsTextFile at HW2_Part2.java:143, took 0.421777 s
[INFO ]20161117@18:31:39,565:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 7) in 316 ms on localhost (1/1)
[INFO ]20161117@18:31:39,565:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:31:39,686:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@18:31:39,706:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@18:31:39,723:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@18:31:39,724:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@18:31:39,727:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@18:31:39,731:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@18:31:39,760:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@18:31:39,763:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@18:31:39,789:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@18:31:39,802:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@18:31:39,807:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-9fe3d550-9d15-4340-8b07-25f55318aa88
[INFO ]20161117@18:36:10,388:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@18:36:11,206:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@18:36:11,874:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@18:36:11,875:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@18:36:11,877:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@18:36:12,671:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 48135.
[INFO ]20161117@18:36:13,653:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@18:36:13,811:Remoting - Starting remoting
[INFO ]20161117@18:36:14,312:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:55411]
[INFO ]20161117@18:36:14,325:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:55411]
[INFO ]20161117@18:36:14,382:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55411.
[INFO ]20161117@18:36:14,498:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@18:36:14,603:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@18:36:14,709:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-a5edd7d4-2c54-498b-bd1a-d328ea305548
[INFO ]20161117@18:36:14,851:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@18:36:15,435:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@18:36:17,265:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@18:36:17,281:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@18:36:17,841:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@18:36:17,953:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35530.
[INFO ]20161117@18:36:17,959:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 35530
[INFO ]20161117@18:36:17,961:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@18:36:17,979:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:35530 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 35530)
[INFO ]20161117@18:36:17,992:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@18:36:20,406:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@18:36:21,317:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@18:36:21,330:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:35530 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:21,347:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@18:36:21,529:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@18:36:21,607:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@18:36:21,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:35530 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:21,615:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:47
[INFO ]20161117@18:36:22,376:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:36:22,516:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:36:22,852:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:139
[INFO ]20161117@18:36:22,899:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:78)
[INFO ]20161117@18:36:22,908:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161117@18:36:22,914:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:139) with 1 output partitions
[INFO ]20161117@18:36:22,915:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:139)
[INFO ]20161117@18:36:22,916:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161117@18:36:22,919:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161117@18:36:22,939:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78), which has no missing parents
[INFO ]20161117@18:36:23,132:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 310.9 KB)
[INFO ]20161117@18:36:23,165:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 313.3 KB)
[INFO ]20161117@18:36:23,170:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:35530 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:23,172:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:23,186:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78)
[INFO ]20161117@18:36:23,193:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@18:36:23,234:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161117@18:36:23,256:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 317.2 KB)
[INFO ]20161117@18:36:23,312:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.5 KB)
[INFO ]20161117@18:36:23,388:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@18:36:23,396:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:35530 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:23,400:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:23,401:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161117@18:36:23,401:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@18:36:23,416:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@18:36:23,537:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@18:36:23,588:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@18:36:23,589:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@18:36:23,590:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@18:36:23,590:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@18:36:23,591:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@18:36:24,042:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@18:36:24,061:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@18:36:24,062:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@18:36:24,076:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@18:36:24,163:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 823 ms on localhost (1/1)
[INFO ]20161117@18:36:24,177:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (filter at HW2_Part2.java:78) finished in 0.932 s
[INFO ]20161117@18:36:24,180:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:36:24,182:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:24,193:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@18:36:24,194:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161117@18:36:24,203:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:36:24,510:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@18:36:24,541:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:62) finished in 1.136 s
[INFO ]20161117@18:36:24,542:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:36:24,542:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:36:24,542:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161117@18:36:24,543:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:36:24,544:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94), which has no missing parents
[INFO ]20161117@18:36:24,546:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 489 ms on localhost (1/1)
[INFO ]20161117@18:36:24,546:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:24,596:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 323.1 KB)
[INFO ]20161117@18:36:24,619:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2042.0 B, free 325.1 KB)
[INFO ]20161117@18:36:24,623:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:35530 (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161117@18:36:24,625:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:24,631:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94)
[INFO ]20161117@18:36:24,633:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@18:36:24,646:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161117@18:36:24,652:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@18:36:24,715:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:24,717:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
[INFO ]20161117@18:36:24,781:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:24,785:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@18:36:25,587:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1124 bytes result sent to driver
[INFO ]20161117@18:36:25,612:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:139) finished in 0.965 s
[INFO ]20161117@18:36:25,614:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 965 ms on localhost (1/1)
[INFO ]20161117@18:36:25,619:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:25,635:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:139, took 2.781703 s
[INFO ]20161117@18:36:25,769:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:140
[INFO ]20161117@18:36:25,786:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:36:25,805:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:36:25,806:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:36:25,808:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:140) with 1 output partitions
[INFO ]20161117@18:36:25,810:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:140)
[INFO ]20161117@18:36:25,810:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161117@18:36:25,810:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161117@18:36:25,815:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@18:36:25,857:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.6 KB, free 329.6 KB)
[INFO ]20161117@18:36:25,884:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.5 KB, free 332.1 KB)
[INFO ]20161117@18:36:25,891:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:35530 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:25,892:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:25,895:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:36:25,896:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161117@18:36:25,900:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:36:25,901:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161117@18:36:25,913:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:35530 in memory (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161117@18:36:25,965:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:25,967:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161117@18:36:25,981:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:25,990:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
[INFO ]20161117@18:36:26,290:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 1303 bytes result sent to driver
[INFO ]20161117@18:36:26,334:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:122) finished in 0.428 s
[INFO ]20161117@18:36:26,336:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:36:26,337:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:36:26,337:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 6)
[INFO ]20161117@18:36:26,339:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:36:26,340:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@18:36:26,343:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 432 ms on localhost (1/1)
[INFO ]20161117@18:36:26,343:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:26,385:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@18:36:26,410:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@18:36:26,413:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:35530 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:26,422:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:26,428:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:36:26,431:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161117@18:36:26,459:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:36:26,462:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161117@18:36:26,508:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:26,518:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
[INFO ]20161117@18:36:26,704:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1402 bytes result sent to driver
[INFO ]20161117@18:36:26,722:org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (take at HW2_Part2.java:140) finished in 0.275 s
[INFO ]20161117@18:36:26,726:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:140, took 0.953596 s
[INFO ]20161117@18:36:26,734:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 277 ms on localhost (1/1)
[INFO ]20161117@18:36:26,735:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:26,804:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:141
[INFO ]20161117@18:36:26,818:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:36:26,820:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:36:26,821:org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:36:26,827:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:141) with 1 output partitions
[INFO ]20161117@18:36:26,832:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (take at HW2_Part2.java:141)
[INFO ]20161117@18:36:26,833:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
[INFO ]20161117@18:36:26,834:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
[INFO ]20161117@18:36:26,841:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@18:36:26,869:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.6 KB, free 336.9 KB)
[INFO ]20161117@18:36:26,903:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161117@18:36:26,908:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:35530 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:26,917:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161117@18:36:26,920:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:35530 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:26,927:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 326.5 KB)
[INFO ]20161117@18:36:26,934:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:35530 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:26,939:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:26,941:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:36:26,942:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks
[INFO ]20161117@18:36:26,946:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:36:26,948:org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 5)
[INFO ]20161117@18:36:26,987:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:26,989:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161117@18:36:27,007:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:27,007:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@18:36:27,193:org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 5). 1303 bytes result sent to driver
[INFO ]20161117@18:36:27,204:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (sortBy at HW2_Part2.java:131) finished in 0.252 s
[INFO ]20161117@18:36:27,205:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:36:27,205:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:36:27,205:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
[INFO ]20161117@18:36:27,205:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:36:27,207:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@18:36:27,211:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 5) in 259 ms on localhost (1/1)
[INFO ]20161117@18:36:27,212:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:27,224:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@18:36:27,243:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@18:36:27,251:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:35530 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:27,252:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:27,254:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:36:27,255:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks
[INFO ]20161117@18:36:27,259:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:36:27,260:org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 6)
[INFO ]20161117@18:36:27,287:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:27,287:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161117@18:36:27,329:org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 6). 1402 bytes result sent to driver
[INFO ]20161117@18:36:27,336:org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (take at HW2_Part2.java:141) finished in 0.069 s
[INFO ]20161117@18:36:27,337:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:141, took 0.524877 s
[INFO ]20161117@18:36:27,349:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 6) in 77 ms on localhost (1/1)
[INFO ]20161117@18:36:27,350:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:27,532:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:36:27,572:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161117@18:36:27,574:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:35530 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:27,576:org.apache.spark.ContextCleaner - Cleaned accumulator 7
[INFO ]20161117@18:36:27,577:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on localhost:35530 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:27,668:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:143
[INFO ]20161117@18:36:27,675:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:36:27,679:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:36:27,680:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@18:36:27,689:org.apache.spark.scheduler.DAGScheduler - Got job 3 (saveAsTextFile at HW2_Part2.java:143) with 1 output partitions
[INFO ]20161117@18:36:27,689:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 14 (saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@18:36:27,689:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 13)
[INFO ]20161117@18:36:27,689:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@18:36:27,693:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143), which has no missing parents
[INFO ]20161117@18:36:27,842:org.apache.spark.storage.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 71.4 KB, free 390.9 KB)
[INFO ]20161117@18:36:27,854:org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.0 KB, free 415.9 KB)
[INFO ]20161117@18:36:27,859:org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on localhost:35530 (size: 25.0 KB, free: 1027.3 MB)
[INFO ]20161117@18:36:27,860:org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:36:27,860:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@18:36:27,861:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 1 tasks
[INFO ]20161117@18:36:27,863:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 7, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:36:27,863:org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 7)
[INFO ]20161117@18:36:27,962:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:36:27,966:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@18:36:28,024:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:36:28,129:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611171836_0014_m_000000_7' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2part2_1479436578745/_temporary/0/task_201611171836_0014_m_000000
[INFO ]20161117@18:36:28,135:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611171836_0014_m_000000_7: Committed
[INFO ]20161117@18:36:28,151:org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 7). 2038 bytes result sent to driver
[INFO ]20161117@18:36:28,164:org.apache.spark.scheduler.DAGScheduler - ResultStage 14 (saveAsTextFile at HW2_Part2.java:143) finished in 0.294 s
[INFO ]20161117@18:36:28,166:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: saveAsTextFile at HW2_Part2.java:143, took 0.493235 s
[INFO ]20161117@18:36:28,170:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 7) in 300 ms on localhost (1/1)
[INFO ]20161117@18:36:28,170:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:36:28,281:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@18:36:28,316:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@18:36:28,345:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@18:36:28,347:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@18:36:28,359:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@18:36:28,366:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@18:36:28,392:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@18:36:28,393:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@18:36:28,408:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@18:36:28,430:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@18:36:28,431:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3472c319-d187-4cb2-87c7-56cbd38a1504
[INFO ]20161117@18:37:24,427:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161117@18:37:25,285:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161117@18:37:25,939:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161117@18:37:25,940:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161117@18:37:25,942:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161117@18:37:26,622:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 41191.
[INFO ]20161117@18:37:27,582:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161117@18:37:27,776:Remoting - Starting remoting
[INFO ]20161117@18:37:28,377:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:40377]
[INFO ]20161117@18:37:28,388:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:40377]
[INFO ]20161117@18:37:28,418:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 40377.
[INFO ]20161117@18:37:28,479:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161117@18:37:28,542:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161117@18:37:28,641:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d79fa27f-56f8-48c7-9dff-9e3bc0756511
[INFO ]20161117@18:37:28,760:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161117@18:37:28,981:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161117@18:37:30,102:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161117@18:37:30,111:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161117@18:37:30,429:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161117@18:37:30,504:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50418.
[INFO ]20161117@18:37:30,505:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50418
[INFO ]20161117@18:37:30,508:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161117@18:37:30,514:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:50418 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 50418)
[INFO ]20161117@18:37:30,520:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161117@18:37:32,612:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161117@18:37:33,223:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161117@18:37:33,234:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50418 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:33,250:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161117@18:37:33,438:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161117@18:37:33,495:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161117@18:37:33,500:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50418 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:33,503:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:47
[INFO ]20161117@18:37:34,289:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:37:34,418:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161117@18:37:34,682:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:139
[INFO ]20161117@18:37:34,734:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:78)
[INFO ]20161117@18:37:34,736:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161117@18:37:34,752:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:139) with 1 output partitions
[INFO ]20161117@18:37:34,755:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:139)
[INFO ]20161117@18:37:34,759:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161117@18:37:34,764:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161117@18:37:34,819:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78), which has no missing parents
[INFO ]20161117@18:37:35,090:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 310.9 KB)
[INFO ]20161117@18:37:35,137:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 313.3 KB)
[INFO ]20161117@18:37:35,150:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:50418 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:35,152:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:35,171:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78)
[INFO ]20161117@18:37:35,179:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161117@18:37:35,270:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161117@18:37:35,288:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 317.2 KB)
[INFO ]20161117@18:37:35,390:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.5 KB)
[INFO ]20161117@18:37:35,488:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161117@18:37:35,502:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:50418 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:35,503:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:35,503:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161117@18:37:35,504:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161117@18:37:35,515:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161117@18:37:35,643:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161117@18:37:35,694:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161117@18:37:35,695:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161117@18:37:35,696:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161117@18:37:35,697:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161117@18:37:35,699:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161117@18:37:36,147:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161117@18:37:36,167:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161117@18:37:36,168:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161117@18:37:36,183:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161117@18:37:36,287:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (filter at HW2_Part2.java:78) finished in 1.018 s
[INFO ]20161117@18:37:36,279:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 869 ms on localhost (1/1)
[INFO ]20161117@18:37:36,304:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:37:36,305:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161117@18:37:36,300:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:36,332:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161117@18:37:36,333:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:37:36,735:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161117@18:37:36,782:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:62) finished in 1.278 s
[INFO ]20161117@18:37:36,783:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:37:36,783:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:37:36,784:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161117@18:37:36,784:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:37:36,785:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94), which has no missing parents
[INFO ]20161117@18:37:36,787:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 620 ms on localhost (1/1)
[INFO ]20161117@18:37:36,788:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:36,830:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 323.1 KB)
[INFO ]20161117@18:37:36,857:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2042.0 B, free 325.1 KB)
[INFO ]20161117@18:37:36,859:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:50418 (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161117@18:37:36,861:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:36,869:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94)
[INFO ]20161117@18:37:36,871:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161117@18:37:36,876:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161117@18:37:36,879:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161117@18:37:36,931:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:36,937:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms
[INFO ]20161117@18:37:36,998:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:37,004:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
[INFO ]20161117@18:37:37,847:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1124 bytes result sent to driver
[INFO ]20161117@18:37:37,875:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:139) finished in 0.998 s
[INFO ]20161117@18:37:37,877:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 995 ms on localhost (1/1)
[INFO ]20161117@18:37:37,883:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:37,912:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:139, took 3.229399 s
[INFO ]20161117@18:37:38,032:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:140
[INFO ]20161117@18:37:38,065:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:37:38,092:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:37:38,100:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:37:38,103:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:140) with 1 output partitions
[INFO ]20161117@18:37:38,110:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:140)
[INFO ]20161117@18:37:38,112:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161117@18:37:38,112:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161117@18:37:38,119:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@18:37:38,131:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:50418 in memory (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161117@18:37:38,151:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.6 KB, free 324.1 KB)
[INFO ]20161117@18:37:38,166:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.5 KB, free 326.5 KB)
[INFO ]20161117@18:37:38,168:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:50418 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:38,172:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:38,175:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:37:38,179:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161117@18:37:38,183:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:37:38,183:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161117@18:37:38,234:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:38,240:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 8 ms
[INFO ]20161117@18:37:38,254:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:38,260:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms
[INFO ]20161117@18:37:38,543:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 1303 bytes result sent to driver
[INFO ]20161117@18:37:38,562:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:122) finished in 0.369 s
[INFO ]20161117@18:37:38,563:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:37:38,563:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:37:38,563:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 6)
[INFO ]20161117@18:37:38,563:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:37:38,565:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161117@18:37:38,568:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 380 ms on localhost (1/1)
[INFO ]20161117@18:37:38,571:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:38,598:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@18:37:38,610:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@18:37:38,612:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:50418 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:38,614:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:38,617:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122)
[INFO ]20161117@18:37:38,622:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161117@18:37:38,636:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:37:38,638:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161117@18:37:38,672:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:38,676:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@18:37:38,821:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1402 bytes result sent to driver
[INFO ]20161117@18:37:38,837:org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (take at HW2_Part2.java:140) finished in 0.204 s
[INFO ]20161117@18:37:38,838:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:140, took 0.804752 s
[INFO ]20161117@18:37:38,842:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 205 ms on localhost (1/1)
[INFO ]20161117@18:37:38,843:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:38,884:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:141
[INFO ]20161117@18:37:38,893:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:37:38,894:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:37:38,895:org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:37:38,898:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:141) with 1 output partitions
[INFO ]20161117@18:37:38,900:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (take at HW2_Part2.java:141)
[INFO ]20161117@18:37:38,900:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
[INFO ]20161117@18:37:38,903:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
[INFO ]20161117@18:37:38,907:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@18:37:38,926:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.6 KB, free 336.9 KB)
[INFO ]20161117@18:37:38,953:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161117@18:37:38,956:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:50418 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:38,959:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161117@18:37:38,961:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:50418 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:38,964:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 326.5 KB)
[INFO ]20161117@18:37:38,969:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:50418 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:38,972:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:38,976:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:37:38,976:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks
[INFO ]20161117@18:37:38,978:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161117@18:37:38,979:org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 5)
[INFO ]20161117@18:37:39,013:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:39,013:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[INFO ]20161117@18:37:39,018:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:39,021:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161117@18:37:39,203:org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 5). 1303 bytes result sent to driver
[INFO ]20161117@18:37:39,219:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (sortBy at HW2_Part2.java:131) finished in 0.231 s
[INFO ]20161117@18:37:39,219:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161117@18:37:39,219:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161117@18:37:39,219:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
[INFO ]20161117@18:37:39,219:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161117@18:37:39,220:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161117@18:37:39,224:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 5) in 237 ms on localhost (1/1)
[INFO ]20161117@18:37:39,225:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:39,237:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161117@18:37:39,248:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161117@18:37:39,251:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:50418 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:39,252:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:39,254:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131)
[INFO ]20161117@18:37:39,260:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks
[INFO ]20161117@18:37:39,263:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:37:39,264:org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 6)
[INFO ]20161117@18:37:39,285:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:39,288:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161117@18:37:39,332:org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 6). 1402 bytes result sent to driver
[INFO ]20161117@18:37:39,346:org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (take at HW2_Part2.java:141) finished in 0.075 s
[INFO ]20161117@18:37:39,348:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:141, took 0.462243 s
[INFO ]20161117@18:37:39,358:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 6) in 83 ms on localhost (1/1)
[INFO ]20161117@18:37:39,358:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:39,522:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:37:39,561:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161117@18:37:39,563:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:50418 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:39,564:org.apache.spark.ContextCleaner - Cleaned accumulator 7
[INFO ]20161117@18:37:39,566:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on localhost:50418 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:39,661:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:143
[INFO ]20161117@18:37:39,670:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161117@18:37:39,673:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161117@18:37:39,676:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161117@18:37:39,680:org.apache.spark.scheduler.DAGScheduler - Got job 3 (saveAsTextFile at HW2_Part2.java:143) with 1 output partitions
[INFO ]20161117@18:37:39,680:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 14 (saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@18:37:39,680:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 13)
[INFO ]20161117@18:37:39,680:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161117@18:37:39,688:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143), which has no missing parents
[INFO ]20161117@18:37:39,745:org.apache.spark.storage.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 71.4 KB, free 390.9 KB)
[INFO ]20161117@18:37:39,765:org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.0 KB, free 415.9 KB)
[INFO ]20161117@18:37:39,775:org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on localhost:50418 (size: 25.0 KB, free: 1027.3 MB)
[INFO ]20161117@18:37:39,782:org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ]20161117@18:37:39,782:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161117@18:37:39,784:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 1 tasks
[INFO ]20161117@18:37:39,788:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 7, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161117@18:37:39,789:org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 7)
[INFO ]20161117@18:37:39,898:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161117@18:37:39,902:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161117@18:37:39,952:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161117@18:37:40,049:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611171837_0014_m_000000_7' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2part2_1479436651146/_temporary/0/task_201611171837_0014_m_000000
[INFO ]20161117@18:37:40,059:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611171837_0014_m_000000_7: Committed
[INFO ]20161117@18:37:40,074:org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 7). 2038 bytes result sent to driver
[INFO ]20161117@18:37:40,081:org.apache.spark.scheduler.DAGScheduler - ResultStage 14 (saveAsTextFile at HW2_Part2.java:143) finished in 0.286 s
[INFO ]20161117@18:37:40,083:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: saveAsTextFile at HW2_Part2.java:143, took 0.415814 s
[INFO ]20161117@18:37:40,088:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 7) in 291 ms on localhost (1/1)
[INFO ]20161117@18:37:40,088:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO ]20161117@18:37:40,188:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161117@18:37:40,229:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161117@18:37:40,256:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161117@18:37:40,258:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161117@18:37:40,264:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161117@18:37:40,270:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161117@18:37:40,290:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161117@18:37:40,303:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161117@18:37:40,313:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161117@18:37:40,313:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161117@18:37:40,316:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-db5b7d37-63bd-4872-8427-8325fa43bc52
[INFO ]20161126@20:18:18,915:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161126@20:18:19,478:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161126@20:18:19,952:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161126@20:18:19,953:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161126@20:18:19,954:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161126@20:18:20,426:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44168.
[INFO ]20161126@20:18:21,023:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161126@20:18:21,100:Remoting - Starting remoting
[INFO ]20161126@20:18:21,443:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:37920]
[INFO ]20161126@20:18:21,453:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:37920]
[INFO ]20161126@20:18:21,477:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 37920.
[INFO ]20161126@20:18:21,523:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161126@20:18:21,568:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161126@20:18:21,629:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-42254ddc-abd1-400e-a21b-3d7a3c814381
[INFO ]20161126@20:18:21,670:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 1027.3 MB
[INFO ]20161126@20:18:21,808:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161126@20:18:22,440:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161126@20:18:22,445:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161126@20:18:22,686:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161126@20:18:22,732:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47207.
[INFO ]20161126@20:18:22,734:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 47207
[INFO ]20161126@20:18:22,736:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161126@20:18:22,744:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:47207 with 1027.3 MB RAM, BlockManagerId(driver, localhost, 47207)
[INFO ]20161126@20:18:22,747:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161126@20:18:24,088:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161126@20:18:24,502:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161126@20:18:24,508:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:47207 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:24,520:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:44
[INFO ]20161126@20:18:24,674:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161126@20:18:24,712:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161126@20:18:24,714:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:47207 (size: 14.9 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:24,720:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:47
[INFO ]20161126@20:18:25,297:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161126@20:18:25,392:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161126@20:18:25,592:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:139
[INFO ]20161126@20:18:25,622:org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (filter at HW2_Part2.java:78)
[INFO ]20161126@20:18:25,624:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:62)
[INFO ]20161126@20:18:25,627:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:139) with 1 output partitions
[INFO ]20161126@20:18:25,628:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:139)
[INFO ]20161126@20:18:25,629:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161126@20:18:25,631:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161126@20:18:25,658:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78), which has no missing parents
[INFO ]20161126@20:18:25,810:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 310.9 KB)
[INFO ]20161126@20:18:25,838:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 313.3 KB)
[INFO ]20161126@20:18:25,839:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:47207 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:25,841:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:25,848:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at filter at HW2_Part2.java:78)
[INFO ]20161126@20:18:25,856:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161126@20:18:25,889:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62), which has no missing parents
[INFO ]20161126@20:18:25,903:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 317.2 KB)
[INFO ]20161126@20:18:25,942:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.5 KB)
[INFO ]20161126@20:18:25,998:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2187 bytes)
[INFO ]20161126@20:18:26,004:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:47207 (size: 2.3 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:26,005:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:26,006:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:62)
[INFO ]20161126@20:18:26,006:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161126@20:18:26,019:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161126@20:18:26,108:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161126@20:18:26,144:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161126@20:18:26,145:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161126@20:18:26,145:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161126@20:18:26,145:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161126@20:18:26,146:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161126@20:18:26,409:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161126@20:18:26,422:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2175 bytes)
[INFO ]20161126@20:18:26,423:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161126@20:18:26,428:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/Desktop/workspace/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161126@20:18:26,514:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (filter at HW2_Part2.java:78) finished in 0.624 s
[INFO ]20161126@20:18:26,520:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 551 ms on localhost (1/1)
[INFO ]20161126@20:18:26,532:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161126@20:18:26,533:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161126@20:18:26,527:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:26,539:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161126@20:18:26,540:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161126@20:18:26,790:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161126@20:18:26,822:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:62) finished in 0.815 s
[INFO ]20161126@20:18:26,822:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161126@20:18:26,822:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161126@20:18:26,822:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161126@20:18:26,822:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161126@20:18:26,824:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94), which has no missing parents
[INFO ]20161126@20:18:26,825:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 407 ms on localhost (1/1)
[INFO ]20161126@20:18:26,826:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:26,858:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 323.1 KB)
[INFO ]20161126@20:18:26,879:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2042.0 B, free 325.1 KB)
[INFO ]20161126@20:18:26,880:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:47207 (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161126@20:18:26,882:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:26,886:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at map at HW2_Part2.java:94)
[INFO ]20161126@20:18:26,889:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161126@20:18:26,894:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161126@20:18:26,899:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161126@20:18:26,933:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:26,935:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161126@20:18:26,987:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:26,991:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161126@20:18:27,616:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1124 bytes result sent to driver
[INFO ]20161126@20:18:27,633:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:139) finished in 0.733 s
[INFO ]20161126@20:18:27,638:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 740 ms on localhost (1/1)
[INFO ]20161126@20:18:27,639:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:27,652:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:139, took 2.059774 s
[INFO ]20161126@20:18:27,735:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:140
[INFO ]20161126@20:18:27,745:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161126@20:18:27,760:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161126@20:18:27,765:org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (sortBy at HW2_Part2.java:122)
[INFO ]20161126@20:18:27,765:org.apache.spark.scheduler.DAGScheduler - Got job 1 (take at HW2_Part2.java:140) with 1 output partitions
[INFO ]20161126@20:18:27,766:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (take at HW2_Part2.java:140)
[INFO ]20161126@20:18:27,766:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161126@20:18:27,766:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
[INFO ]20161126@20:18:27,770:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161126@20:18:27,786:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.6 KB, free 329.6 KB)
[INFO ]20161126@20:18:27,824:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.5 KB, free 332.1 KB)
[INFO ]20161126@20:18:27,838:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:47207 (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:27,845:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:27,846:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[11] at sortBy at HW2_Part2.java:122)
[INFO ]20161126@20:18:27,847:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161126@20:18:27,849:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161126@20:18:27,850:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161126@20:18:27,875:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:47207 in memory (size: 2042.0 B, free: 1027.3 MB)
[INFO ]20161126@20:18:27,909:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:27,910:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161126@20:18:27,921:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:27,924:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161126@20:18:28,166:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 1303 bytes result sent to driver
[INFO ]20161126@20:18:28,183:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (sortBy at HW2_Part2.java:122) finished in 0.335 s
[INFO ]20161126@20:18:28,183:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161126@20:18:28,183:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161126@20:18:28,183:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 6)
[INFO ]20161126@20:18:28,183:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161126@20:18:28,184:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122), which has no missing parents
[INFO ]20161126@20:18:28,188:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 332 ms on localhost (1/1)
[INFO ]20161126@20:18:28,188:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:28,211:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161126@20:18:28,225:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161126@20:18:28,226:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:47207 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:28,228:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:28,232:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at sortBy at HW2_Part2.java:122)
[INFO ]20161126@20:18:28,232:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161126@20:18:28,244:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161126@20:18:28,247:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161126@20:18:28,270:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:28,271:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161126@20:18:28,385:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1402 bytes result sent to driver
[INFO ]20161126@20:18:28,394:org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (take at HW2_Part2.java:140) finished in 0.150 s
[INFO ]20161126@20:18:28,395:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: take at HW2_Part2.java:140, took 0.655215 s
[INFO ]20161126@20:18:28,400:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 153 ms on localhost (1/1)
[INFO ]20161126@20:18:28,400:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:28,424:org.apache.spark.SparkContext - Starting job: take at HW2_Part2.java:141
[INFO ]20161126@20:18:28,429:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161126@20:18:28,431:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161126@20:18:28,431:org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (sortBy at HW2_Part2.java:131)
[INFO ]20161126@20:18:28,432:org.apache.spark.scheduler.DAGScheduler - Got job 2 (take at HW2_Part2.java:141) with 1 output partitions
[INFO ]20161126@20:18:28,434:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (take at HW2_Part2.java:141)
[INFO ]20161126@20:18:28,435:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
[INFO ]20161126@20:18:28,436:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 9)
[INFO ]20161126@20:18:28,439:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161126@20:18:28,451:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.6 KB, free 336.9 KB)
[INFO ]20161126@20:18:28,481:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 339.4 KB)
[INFO ]20161126@20:18:28,485:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:47207 (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:28,487:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161126@20:18:28,490:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:47207 in memory (size: 2.5 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:28,491:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161126@20:18:28,493:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:47207 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:28,495:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:28,495:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[14] at sortBy at HW2_Part2.java:131)
[INFO ]20161126@20:18:28,499:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 1 tasks
[INFO ]20161126@20:18:28,501:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1956 bytes)
[INFO ]20161126@20:18:28,501:org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 5)
[INFO ]20161126@20:18:28,517:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:28,524:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
[INFO ]20161126@20:18:28,532:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:28,535:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
[INFO ]20161126@20:18:28,696:org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 5). 1303 bytes result sent to driver
[INFO ]20161126@20:18:28,705:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 9 (sortBy at HW2_Part2.java:131) finished in 0.199 s
[INFO ]20161126@20:18:28,705:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161126@20:18:28,705:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161126@20:18:28,705:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 10)
[INFO ]20161126@20:18:28,705:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161126@20:18:28,707:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131), which has no missing parents
[INFO ]20161126@20:18:28,713:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 5) in 204 ms on localhost (1/1)
[INFO ]20161126@20:18:28,713:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:28,716:org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 330.2 KB)
[INFO ]20161126@20:18:28,726:org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 332.3 KB)
[INFO ]20161126@20:18:28,729:org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:47207 (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:28,730:org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:28,730:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[16] at sortBy at HW2_Part2.java:131)
[INFO ]20161126@20:18:28,731:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks
[INFO ]20161126@20:18:28,733:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 6, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161126@20:18:28,734:org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 6)
[INFO ]20161126@20:18:28,753:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:28,755:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
[INFO ]20161126@20:18:28,795:org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 6). 1402 bytes result sent to driver
[INFO ]20161126@20:18:28,802:org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (take at HW2_Part2.java:141) finished in 0.062 s
[INFO ]20161126@20:18:28,803:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: take at HW2_Part2.java:141, took 0.376384 s
[INFO ]20161126@20:18:28,812:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 6) in 69 ms on localhost (1/1)
[INFO ]20161126@20:18:28,813:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:29,049:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161126@20:18:29,106:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161126@20:18:29,108:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:47207 in memory (size: 2.4 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:29,109:org.apache.spark.ContextCleaner - Cleaned accumulator 7
[INFO ]20161126@20:18:29,110:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on localhost:47207 in memory (size: 2.1 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:29,232:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part2.java:143
[INFO ]20161126@20:18:29,233:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161126@20:18:29,236:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161126@20:18:29,239:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 2 is 143 bytes
[INFO ]20161126@20:18:29,252:org.apache.spark.scheduler.DAGScheduler - Got job 3 (saveAsTextFile at HW2_Part2.java:143) with 1 output partitions
[INFO ]20161126@20:18:29,252:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 14 (saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161126@20:18:29,252:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 13)
[INFO ]20161126@20:18:29,253:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161126@20:18:29,255:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143), which has no missing parents
[INFO ]20161126@20:18:29,320:org.apache.spark.storage.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 71.4 KB, free 390.9 KB)
[INFO ]20161126@20:18:29,340:org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 25.0 KB, free 415.9 KB)
[INFO ]20161126@20:18:29,341:org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on localhost:47207 (size: 25.0 KB, free: 1027.3 MB)
[INFO ]20161126@20:18:29,341:org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
[INFO ]20161126@20:18:29,342:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[17] at saveAsTextFile at HW2_Part2.java:143)
[INFO ]20161126@20:18:29,343:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 1 tasks
[INFO ]20161126@20:18:29,348:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 7, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161126@20:18:29,349:org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 7)
[INFO ]20161126@20:18:29,463:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161126@20:18:29,463:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[INFO ]20161126@20:18:29,535:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161126@20:18:29,674:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611262018_0014_m_000000_7' to file:/home/cloudera/Desktop/workspace/homework2/output/hw2part2_1480220303147/_temporary/0/task_201611262018_0014_m_000000
[INFO ]20161126@20:18:29,681:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611262018_0014_m_000000_7: Committed
[INFO ]20161126@20:18:29,693:org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 7). 2038 bytes result sent to driver
[INFO ]20161126@20:18:29,701:org.apache.spark.scheduler.DAGScheduler - ResultStage 14 (saveAsTextFile at HW2_Part2.java:143) finished in 0.344 s
[INFO ]20161126@20:18:29,703:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: saveAsTextFile at HW2_Part2.java:143, took 0.471303 s
[INFO ]20161126@20:18:29,710:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 7) in 353 ms on localhost (1/1)
[INFO ]20161126@20:18:29,710:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO ]20161126@20:18:29,798:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161126@20:18:29,823:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161126@20:18:29,840:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161126@20:18:29,843:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161126@20:18:29,846:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161126@20:18:29,849:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161126@20:18:29,860:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161126@20:18:29,865:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161126@20:18:29,868:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161126@20:18:29,872:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161126@20:18:29,881:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a226b26c-b0c9-490f-abb5-15bedfcdae42
